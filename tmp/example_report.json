{
  "$schema": "https://raw.githubusercontent.com/klenioaraujo/Reformulating-Transformers-for-LLMs/master/schemas/report_schema.json",
  "metadata": {
    "project": "\u03a8QRH Transformer",
    "version": "1.0.0",
    "doi": "https://zenodo.org/records/17171112",
    "license": "GPL-3.0-or-later"
  },
  "report_type": "energy_conservation",
  "timestamp": "2025-09-30T09:10:44.224183Z",
  "version": "1.0",
  "provenance": {
    "software_version": "1.0.0",
    "git_commit": "abc1234",
    "hardware": {
      "cpu": "Intel Xeon",
      "gpu": "NVIDIA RTX 3090",
      "ram": "32GB",
      "device": "cuda"
    },
    "execution_environment": {
      "python_version": "3.10.0",
      "pytorch_version": "2.0.0",
      "numpy_version": "1.26.4",
      "platform": "Linux",
      "os": "Ubuntu 22.04"
    },
    "input_data_hash": "sha256:abc123def456",
    "random_seed": 42,
    "execution_time": 123.45
  },
  "configuration": {
    "model_params": {
      "d_model": 512,
      "n_heads": 8,
      "n_layers": 6,
      "vocab_size": 50000,
      "max_seq_length": 512
    }
  },
  "energy_conservation": {
    "overall_conservation": 99.8,
    "layer_conservation": [
      {
        "layer": 0,
        "conservation": 99.9,
        "input_energy": 1000.0,
        "output_energy": 999.0
      },
      {
        "layer": 1,
        "conservation": 99.7,
        "input_energy": 999.0,
        "output_energy": 996.0
      }
    ],
    "deviation": 0.1,
    "status": "PASS"
  },
  "validation_summary": {
    "status": "PASS",
    "passed_tests": 10,
    "failed_tests": 0,
    "total_tests": 10,
    "success_rate": 100.0,
    "notes": [
      "All tests passed successfully"
    ],
    "warnings": [],
    "errors": []
  }
}