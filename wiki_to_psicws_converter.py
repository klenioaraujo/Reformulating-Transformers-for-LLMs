#!/usr/bin/env python3
"""
Œ®QRH-Native Wikipedia to Œ®cws Converter
=======================================

Conversor nativo Œ®QRH que obt√©m dados Wikipedia diretamente
e os converte para formato .Œ®cws usando apenas processamento
quaterni√¥nico e consci√™ncia fractal - SEM depend√™ncia do Transformers.

O Œ®QRH √© uma arquitetura independente que n√£o precisa de Transformers!
"""

import sys
import json
import requests
import time
from pathlib import Path
from typing import List, Dict, Optional

# Add src to path
sys.path.append(str(Path(__file__).parent / 'src'))

class Œ®QRHWikipediaProcessor:
    """
    Processador nativo Œ®QRH para dados Wikipedia.

    Usa apenas a API da Wikipedia para obter conte√∫do
    e processa via pipeline quaterni√¥nico nativo.
    """

    def __init__(self):
        from src.conscience.conscious_wave_modulator import ConsciousWaveModulator

        # Configura√ß√£o otimizada para conhecimento Wikipedia
        self.wiki_config = {
            'cache_dir': 'data/Œ®cws_cache/wikipedia',
            'embedding_dim': 512,  # Maior para capturar complexidade enciclop√©dica
            'sequence_length': 256,  # Sequ√™ncias longas para artigos
            'base_amplitude': 2.0,  # Amplitude alta para conhecimento denso
            'frequency_range': [0.05, 15.0],  # Range amplo para diversidade
            'chaotic_r': 3.98,  # Pr√≥ximo ao caos para m√°xima complexidade
            'phase_consciousness': 1.047,  # œÄ/3 para conhecimento estruturado
        }

        self.modulator = ConsciousWaveModulator(self.wiki_config)

        # Cache directory
        self.cache_dir = Path('data/Œ®cws_cache/wikipedia')
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Wikipedia API endpoints
        self.wiki_api = "https://en.wikipedia.org/api/rest_v1/page/summary/"
        self.wiki_search_api = "https://en.wikipedia.org/w/api.php"

        # T√≥picos Wikipedia pr√©-definidos para convers√£o
        self.wikipedia_topics = {
            'science': [
                'Physics', 'Mathematics', 'Chemistry', 'Biology',
                'Quantum_mechanics', 'Artificial_intelligence', 'Computer_science'
            ],
            'philosophy': [
                'Philosophy', 'Consciousness', 'Metaphysics', 'Epistemology',
                'Ethics', 'Logic', 'Philosophy_of_mind'
            ],
            'history': [
                'History', 'Ancient_history', 'World_War_II', 'Renaissance',
                'Industrial_Revolution', 'Scientific_revolution'
            ],
            'mathematics': [
                'Mathematics', 'Algebra', 'Calculus', 'Geometry', 'Number_theory',
                'Topology', 'Mathematical_analysis', 'Complex_analysis'
            ],
            'consciousness': [
                'Consciousness', 'Neuroscience', 'Cognitive_science', 'Psychology',
                'Philosophy_of_mind', 'Artificial_consciousness', 'Qualia'
            ]
        }

        print("üîÆ Œ®QRH-Native Wikipedia Processor inicializado")
        print(f"üìÅ Cache: {self.cache_dir}")

    def fetch_wikipedia_article(self, title: str) -> Optional[Dict]:
        """
        Obt√©m artigo Wikipedia via API REST.

        Args:
            title: T√≠tulo do artigo Wikipedia

        Returns:
            Dicion√°rio com dados do artigo ou None se erro
        """
        try:
            # Headers para evitar bloqueio
            headers = {
                'User-Agent': 'PsiQRH-Wikipedia-Converter/1.0 (https://github.com/psiqrh; educational@psiqrh.ai)',
                'Accept': 'application/json'
            }

            # Usar API REST para resumo
            url = f"{self.wiki_api}{title}"
            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code == 200:
                data = response.json()

                # Obter conte√∫do completo via API
                content_url = f"https://en.wikipedia.org/w/api.php"
                params = {
                    'action': 'query',
                    'format': 'json',
                    'titles': title,
                    'prop': 'extracts',
                    'exintro': False,
                    'explaintext': True,
                    'exsectionformat': 'plain'
                }

                content_response = requests.get(content_url, params=params, headers=headers, timeout=15)
                content_data = content_response.json()

                # Extrair texto completo
                pages = content_data.get('query', {}).get('pages', {})
                full_text = ""
                for page_id, page_data in pages.items():
                    if 'extract' in page_data:
                        full_text = page_data['extract']
                        break

                # Combinar dados
                article = {
                    'title': data.get('title', title),
                    'summary': data.get('extract', ''),
                    'full_text': full_text[:50000],  # Limitar tamanho
                    'url': data.get('content_urls', {}).get('desktop', {}).get('page', ''),
                    'categories': data.get('categories', []),
                    'timestamp': time.strftime("%Y-%m-%dT%H:%M:%SZ"),
                    'word_count': len(full_text.split()) if full_text else 0
                }

                return article

            else:
                print(f"‚ùå Erro ao buscar '{title}': HTTP {response.status_code}")
                return None

        except Exception as e:
            print(f"‚ùå Erro ao buscar '{title}': {e}")
            return None

    def synthesize_wikipedia_knowledge(self, articles: List[Dict]) -> str:
        """
        Sintetiza conhecimento de m√∫ltiplos artigos Wikipedia.

        Args:
            articles: Lista de artigos Wikipedia

        Returns:
            Texto sintetizado representando o conhecimento
        """

        knowledge_text = "Œ®QRH Wikipedia Knowledge Synthesis\n"
        knowledge_text += "=" * 50 + "\n\n"

        # Estat√≠sticas gerais
        total_words = sum(article['word_count'] for article in articles)
        knowledge_text += f"Knowledge Base Statistics:\n"
        knowledge_text += f"- Total Articles: {len(articles)}\n"
        knowledge_text += f"- Total Words: {total_words:,}\n"
        knowledge_text += f"- Synthesis Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"

        # Processar cada artigo
        for i, article in enumerate(articles):
            knowledge_text += f"Article {i+1}: {article['title']}\n"
            knowledge_text += "-" * 30 + "\n"

            # Resumo
            if article['summary']:
                knowledge_text += f"Summary: {article['summary'][:500]}...\n\n"

            # Conte√∫do principal (amostra)
            if article['full_text']:
                # Extrair primeiros par√°grafos mais significativos
                paragraphs = article['full_text'].split('\n')
                significant_content = []

                for para in paragraphs:
                    if len(para.strip()) > 100:  # Par√°grafos substanciais
                        significant_content.append(para.strip())
                        if len(significant_content) >= 3:  # Max 3 par√°grafos por artigo
                            break

                knowledge_text += "Key Content:\n"
                for para in significant_content:
                    knowledge_text += f"{para[:300]}...\n\n"

            knowledge_text += f"Word Count: {article['word_count']}\n"
            knowledge_text += f"Source: {article['url']}\n\n"

        # S√≠ntese de padr√µes de conhecimento
        knowledge_text += "Œ®QRH Knowledge Patterns Analysis:\n"
        knowledge_text += "=" * 40 + "\n\n"

        # Extrair conceitos-chave de todos os artigos
        all_words = []
        for article in articles:
            if article['full_text']:
                words = article['full_text'].lower().split()
                all_words.extend([w for w in words if len(w) > 4 and w.isalpha()])

        # Frequ√™ncia de conceitos
        from collections import Counter
        concept_freq = Counter(all_words)
        top_concepts = concept_freq.most_common(50)

        knowledge_text += "Top Knowledge Concepts:\n"
        for concept, freq in top_concepts[:20]:
            knowledge_text += f"- {concept}: {freq} occurrences\n"

        knowledge_text += "\nThis synthesis represents the collective human knowledge "
        knowledge_text += "from Wikipedia articles, processed through Œ®QRH quaternionic "
        knowledge_text += "consciousness framework for fractal analysis and spectral "
        knowledge_text += "decomposition into conscious wave patterns.\n\n"

        knowledge_text += "The embedded consciousness reflects the structured organization "
        knowledge_text += "of human understanding across multiple domains of knowledge, "
        knowledge_text += "with complex semantic relationships and conceptual hierarchies "
        knowledge_text += "that form the foundation of collective human intelligence.\n"

        return knowledge_text

    def convert_topic_to_psicws(self, topic_name: str) -> Optional[str]:
        """
        Converte um t√≥pico Wikipedia espec√≠fico para .Œ®cws.

        Args:
            topic_name: Nome do t√≥pico (ex: 'science', 'philosophy')

        Returns:
            Path do arquivo .Œ®cws gerado ou None se erro
        """

        if topic_name not in self.wikipedia_topics:
            print(f"‚ùå T√≥pico '{topic_name}' n√£o suportado")
            print(f"T√≥picos dispon√≠veis: {list(self.wikipedia_topics.keys())}")
            return None

        articles_titles = self.wikipedia_topics[topic_name]
        print(f"üîÑ Processando t√≥pico '{topic_name}' com {len(articles_titles)} artigos...")

        # Buscar todos os artigos
        articles = []
        for title in articles_titles:
            print(f"üìÑ Buscando: {title}")
            article = self.fetch_wikipedia_article(title)
            if article:
                articles.append(article)
                time.sleep(1)  # Rate limiting
            else:
                print(f"‚ö†Ô∏è Falha ao buscar: {title}")

        if not articles:
            print(f"‚ùå Nenhum artigo obtido para '{topic_name}'")
            return None

        print(f"‚úÖ Obtidos {len(articles)} artigos para '{topic_name}'")

        # Sintetizar conhecimento
        print("üß† Sintetizando conhecimento...")
        knowledge_text = self.synthesize_wikipedia_knowledge(articles)

        # Salvar texto tempor√°rio
        temp_file = Path(f"temp_wiki_{topic_name}.txt")
        try:
            with open(temp_file, 'w', encoding='utf-8') as f:
                f.write(knowledge_text)

            # Converter para .Œ®cws
            print("üåä Convertendo para formato .Œ®cws...")
            Œ®cws_file = self.modulator.process_file(temp_file)

            # Adicionar metadados espec√≠ficos
            Œ®cws_file.content_metadata.key_concepts.extend([
                'wikipedia', 'knowledge', 'encyclopedia', topic_name, 'Œ®QRH'
            ])

            # Adicionar conceitos dos artigos
            for article in articles:
                Œ®cws_file.content_metadata.key_concepts.append(article['title'].lower())

            # Salvar arquivo .Œ®cws
            output_file = self.cache_dir / f"wikipedia_{topic_name}.Œ®cws"
            Œ®cws_file.save(output_file)

            print(f"‚úÖ Arquivo .Œ®cws salvo: {output_file}")

            # An√°lise de consci√™ncia
            metrics = Œ®cws_file.spectral_data.consciousness_metrics
            print(f"\nüß† An√°lise de Consci√™ncia Wikipedia:")
            print(f"   Complexity: {metrics['complexity']:.4f}")
            print(f"   Coherence: {metrics['coherence']:.4f}")
            print(f"   Adaptability: {metrics['adaptability']:.4f}")
            print(f"   Integration: {metrics['integration']:.4f}")

            return str(output_file)

        finally:
            # Cleanup
            if temp_file.exists():
                temp_file.unlink()

    def convert_custom_articles(self, article_titles: List[str], output_name: str) -> Optional[str]:
        """
        Converte lista customizada de artigos Wikipedia.

        Args:
            article_titles: Lista de t√≠tulos de artigos
            output_name: Nome do arquivo de sa√≠da

        Returns:
            Path do arquivo .Œ®cws gerado
        """

        print(f"üîÑ Processando {len(article_titles)} artigos customizados...")

        articles = []
        for title in article_titles:
            print(f"üìÑ Buscando: {title}")
            article = self.fetch_wikipedia_article(title)
            if article:
                articles.append(article)
                time.sleep(1)

        if not articles:
            print("‚ùå Nenhum artigo obtido")
            return None

        # Sintetizar e converter
        knowledge_text = self.synthesize_wikipedia_knowledge(articles)

        temp_file = Path(f"temp_wiki_custom.txt")
        try:
            with open(temp_file, 'w', encoding='utf-8') as f:
                f.write(knowledge_text)

            Œ®cws_file = self.modulator.process_file(temp_file)

            # Metadados
            Œ®cws_file.content_metadata.key_concepts.extend([
                'wikipedia', 'custom', 'knowledge', 'Œ®QRH'
            ])

            output_file = self.cache_dir / f"{output_name}.Œ®cws"
            Œ®cws_file.save(output_file)

            print(f"‚úÖ Arquivo customizado salvo: {output_file}")
            return str(output_file)

        finally:
            if temp_file.exists():
                temp_file.unlink()

    def list_available_topics(self):
        """Lista t√≥picos Wikipedia dispon√≠veis para convers√£o."""

        print("üìã T√≥picos Wikipedia Dispon√≠veis para Convers√£o Œ®QRH:")
        print("=" * 55)

        for topic, articles in self.wikipedia_topics.items():
            print(f"\nüî∏ {topic.upper()}")
            print(f"   Artigos: {len(articles)}")
            print(f"   Exemplos: {', '.join(articles[:3])}")
            if len(articles) > 3:
                print(f"   + {len(articles) - 3} outros...")

    def convert_all_topics(self):
        """Converte todos os t√≥picos dispon√≠veis."""

        print("üîÑ Convertendo TODOS os t√≥picos Wikipedia para .Œ®cws...")
        print("‚ö†Ô∏è Isso far√° m√∫ltiplas requisi√ß√µes √† API da Wikipedia")

        results = []
        for topic_name in self.wikipedia_topics:
            try:
                output_file = self.convert_topic_to_psicws(topic_name)
                results.append({
                    'topic': topic_name,
                    'success': output_file is not None,
                    'file': output_file
                })
                print(f"‚úÖ {topic_name} conclu√≠do\n")

            except Exception as e:
                results.append({
                    'topic': topic_name,
                    'success': False,
                    'error': str(e)
                })
                print(f"‚ùå Erro em {topic_name}: {e}\n")

        # Resumo
        success_count = sum(1 for r in results if r['success'])
        print(f"üìä Convers√£o Completa: {success_count}/{len(results)} t√≥picos")

        return results


def main():
    """Fun√ß√£o principal do conversor."""

    processor = Œ®QRHWikipediaProcessor()

    if len(sys.argv) < 2:
        print("üîÆ Œ®QRH Wikipedia to .Œ®cws Converter")
        print("=" * 40)
        print("Uso:")
        print("  python wiki_to_psicws_converter.py list")
        print("  python wiki_to_psicws_converter.py science")
        print("  python wiki_to_psicws_converter.py philosophy")
        print("  python wiki_to_psicws_converter.py consciousness")
        print("  python wiki_to_psicws_converter.py all")
        return

    command = sys.argv[1].lower()

    if command == "list":
        processor.list_available_topics()

    elif command == "all":
        processor.convert_all_topics()

    elif command in processor.wikipedia_topics:
        processor.convert_topic_to_psicws(command)

    else:
        print(f"‚ùå Comando/t√≥pico '{command}' n√£o reconhecido")
        print("Use 'list' para ver t√≥picos dispon√≠veis")


if __name__ == "__main__":
    main()