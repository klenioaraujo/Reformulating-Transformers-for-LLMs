#!/usr/bin/env python3
"""
Treinamento com EvoluÃ§Ã£o QuÃ¢ntica EstÃ¡vel
==========================================

ImplementaÃ§Ã£o do treinamento usando Prime Resonant Filtering + Leech Lattice Embedding
para resolver problemas de instabilidade numÃ©rica e colapso de similaridade.

Este treinamento substitui o FFT padrÃ£o por tÃ©cnicas de estabilizaÃ§Ã£o baseadas
em princÃ­pios matemÃ¡ticos avanÃ§ados.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import json
import os
from pathlib import Path
import time
from typing import Dict, List, Tuple, Optional
import argparse

# Import Î¨QRH components
from psiqrh import Î¨QRHPipeline
from src.core.losses import QuantumContrastiveLoss
from src.core.prime_resonant_filter import StableQuantumEvolution


class StableEvolutionTrainer:
    """
    Treinador para evoluÃ§Ã£o quÃ¢ntica estÃ¡vel usando filtragem ressonante
    e embedding em Leech Lattice.
    """

    def __init__(self, embed_dim: int = 64, device: str = 'cpu'):
        """
        Inicializa o treinador.

        Args:
            embed_dim: DimensÃ£o do embedding
            device: Dispositivo para computaÃ§Ã£o
        """
        self.embed_dim = embed_dim
        self.device = device

        # Componentes principais
        self.stable_evolution = StableQuantumEvolution(embed_dim=embed_dim, device=device)
        self.contrastive_loss = QuantumContrastiveLoss(margin=0.5)

        # Otimizadores
        self.optimizer = optim.AdamW(
            self.stable_evolution.parameters(),
            lr=1e-4,
            weight_decay=0.01
        )

        # Scheduler para decaimento de learning rate
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, T_0=1000, T_mult=2
        )

        print("ğŸ“ Stable Evolution Trainer initialized")
        print(f"   ğŸ“ Embed dim: {embed_dim}")
        print(f"   ğŸ”§ Device: {device}")

    def prepare_training_data(self, vocab_size: int = 256) -> DataLoader:
        """
        Prepara dados de treinamento usando caracteres ASCII.

        Args:
            vocab_size: Tamanho do vocabulÃ¡rio

        Returns:
            DataLoader com pares de treinamento
        """
        print(f"ğŸ“š Preparando dados de treinamento (vocab_size={vocab_size})...")

        # Criar pares de caracteres similares e diferentes
        training_pairs = []

        # Caracteres similares (mesma categoria)
        similar_pairs = [
            ('a', 'A'), ('e', 'E'), ('i', 'I'), ('o', 'O'), ('u', 'U'),  # Vogais
            ('b', 'B'), ('c', 'C'), ('d', 'D'), ('f', 'F'), ('g', 'G'),  # Consoantes
            ('1', 'l'), ('0', 'O'), ('2', 'Z'), ('5', 'S'), ('8', 'B'),  # Visualmente similares
        ]

        # Caracteres diferentes (para contraste)
        all_chars = [chr(i) for i in range(32, 127)]  # Printable ASCII

        for char1, char2 in similar_pairs:
            if char1 in all_chars and char2 in all_chars:
                # Criar embeddings quÃ¢nticos
                psi1 = self._char_to_quantum(char1)
                psi2 = self._char_to_quantum(char2)

                # Escolher caractere negativo aleatÃ³rio
                negative_char = np.random.choice([c for c in all_chars if c not in [char1, char2]])
                psi_negative = self._char_to_quantum(negative_char)

                training_pairs.append((psi1, psi2, psi_negative))

        # Adicionar mais pares aleatÃ³rios para aumentar o dataset
        for _ in range(1000):
            char1, char2 = np.random.choice(all_chars, 2, replace=False)
            psi1 = self._char_to_quantum(char1)
            psi2 = self._char_to_quantum(char2)

            negative_char = np.random.choice([c for c in all_chars if c not in [char1, char2]])
            psi_negative = self._char_to_quantum(negative_char)

            training_pairs.append((psi1, psi2, psi_negative))

        # Converter para tensores
        contexts = torch.stack([p[0] for p in training_pairs])
        positives = torch.stack([p[1] for p in training_pairs])
        negatives = torch.stack([p[2] for p in training_pairs])

        # Criar dataset e dataloader
        dataset = TensorDataset(contexts, positives, negatives)
        dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

        print(f"   âœ… Dados preparados: {len(training_pairs)} pares de treinamento")
        return dataloader

    def _char_to_quantum(self, char: str) -> torch.Tensor:
        """
        Converte caractere para representaÃ§Ã£o quÃ¢ntica.

        Args:
            char: Caractere a converter

        Returns:
            Tensor quÃ¢ntico [embed_dim, 4]
        """
        # Usar codificaÃ§Ã£o simples baseada em ASCII
        ascii_val = ord(char)
        psi = torch.zeros(self.embed_dim, 4, dtype=torch.float32, device=self.device)

        for j in range(self.embed_dim):
            # CodificaÃ§Ã£o determinÃ­stica baseada no caractere
            phase = (ascii_val + j) * 2 * np.pi / 256.0
            amplitude = (ascii_val / 255.0) * (j / self.embed_dim)

            psi[j, 0] = amplitude * np.cos(phase)  # w (real)
            psi[j, 1] = amplitude * np.sin(phase)  # x (i)
            psi[j, 2] = 0.1 * amplitude  # y (j) - reduzido
            psi[j, 3] = 0.1 * amplitude  # z (k) - reduzido

        return psi

    def train_epoch(self, dataloader: DataLoader) -> Dict[str, float]:
        """
        Executa uma Ã©poca de treinamento.

        Args:
            dataloader: DataLoader com dados de treinamento

        Returns:
            MÃ©tricas da Ã©poca
        """
        self.stable_evolution.train()
        epoch_loss = 0.0
        num_batches = 0

        for batch_idx, (contexts, positives, negatives) in enumerate(dataloader):
            # Mover para dispositivo
            contexts = contexts.to(self.device)
            positives = positives.to(self.device)
            negatives = negatives.to(self.device)

            # Aplicar evoluÃ§Ã£o estÃ¡vel aos contextos
            evolved_contexts = self.stable_evolution(contexts)

            # Calcular perda de contraste
            loss = self.contrastive_loss(evolved_contexts, positives, negatives)

            # Backward pass
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            epoch_loss += loss.item()
            num_batches += 1

            if batch_idx % 10 == 0:
                print(f"   ğŸ“Š Batch {batch_idx}/{len(dataloader)}: Loss={loss.item():.4f}")

        # Atualizar scheduler
        self.scheduler.step()

        avg_loss = epoch_loss / num_batches
        return {'loss': avg_loss}

    def train(self, num_epochs: int = 10, save_path: str = 'models/stable_evolution'):
        """
        Executa treinamento completo.

        Args:
            num_epochs: NÃºmero de Ã©pocas
            save_path: Caminho para salvar checkpoints
        """
        print("ğŸš€ Iniciando treinamento com evoluÃ§Ã£o quÃ¢ntica estÃ¡vel...")
        print(f"   ğŸ¯ Ã‰pocas: {num_epochs}")
        print(f"   ğŸ’¾ Checkpoint path: {save_path}")

        # Preparar dados
        dataloader = self.prepare_training_data()

        # Criar diretÃ³rio de checkpoints
        os.makedirs(save_path, exist_ok=True)

        # HistÃ³rico de treinamento
        training_history = []

        for epoch in range(num_epochs):
            print(f"\nğŸ¯ Epoch {epoch+1}/{num_epochs}")

            # Treinar Ã©poca
            start_time = time.time()
            metrics = self.train_epoch(dataloader)
            epoch_time = time.time() - start_time

            # Registrar mÃ©tricas
            epoch_data = {
                'epoch': epoch + 1,
                'loss': metrics['loss'],
                'time': epoch_time,
                'learning_rate': self.optimizer.param_groups[0]['lr']
            }
            training_history.append(epoch_data)

            print(f"   âœ… Epoch concluÃ­da: Loss={metrics['loss']:.4f}, Time={epoch_time:.2f}s")

            # Salvar checkpoint a cada 5 Ã©pocas
            if (epoch + 1) % 5 == 0:
                checkpoint_path = os.path.join(save_path, f'checkpoint_epoch_{epoch+1}.pt')
                self.save_checkpoint(checkpoint_path)
                print(f"   ğŸ’¾ Checkpoint salvo: {checkpoint_path}")

        # Salvar modelo final
        final_path = os.path.join(save_path, 'final_model.pt')
        self.save_checkpoint(final_path)
        print(f"   ğŸ‰ Modelo final salvo: {final_path}")

        # Salvar histÃ³rico
        history_path = os.path.join(save_path, 'training_history.json')
        with open(history_path, 'w') as f:
            json.dump(training_history, f, indent=2)
        print(f"   ğŸ“Š HistÃ³rico salvo: {history_path}")

        return training_history

    def save_checkpoint(self, path: str):
        """Salva checkpoint do modelo."""
        checkpoint = {
            'stable_evolution_state_dict': self.stable_evolution.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'embed_dim': self.embed_dim
        }
        torch.save(checkpoint, path)

    def load_checkpoint(self, path: str):
        """Carrega checkpoint do modelo."""
        checkpoint = torch.load(path, map_location=self.device)
        self.stable_evolution.load_state_dict(checkpoint['stable_evolution_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        print(f"âœ… Checkpoint carregado: {path}")

    def evaluate_stability(self) -> Dict[str, float]:
        """
        Avalia mÃ©tricas de estabilidade do sistema treinado.

        Returns:
            DicionÃ¡rio com mÃ©tricas de estabilidade
        """
        print("ğŸ”¬ Avaliando estabilidade do sistema...")

        # Obter mÃ©tricas de estabilidade dos componentes
        stability_metrics = self.stable_evolution.get_stability_metrics()

        # Teste adicional: verificar preservaÃ§Ã£o de energia
        test_input = torch.randn(1, self.embed_dim, 4, device=self.device)
        test_output = self.stable_evolution(test_input)

        energy_preservation = torch.norm(test_output) / torch.norm(test_input)
        energy_error = abs(energy_preservation.item() - 1.0)

        # Teste de similaridade: verificar se caracteres similares permanecem prÃ³ximos
        char_a = self._char_to_quantum('a').unsqueeze(0)
        char_A = self._char_to_quantum('A').unsqueeze(0)
        char_z = self._char_to_quantum('z').unsqueeze(0)

        evolved_a = self.stable_evolution(char_a)
        evolved_A = self.stable_evolution(char_A)
        evolved_z = self.stable_evolution(char_z)

        # Calcular similaridades
        sim_a_A = torch.cosine_similarity(evolved_a.flatten(), evolved_A.flatten(), dim=0)
        sim_a_z = torch.cosine_similarity(evolved_a.flatten(), evolved_z.flatten(), dim=0)

        similarity_preservation = sim_a_A > sim_a_z  # Caracteres similares devem ser mais prÃ³ximos

        evaluation_results = {
            **stability_metrics,
            'energy_preservation': energy_preservation.item(),
            'energy_error': energy_error,
            'similarity_preservation': float(similarity_preservation),
            'similarity_a_A': sim_a_A.item(),
            'similarity_a_z': sim_a_z.item()
        }

        print("   ğŸ“Š MÃ©tricas de estabilidade:")
        for key, value in evaluation_results.items():
            print(".4f")

        return evaluation_results


def main():
    """FunÃ§Ã£o principal para treinamento."""
    parser = argparse.ArgumentParser(description='Treinamento com EvoluÃ§Ã£o QuÃ¢ntica EstÃ¡vel')
    parser.add_argument('--embed-dim', type=int, default=64, help='DimensÃ£o do embedding')
    parser.add_argument('--epochs', type=int, default=10, help='NÃºmero de Ã©pocas')
    parser.add_argument('--device', type=str, default='cpu', help='Dispositivo (cpu/cuda)')
    parser.add_argument('--save-path', type=str, default='models/stable_evolution', help='Caminho para salvar')
    parser.add_argument('--load-checkpoint', type=str, help='Carregar checkpoint existente')

    args = parser.parse_args()

    # Verificar dispositivo
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸ CUDA nÃ£o disponÃ­vel, usando CPU")
        args.device = 'cpu'

    # Inicializar treinador
    trainer = StableEvolutionTrainer(embed_dim=args.embed_dim, device=args.device)

    # Carregar checkpoint se especificado
    if args.load_checkpoint:
        trainer.load_checkpoint(args.load_checkpoint)

    # Executar treinamento
    try:
        history = trainer.train(num_epochs=args.epochs, save_path=args.save_path)

        # Avaliar estabilidade final
        stability_metrics = trainer.evaluate_stability()

        # Salvar resultados finais
        final_results = {
            'training_history': history,
            'final_stability_metrics': stability_metrics,
            'config': {
                'embed_dim': args.embed_dim,
                'epochs': args.epochs,
                'device': args.device
            }
        }

        results_path = os.path.join(args.save_path, 'final_results.json')
        with open(results_path, 'w') as f:
            json.dump(final_results, f, indent=2)

        print(f"\nğŸ‰ Treinamento concluÃ­do com sucesso!")
        print(f"   ğŸ“Š Loss final: {history[-1]['loss']:.4f}")
        print(f"   ğŸ”¬ Erro de unitariedade: {stability_metrics['unitarity_error']:.6f}")
        print(f"   ğŸ’¾ Resultados salvos em: {results_path}")

    except Exception as e:
        print(f"âŒ Erro durante treinamento: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == '__main__':
    exit(main())