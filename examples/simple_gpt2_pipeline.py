#!/usr/bin/env python3
"""
Pipeline Simples de Entrada-Sa√≠da usando GPT-2
Este script implementa um pipeline b√°sico usando o modelo GPT-2
como refer√™ncia para demonstrar o funcionamento de entrada-sa√≠da.
"""
import sys
import os
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Adicionar o diret√≥rio pai ao path para importar m√≥dulos locais
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def setup_gpt2_pipeline():
    """
    Configura o pipeline GPT-2 com modelo e tokenizer
    """
    print("üöÄ Inicializando Pipeline GPT-2...")

    # Carregar modelo e tokenizer
    model_name = "gpt2"
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)

    # Configurar tokenizer para padding
    tokenizer.pad_token = tokenizer.eos_token

    print(f"‚úÖ Modelo {model_name} carregado com sucesso!")
    print(f"   - Vocabul√°rio: {tokenizer.vocab_size} tokens")
    print(f"   - Par√¢metros: {sum(p.numel() for p in model.parameters()):,}")

    return model, tokenizer

def process_text_input(model, tokenizer, text_input, max_length=50):
    """
    Processa entrada de texto e gera sa√≠da usando GPT-2
    """
    print(f"\nüì• Processando entrada: '{text_input}'")

    # Tokenizar entrada
    inputs = tokenizer(text_input, return_tensors="pt", padding=True, truncation=True)
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    print(f"   - Tokens de entrada: {input_ids.shape[1]}")
    print(f"   - IDs: {input_ids.tolist()[0]}")

    # Gerar sa√≠da
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.8,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    # Decodificar sa√≠da
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return generated_text

def analyze_pipeline_flow(model, tokenizer, sample_texts):
    """
    Analisa o fluxo completo do pipeline para m√∫ltiplas entradas
    """
    print("\n" + "="*60)
    print("üîç AN√ÅLISE DO FLUXO DO PIPELINE")
    print("="*60)

    for i, text in enumerate(sample_texts, 1):
        print(f"\nüìã Exemplo {i}:")
        print(f"   Entrada: '{text}'")

        # Processar entrada
        output = process_text_input(model, tokenizer, text)

        print(f"   Sa√≠da: '{output}'")

        # An√°lise adicional
        inputs = tokenizer(text, return_tensors="pt")
        print(f"   - Comprimento entrada: {inputs['input_ids'].shape[1]} tokens")
        print(f"   - Comprimento sa√≠da: {len(tokenizer.encode(output))} tokens")

def test_batch_processing(model, tokenizer):
    """
    Testa processamento em lote
    """
    print("\n" + "="*60)
    print("üß™ TESTE DE PROCESSAMENTO EM LOTE")
    print("="*60)

    batch_texts = [
        "O c√©u √©",
        "A tecnologia avan√ßa",
        "Aprendizado de m√°quina √©"
    ]

    print(f"Processando lote com {len(batch_texts)} textos...")

    for text in batch_texts:
        output = process_text_input(model, tokenizer, text, max_length=30)
        print(f"\nüì• '{text}'")
        print(f"üì§ '{output}'")

def demonstrate_tokenization(tokenizer):
    """
    Demonstra o processo de tokeniza√ß√£o
    """
    print("\n" + "="*60)
    print("üî§ DEMONSTRA√á√ÉO DE TOKENIZA√á√ÉO")
    print("="*60)

    sample_text = "Transformers s√£o incr√≠veis para NLP!"

    print(f"Texto: '{sample_text}'")

    # Tokeniza√ß√£o
    tokens = tokenizer.tokenize(sample_text)
    token_ids = tokenizer.encode(sample_text)

    print(f"Tokens: {tokens}")
    print(f"IDs: {token_ids}")

    # Decodifica√ß√£o
    decoded = tokenizer.decode(token_ids)
    print(f"Decodificado: '{decoded}'")

def main():
    """
    Fun√ß√£o principal do pipeline
    """
    print("üöÄ INICIANDO PIPELINE GPT-2")
    print("="*60)

    try:
        # Configurar pipeline
        model, tokenizer = setup_gpt2_pipeline()

        # Textos de exemplo para teste
        sample_texts = [
            "O futuro da intelig√™ncia artificial",
            "A ci√™ncia nos permite",
            "Python √© uma linguagem",
            "A matem√°tica √© a"
        ]

        # Executar an√°lises
        analyze_pipeline_flow(model, tokenizer, sample_texts)
        demonstrate_tokenization(tokenizer)
        test_batch_processing(model, tokenizer)

        # Teste interativo
        print("\n" + "="*60)
        print("üí¨ TESTE INTERATIVO")
        print("="*60)
        print("Digite 'quit' para sair")

        while True:
            user_input = input("\nüì• Digite um texto: ").strip()

            if user_input.lower() in ['quit', 'exit', 'sair']:
                break

            if user_input:
                output = process_text_input(model, tokenizer, user_input)
                print(f"üì§ Resposta: {output}")

        print("\n‚úÖ Pipeline executado com sucesso!")

    except Exception as e:
        print(f"\n‚ùå Erro no pipeline: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()