#!/usr/bin/env python3
"""
Î¨QRH Pipeline usando QuantumCharacterMatrix
===========================================

Pipeline alternativo usando apenas a QuantumCharacterMatrix para geraÃ§Ã£o de texto.
"""

import torch
import sys
import os
from typing import List

# Adicionar diretÃ³rio base ao path
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, BASE_DIR)

from quantum_character_matrix import QuantumCharacterMatrix

class Î¨QRHPipelineQuantumMatrix:
    """
    Pipeline simples usando apenas QuantumCharacterMatrix para geraÃ§Ã£o.
    """

    def __init__(self, vocabulary: List[str] = None):
        if vocabulary is None:
            vocabulary = list("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,!?")

        self.qcm = QuantumCharacterMatrix(vocabulary=vocabulary)
        self.vocabulary = vocabulary

        print("âœ… Î¨QRH Pipeline QuantumMatrix inicializado com sucesso.")
        print(f"   ðŸ“š VocabulÃ¡rio: {len(self.vocabulary)} caracteres.")

    def process(self, input_text: str, max_length: int = 20) -> str:
        """
        Gera texto usando apenas QuantumCharacterMatrix com preservaÃ§Ã£o de contexto melhorada.
        """
        print(f"\nðŸ”„ Processando: '{input_text}'")

        # Codificar input
        input_states = []
        for i, char in enumerate(input_text):
            if char in self.vocabulary:
                state = self.qcm.encode_character(char, position=i)
                input_states.append(state)

        if not input_states:
            return ""

        # Usar uma combinaÃ§Ã£o dos estados de input como contexto inicial
        # Ponderar mais os Ãºltimos caracteres
        weights = torch.linspace(0.5, 1.0, len(input_states))
        weighted_states = [state.flatten() * weight for state, weight in zip(input_states, weights)]
        current_context = torch.stack(weighted_states).mean(dim=0)

        generated_chars = []
        current_position = len(input_text)

        for i in range(max_length):
            with torch.no_grad():
                # Decodificar contexto atual
                context_to_decode = current_context.view(self.qcm.embed_dim, 4)
                decoded_results = self.qcm.decode_quantum_state(
                    context_to_decode, top_k=5, position=current_position
                )

                if not decoded_results:
                    break

                # ðŸ”¥ SELEÃ‡ÃƒO INTELIGENTE COM DIVERSIDADE
                next_char = None
                best_score = -1.0

                for char_idx, (char, confidence) in enumerate(decoded_results):
                    # Penalizar caracteres repetidos recentemente
                    repetition_penalty = 0.0
                    if len(generated_chars) > 0:
                        recent_chars = generated_chars[-3:]  # Ãšltimos 3 caracteres
                        if char in recent_chars:
                            repetition_penalty = 0.3

                    # Penalizar posiÃ§Ã£o no ranking (incentivar diversidade)
                    rank_penalty = char_idx * 0.1

                    # Bonus para caracteres alfanumÃ©ricos e espaÃ§os
                    content_bonus = 0.0
                    if char in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,!?':
                        content_bonus = 0.2

                    # Score final
                    score = confidence - repetition_penalty - rank_penalty + content_bonus

                    if score > best_score:
                        next_char = char
                        best_score = score

                if next_char is None:
                    next_char, _ = decoded_results[0]

                # CritÃ©rio de parada
                if next_char == '<UNK>' or best_score < 0.05:
                    break

                generated_chars.append(next_char)

                # ðŸ”¥ ATUALIZAÃ‡ÃƒO DINÃ‚MICA DO CONTEXTO
                new_char_state = self.qcm.encode_character(next_char, position=current_position)

                # Blend ratio dinÃ¢mico: mais conservador no inÃ­cio, mais criativo depois
                if len(generated_chars) < 5:
                    context_blend_ratio = 0.6  # Mais conservador
                else:
                    context_blend_ratio = 0.4  # Mais criativo

                current_context = (
                    context_blend_ratio * current_context +
                    (1 - context_blend_ratio) * new_char_state.flatten()
                )

                # ðŸ”¥ ADICIONAR RUÃDO CONTROLADO PARA DIVERSIDADE
                if len(generated_chars) > 3:
                    noise = torch.normal(0.0, 0.01, size=current_context.shape)
                    current_context = current_context + noise

                current_position += 1

        generated_text = "".join(generated_chars)
        print(f"   ðŸ”¬ Resposta Gerada: '{generated_text}'")
        return generated_text

def main():
    """Teste do pipeline alternativo."""
    pipeline = Î¨QRHPipelineQuantumMatrix()

    test_inputs = [
        "hello",
        "what is",
        "the meaning of",
        "life is"
    ]

    for input_text in test_inputs:
        print(f"\nðŸŽ¯ Input: '{input_text}'")
        result = pipeline.process(input_text)
        print(f"ðŸŽ¯ Output: '{result}'")

if __name__ == "__main__":
    main()