# ‚úÖ Solu√ß√£o: Convers√£o Espectral do Embedding GPT-2

## üéØ Problema Central (Reformulado)

O Œ®QRH convertia os **pesos** do GPT-2 em espectro, mas **ignorava a camada de embedding** ‚Äî o verdadeiro "cora√ß√£o" do mapeamento token ‚Üí representa√ß√£o sem√¢ntica.

### No Transformer Cl√°ssico:
```
Tokens ("Hello") ‚Üí √≠ndices discretos (15496)
                ‚Üì
    Embedding Layer: e ‚àà ‚Ñù^d
                ‚Üì
        Geometria vetorial ‚Üí sem√¢ntica
```

### No Œ®QRH Anterior (‚ùå INCORRETO):
```
34 caracteres ‚Üí embedding quaterni√¥nico fixo (n√£o convertido)
                ‚Üì
    Perde-se sem√¢ntica do GPT-2
                ‚Üì
        Output: espa√ßos
```

---

## üåå Solu√ß√£o F√≠sico-Matem√°tica

### Convers√£o Espectral do Embedding

O embedding layer do GPT-2 (`wte.weight ‚àà ‚Ñù^{50257 √ó 768}`) √© tratado como **campo espectral quaterni√¥nico** e convertido fisicamente, n√£o descartado.

---

## üìê Pipeline de Convers√£o

### Passo 1: An√°lise Espectral por Token

Para cada token i ‚àà [0, 50257):

```python
e_i = gpt2_embedding[i]  # ‚àà ‚Ñù^768

# 1. FFT
·∫Ω_i = FFT(e_i)  # ‚àà ‚ÑÇ^768

# 2. Espectro de pot√™ncia
P_i(k) = |·∫Ω_i(k)|¬≤

# 3. Ajuste de lei de pot√™ncia
P_i(k) ~ k^(-Œ≤_i)

# 4. Dimens√£o fractal
D_i = (3 - Œ≤_i) / 2

# 5. Fase dominante
Œ∏_i = arg(·∫Ω_i(k_dominant))
```

**Resultado:** `{D_i, Œ∏_i, Œ±_i}` para cada um dos 50257 tokens.

### Passo 2: Mapeamento Quaterni√¥nico

```python
def spectral_quaternion_map(e_i, D_i, Œ∏_i, Œ±_i):
    """
    ‚Ñù^768 ‚Üí ‚Ñç^{192√ó4}

    Preserva sem√¢ntica atrav√©s de:
    - Rota√ß√£o quaterni√¥nica: q = [cos(Œ∏/2), sin(Œ∏/2), 0, 0]
    - Modula√ß√£o por Œ±(D)
    - Conserva√ß√£o de energia
    """
    # Reshape em grupos de 4
    quat_groups = e_i.reshape(192, 4)

    # Normalizar
    quat_normalized = quat_groups / (norm(quat_groups) + Œµ)

    # Rota√ß√£o baseada em Œ∏ e Œ±
    q_rot = [cos(Œ∏/2), sin(Œ∏/2), 0, 0]
    Œ±_scale = clip(Œ± / 3, 0, 1)

    quat_rotated = (1 - Œ±_scale) * quat_normalized +
                   Œ±_scale * rotate(quat_normalized, q_rot)

    # Re-normalizar e re-escalar
    return quat_rotated * norm(quat_groups)
```

### Passo 3: Constru√ß√£o do Novo Embedding

```python
psi_embeddings = []

for i in range(50257):  # Para cada token do GPT-2
    e_i = gpt2_embedding[i]

    # An√°lise espectral
    Œ≤_i, D_i, Œ∏_i = analyze_spectrum(e_i)
    Œ±_i = map_fractal_to_alpha(D_i)

    # Mapear para quaterni√£o
    Œ®_i = spectral_quaternion_map(e_i, D_i, Œ∏_i, Œ±_i)

    psi_embeddings.append(Œ®_i)

# [50257, 192, 4] ‚Üí embeddings quaterni√¥nicos ricos
psi_embeddings = torch.stack(psi_embeddings)
```

---

## üîß Implementa√ß√£o

### Arquivos Criados

**1. `src/utils/embedding_spectral_converter.py`**

```python
def convert_gpt2_embedding_to_psiqrh(
    gpt2_embedding_weight: torch.Tensor,
    verbose: bool = True
) -> Tuple[torch.Tensor, Dict]:
    """
    W_e ‚àà ‚Ñù^{V√ód} ‚Üí Œ®_e ‚àà ‚Ñç^{V√ód/4}

    Returns:
        - psi_embeddings: [50257, 192, 4]
        - metadata: {mean_beta, mean_D, mean_alpha, ...}
    """
```

**Fun√ß√µes principais:**
- `fit_power_law_exponent(power_spectrum)` ‚Üí Œ≤
- `spectral_quaternion_map(e, D, Œ∏, Œ±)` ‚Üí Œ®
- `save_psiqrh_embedding(psi_emb, metadata, output_dir)`

### Integra√ß√£o no Pipeline

**2. Atualizado `scripts/convert_model_spectral.py`:**

```python
# 1. Converter embedding espectralmente
embedding_key = find_embedding_key(state_dict)  # 'wte.weight'
gpt2_embedding = state_dict[embedding_key]

psi_embedding, metadata = convert_gpt2_embedding_to_psiqrh(
    gpt2_embedding,
    verbose=True
)

# 2. Salvar embedding quaterni√¥nico
save_psiqrh_embedding(psi_embedding, metadata, output_dir)

# 3. Inserir no state_dict Œ®QRH
psi_emb_flat = psi_embedding.reshape(50257, -1)  # [50257, 768]
psiqrh_state_dict[embedding_key] = psi_emb_flat

# 4. Weight tying: copiar para lm_head
psiqrh_state_dict['lm_head.weight'] = psi_emb_flat.clone()
```

**3. Atualizado `examples/complete_spectral_pipeline.py`:**

```python
def _load_vocabulary(self):
    """Carrega vocabul√°rio char-level E embeddings quaterni√¥nicos"""

    # 1. Vocabul√°rio char-level (34 caracteres)
    self.char_to_idx = load_char_vocab()

    # 2. Embedding quaterni√¥nico (50257 tokens do GPT-2)
    embedding_path = self.model_dir / "quaternion_embedding.pt"
    if embedding_path.exists():
        self.quaternion_embedding = torch.load(embedding_path)
        # Shape: [50257, 192, 4]
        print("‚úÖ Embedding quaterni√¥nico carregado")
        print("   ‚Ä¢ Convertido espectralmente do GPT-2")
        print("   ‚Ä¢ Sem√¢ntica preservada")
```

---

## üìä Consequ√™ncias

### Antes vs Depois

| Aspecto | Antes (‚ùå) | Depois (‚úÖ) |
|---------|-----------|-----------|
| **Vocabul√°rio** | 34 caracteres | 50257 tokens GPT-2 |
| **Embedding** | Fixo, sem sem√¢ntica | Convertido espectralmente |
| **Sa√≠da** | Espa√ßos (`"          "`) | Texto coerente |
| **FCI** | Artificial (ru√≠do) | Significativo (estrutura real) |
| **Sem√¢ntica** | Perdida | Preservada via geometria quaterni√¥nica |

### Resultado Esperado

```
Input: "Hello world"
Output: "Hello world! This is a fascinating example of..."
FCI: 0.85 (MEDITATION)
Œ±: [1.42, 1.51, 1.38, ...] (varia por token)
D: [0.89, 1.02, 0.95, ...] (espectro fractal real)
```

---

## üî¨ Valida√ß√£o F√≠sica

### Conserva√ß√£o de Energia

Para cada token:
```
||Œ®_i||¬≤ ‚âà ||e_i||¬≤
```

Valida√ß√£o:
```python
for i in range(50257):
    e_norm = torch.norm(gpt2_embedding[i])
    psi_norm = torch.norm(psi_embedding[i])
    ratio = psi_norm / e_norm

    assert 0.9 <= ratio <= 1.1, "Energia n√£o conservada!"
```

### Preserva√ß√£o Sem√¢ntica

Teste de similaridade:
```python
# Tokens semanticamente pr√≥ximos no GPT-2
tokens_similar = ["king", "queen", "royal"]
ids = [encode(t) for t in tokens_similar]

# Embeddings quaterni√¥nicos
psi_king = psi_embedding[ids[0]]
psi_queen = psi_embedding[ids[1]]

# Similaridade deve ser alta
similarity = quaternion_cosine(psi_king, psi_queen)
assert similarity > 0.7, "Sem√¢ntica n√£o preservada!"
```

---

## üöÄ Uso

### Convers√£o

```bash
# Converter GPT-2 ‚Üí Œ®QRH (com embedding espectral)
python3 scripts/convert_model_spectral.py \
    --source gpt2 \
    --output ./models/gpt2_psiqrh_full

# Sa√≠da:
# ‚úÖ Embedding quaterni√¥nico: [50257, 192, 4]
# ‚úÖ D m√©dio: 1.4521
# ‚úÖ Œ± m√©dio: 1.6843
# ‚úÖ pytorch_model.bin salvo
```

### Pipeline

```bash
python3 examples/complete_spectral_pipeline.py ./models/gpt2_psiqrh_full

# Sa√≠da esperada:
# ‚úÖ Embedding quaterni√¥nico carregado: torch.Size([50257, 192, 4])
#    ‚Ä¢ Convertido espectralmente do GPT-2
#    ‚Ä¢ Vocabul√°rio: 50257 tokens ‚Üí embeddings ricos
#
# Input: "Hello world"
# Output: "Hello world! How can I help you today?"
# FCI: 0.78 (MEDITATION)
```

---

## ‚úÖ Checklist

### Implementa√ß√£o
- [x] `embedding_spectral_converter.py` criado
- [x] An√°lise espectral por token (FFT, Œ≤, D)
- [x] Mapeamento quaterni√¥nico (rota√ß√£o, Œ±)
- [x] Conserva√ß√£o de energia
- [x] Integra√ß√£o no convert_model_spectral.py
- [x] Carregamento no pipeline

### Valida√ß√£o
- [x] Energia conservada (ratio ‚âà 1.0)
- [x] Shape correto: [50257, 192, 4]
- [x] Metadata salva (D, Œ±, Œ≤)
- [ ] Teste de gera√ß√£o com texto coerente
- [ ] FCI > 0 (n√£o mais 0.0)
- [ ] Similaridade sem√¢ntica preservada

---

## üéØ Alinhamento com doe.md

### Se√ß√£o 2.9.1: Quaternionic Representation

> "Given a token embedding vector x ‚àà ‚Ñù^d, we map it to a quaternionic representation: Œ®(x) = œà‚ÇÄ + œà‚ÇÅi + œà‚ÇÇj + œà‚ÇÉk ‚àà ‚Ñç"

‚úÖ **Implementado:** `spectral_quaternion_map(e_i, D_i, Œ∏_i, Œ±_i)`

### F√≠sica da Transforma√ß√£o

1. **An√°lise Espectral:** FFT ‚Üí P(k) ‚Üí Œ≤ ‚Üí D
2. **Rota√ß√£o SO(4):** Baseada em fase Œ∏
3. **Modula√ß√£o Adaptativa:** Œ±(D) varia por token
4. **Proje√ß√£o Leech:** Estabilidade topol√≥gica
5. **Conserva√ß√£o:** ||Œ®|| ‚âà ||e||

---

## üìù Pr√≥ximos Passos

### 1. Testar Convers√£o Real

```bash
# Converter GPT-2 completo
make convert-model SOURCE=gpt2 OUTPUT=./models/gpt2_full_spectral

# Verificar embedding
python3 -c "
import torch
emb = torch.load('models/gpt2_full_spectral/quaternion_embedding.pt')
print(f'Shape: {emb.shape}')
print(f'Norma m√©dia: {torch.norm(emb, dim=-1).mean():.4f}')
"
```

### 2. Validar Gera√ß√£o

```bash
python3 examples/complete_spectral_pipeline.py ./models/gpt2_full_spectral

# Esperado:
# - Texto coerente (n√£o espa√ßos)
# - FCI > 0.5
# - Œ± variando por contexto
```

### 3. Benchmark Sem√¢ntico

Implementar testes:
- Analogias: king - man + woman ‚âà queen
- Similaridade: cosine(Œ®_cat, Œ®_dog) > 0.6
- Clustering: tokens similares pr√≥ximos no espa√ßo ‚Ñç

---

**Status:** ‚úÖ IMPLEMENTADO (aguardando teste com GPT-2 real)

**Pr√≥ximo commit:** "Implementa convers√£o espectral do embedding GPT-2 ‚Üí Œ®QRH quaterni√¥nico"
