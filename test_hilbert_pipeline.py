#!/usr/bin/env python3
"""
Teste do Pipeline Î¨QRH-Transformers com DeepSeek
================================================

Demonstra o uso do pipeline hÃ­brido com modelos reais como DeepSeek.
"""

import torch
import sys
import os
from pathlib import Path

# Adicionar diretÃ³rio base ao path
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, BASE_DIR)

from psiqrh_transformers import (
    HilbertConfig,
    HilbertLlamaForCausalLM,
    create_hilbert_pipeline_example
)

def test_with_deepseek_like_model():
    """
    Testa o pipeline com um modelo similar ao DeepSeek
    (usando configuraÃ§Ã£o compatÃ­vel com modelos de 7B parÃ¢metros)
    """
    print("ğŸš€ Testando Î¨QRH-Transformers com modelo DeepSeek-like")
    print("=" * 60)

    try:
        # ConfiguraÃ§Ã£o similar ao DeepSeek (7B parameters)
        config = HilbertConfig(
            vocab_size=32000,  # DeepSeek vocab size aproximado
            hidden_size=4096,  # DeepSeek hidden size
            num_hidden_layers=32,  # DeepSeek layers
            num_attention_heads=32,  # DeepSeek attention heads
            intermediate_size=11008,  # DeepSeek intermediate size
            hilbert_space="quaternion",  # Usar espaÃ§o quaterniÃ³nico
            spectral_alpha=1.0,
            fractal_dimension=1.5,
            use_spectral_filtering=True,
            use_fractal_embedding=True,
        )

        print("âœ… ConfiguraÃ§Ã£o DeepSeek-like criada:")
        print(f"   ğŸ“ EspaÃ§o de Hilbert: {config.hilbert_space}")
        print(f"   ğŸ§  Hidden Size: {config.hidden_size}")
        print(f"   ğŸ“š Vocab Size: {config.vocab_size}")
        print(f"   ğŸ”¢ Layers: {config.num_hidden_layers}")
        print(f"   ğŸ¯ Attention Heads: {config.num_attention_heads}")

        # Criar modelo (nota: isso cria um modelo do zero, nÃ£o carrega pesos prÃ©-treinados)
        print("\nğŸ”„ Criando modelo Hilbert-DeepSeek...")
        model = HilbertLlamaForCausalLM(config)

        # Contar parÃ¢metros
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        print("âœ… Modelo criado com sucesso!")
        print(f"   ğŸ“Š Total de parÃ¢metros: {total_params:,} ({total_params/1e9:.2f}B)")
        print(f"   ğŸ“ ParÃ¢metros treinÃ¡veis: {trainable_params:,}")

        # Teste de forward pass
        print("\nğŸ§ª Testando forward pass...")
        batch_size, seq_len = 1, 10
        input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))

        with torch.no_grad():
            outputs = model(input_ids, return_dict=True)
            logits = outputs['logits']

        print("âœ… Forward pass bem-sucedido!")
        print(f"   ğŸ“¥ Input shape: {input_ids.shape}")
        print(f"   ğŸ“¤ Output shape: {logits.shape}")
        print(f"   ğŸ“Š Logits range: [{logits.min().item():.3f}, {logits.max().item():.3f}]")

        # Teste de geraÃ§Ã£o de texto
        print("\nğŸ¤– Testando geraÃ§Ã£o de texto...")
        test_prompt = "The quantum nature of consciousness"

        # Simular geraÃ§Ã£o simples (greedy decoding)
        generated = input_ids.clone()
        max_new_tokens = 5

        for _ in range(max_new_tokens):
            with torch.no_grad():
                outputs = model(generated, return_dict=True)
                next_token_logits = outputs['logits'][:, -1, :]
                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                generated = torch.cat([generated, next_token], dim=-1)

        print("âœ… GeraÃ§Ã£o de texto bem-sucedida!")
        print(f"   ğŸ“ Prompt: '{test_prompt}'")
        print(f"   ğŸ¤– Generated tokens: {generated[0].tolist()}")

        return True

    except Exception as e:
        print(f"âŒ Erro no teste: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_pipeline_integration():
    """
    Testa a integraÃ§Ã£o com pipeline do Hugging Face
    """
    print("\nğŸ”— Testando integraÃ§Ã£o com pipeline...")
    print("=" * 60)

    try:
        # Tentar criar pipeline de exemplo
        pipe = create_hilbert_pipeline_example()
        print("âœ… Pipeline criado com sucesso!")
        return True

    except Exception as e:
        print(f"âš ï¸  Pipeline nÃ£o pÃ´de ser criado (esperado sem modelo real): {e}")
        print("ğŸ’¡ Para usar com modelo real, baixe um modelo Llama/DeepSeek")
        return False

def main():
    """
    FunÃ§Ã£o principal para executar todos os testes
    """
    print("Î¨QRH Transformers - Teste com DeepSeek")
    print("=" * 60)

    # Teste 1: Modelo DeepSeek-like
    success1 = test_with_deepseek_like_model()

    # Teste 2: IntegraÃ§Ã£o com pipeline
    success2 = test_pipeline_integration()

    # Resultado final
    print("\n" + "=" * 60)
    if success1:
        print("ğŸ‰ Teste principal BEM-SUCEDIDO!")
        print("âœ… Î¨QRH-Transformers compatÃ­vel com arquitetura DeepSeek")
        print("âœ… EspaÃ§o de Hilbert quaterniÃ³nico funcionando")
        print("âœ… Forward pass e geraÃ§Ã£o de texto operacionais")
    else:
        print("âŒ Teste principal FALHOU")

    if success2:
        print("âœ… IntegraÃ§Ã£o com pipeline Hugging Face funcionando")
    else:
        print("âš ï¸  IntegraÃ§Ã£o com pipeline limitada (requer modelo real)")

    print("\nğŸ’¡ Para usar com DeepSeek real:")
    print("   1. Instale transformers: pip install transformers")
    print("   2. Baixe modelo: huggingface-cli download deepseek-ai/deepseek-7b")
    print("   3. Adapte o cÃ³digo para carregar pesos prÃ©-treinados")
    print("   4. Use HilbertLlamaForCausalLM.from_pretrained()")

if __name__ == "__main__":
    main()