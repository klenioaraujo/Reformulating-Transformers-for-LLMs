#!/usr/bin/env python3
"""
Convers√£o Espectral de Modelos para Œ®QRH
==========================================

Script standalone para converter modelos tradicionais (GPT-2, BERT, etc.)
para Œ®QRH usando an√°lise espectral f√≠sica.

Usage:
    python3 convert_model_spectral.py --source gpt2 --output ./models/gpt2_psiqrh
    python3 convert_model_spectral.py --source ./path/to/model --output ./models/converted
    python3 convert_model_spectral.py --source bert-base-uncased --use-leech --validate-energy

Copyright (C) 2025 Klenio Araujo Padilha
Licensed under GNU GPLv3
"""

import argparse
import sys
import json
from pathlib import Path
import torch

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.spectral_model_converter import SpectralModelConverter, save_conversion_report
from src.utils.spectral_weight_mapper import map_spectral_to_state_dict, validate_energy_preservation
from src.utils.embedding_spectral_converter import (
    convert_gpt2_embedding_to_psiqrh,
    save_psiqrh_embedding
)


def load_source_model(source: str, device: str = 'cpu'):
    """
    Carrega modelo da fonte (local apenas - sistema aut√¥nomo Œ®QRH)

    Args:
        source: Fonte do modelo (path local)
        device: Dispositivo para carregar

    Returns:
        Modelo carregado
    """
    print(f"üì¶ Carregando modelo de: {source}")

    # Sistema aut√¥nomo Œ®QRH: apenas carregamento local
    print("   Sistema aut√¥nomo Œ®QRH - sem depend√™ncias externas")

    # Tentar carregar de arquivo local
    source_path = Path(source)
    if source_path.exists():
        print("   Tentando carregar de arquivo local...")

        # Se for diret√≥rio, procurar por pytorch_model.bin
        if source_path.is_dir():
            model_file = source_path / "pytorch_model.bin"
            if model_file.exists():
                print(f"   ‚úÖ Encontrado: {model_file}")
                model_state = torch.load(model_file, map_location=device)
                # TODO: Reconstruir modelo baseado em config.json
                return model_state, None
        else:
            # Arquivo √∫nico
            print(f"   ‚úÖ Carregando: {source_path}")
            model_state = torch.load(source_path, map_location=device)
            return model_state, None

    raise ValueError(f"‚ùå N√£o foi poss√≠vel carregar modelo de: {source}")


def save_converted_model(
    converted_params: dict,
    output_dir: Path,
    source_info: dict
):
    """
    Salva modelo convertido em formato compat√≠vel com Œ®QRH.

    Args:
        converted_params: Par√¢metros convertidos
        output_dir: Diret√≥rio de sa√≠da
        source_info: Informa√ß√µes do modelo fonte
    """
    output_dir.mkdir(parents=True, exist_ok=True)

    # Salvar par√¢metros convertidos
    params_file = output_dir / "converted_params.json"
    with open(params_file, 'w') as f:
        json.dump(converted_params, f, indent=2, default=str)
    print(f"‚úÖ Par√¢metros salvos: {params_file}")

    # Salvar configura√ß√£o para Œ®QRH
    config = {
        "model_type": "PsiQRHTransformerComplete",
        "source_model": source_info.get('model_type', 'unknown'),
        "framework": "Œ®QRH",
        "version": "2.0.0",
        "conversion_method": "spectral_analysis",
        "avg_fractal_dim": converted_params.get('avg_fractal_dim', 1.5),
        "avg_alpha": converted_params.get('avg_alpha', 1.5),
        "n_layers_analyzed": converted_params.get('n_layers_analyzed', 0)
    }

    config_file = output_dir / "config.json"
    with open(config_file, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"‚úÖ Configura√ß√£o salva: {config_file}")

    # Salvar relat√≥rio de convers√£o
    report_file = output_dir / "conversion_report.json"
    with open(report_file, 'w') as f:
        json.dump(converted_params, f, indent=2, default=str)
    print(f"‚úÖ Relat√≥rio salvo: {report_file}")

    # ‚úÖ ADICIONAR: Converter embedding layer espectralmente
    print("\nüîÑ Convertendo embedding layer do GPT-2...")

    if 'source_model' in source_info and hasattr(source_info['source_model'], 'state_dict'):
        source_model = source_info['source_model']
        source_state_dict = source_model.state_dict()

        # 1. Converter embedding espectralmente
        # Procurar embedding layer (pode ser wte.weight, transformer.wte.weight, etc.)
        embedding_key = None
        for key in source_state_dict.keys():
            if 'wte.weight' in key or 'word_embeddings.weight' in key or 'embedding.weight' in key:
                embedding_key = key
                break

        if embedding_key:
            print(f"   ‚Ä¢ Encontrado embedding: {embedding_key}")
            gpt2_embedding = source_state_dict[embedding_key]
            print(f"   ‚Ä¢ Shape: {gpt2_embedding.shape}")

            # Converter para quaterni√¥nico
            psi_embedding, embedding_metadata = convert_gpt2_embedding_to_psiqrh(
                gpt2_embedding,
                verbose=True
            )

            # Salvar embedding quaterni√¥nico (sem tokenizer - sistema aut√¥nomo)
            save_psiqrh_embedding(
                psi_embedding,
                embedding_metadata,
                output_dir
            )

        else:
            print(f"   ‚ö†Ô∏è  Embedding layer n√£o encontrado no modelo")

        # 2. Mapear pesos usando transforma√ß√µes quaterni√¥nicas
        print("\nüíæ Mapeando pesos usando par√¢metros espectrais...")

        psiqrh_state_dict = map_spectral_to_state_dict(
            source_state_dict,
            converted_params['converted_params']
        )

        # Substituir embedding cl√°ssico por quaterni√¥nico
        if embedding_key and embedding_key in psiqrh_state_dict:
            # Flatten quaternion embedding [V, d/4, 4] ‚Üí [V, d]
            psi_emb_flat = psi_embedding.reshape(psi_embedding.shape[0], -1)
            psiqrh_state_dict[embedding_key] = psi_emb_flat
            print(f"   ‚úÖ Embedding quaterni√¥nico inserido em {embedding_key}")

            # Weight tying: copiar para lm_head se existir
            lm_head_key = None
            for key in psiqrh_state_dict.keys():
                if 'lm_head.weight' in key or 'decoder.weight' in key:
                    lm_head_key = key
                    break

            if lm_head_key:
                psiqrh_state_dict[lm_head_key] = psi_emb_flat.clone()
                print(f"   ‚úÖ Weight tying: {lm_head_key} compartilha embedding")

        # Validar preserva√ß√£o de energia
        validation = validate_energy_preservation(
            source_state_dict,
            psiqrh_state_dict,
            tolerance=0.1
        )

        # Salvar state_dict transformado
        state_dict_path = output_dir / "pytorch_model.bin"
        torch.save(psiqrh_state_dict, state_dict_path)
        print(f"\n‚úÖ State dict mapeado salvo: {state_dict_path}")
        print(f"   N√∫mero de tensores: {len(psiqrh_state_dict)}")

        # Calcular tamanho
        total_params = sum(t.numel() for t in psiqrh_state_dict.values())
        total_size_mb = sum(t.element_size() * t.numel() for t in psiqrh_state_dict.values()) / (1024**2)
        print(f"   Total de par√¢metros: {total_params:,}")
        print(f"   Tamanho: {total_size_mb:.2f} MB")
        print(f"   Raz√£o de energia m√©dia: {validation['mean_energy_ratio']:.4f}")

        # Salvar metadados de valida√ß√£o
        validation_file = output_dir / "weight_mapping_validation.json"
        with open(validation_file, 'w') as f:
            json.dump(validation, f, indent=2)
        print(f"‚úÖ Valida√ß√£o salva: {validation_file}")

    else:
        print("‚ö†Ô∏è  Source model n√£o dispon√≠vel - state_dict n√£o ser√° salvo")
        print("   Apenas metadata espectral ser√° salva")


def main():
    parser = argparse.ArgumentParser(
        description="Convers√£o Espectral de Modelos para Œ®QRH",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Argumentos principais
    parser.add_argument(
        "--source",
        type=str,
        required=True,
        help="Fonte do modelo (nome HF, path local, URL)"
    )

    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="Diret√≥rio de sa√≠da para modelo convertido"
    )

    # Par√¢metros de convers√£o
    parser.add_argument(
        "--alpha-min",
        type=float,
        default=0.1,
        help="Valor m√≠nimo de Œ±"
    )

    parser.add_argument(
        "--alpha-max",
        type=float,
        default=3.0,
        help="Valor m√°ximo de Œ±"
    )

    parser.add_argument(
        "--lambda-coupling",
        type=float,
        default=1.0,
        help="Constante de acoplamento Œª"
    )

    parser.add_argument(
        "--use-leech",
        action="store_true",
        default=True,
        help="Usar corre√ß√£o topol√≥gica com Rede de Leech"
    )

    parser.add_argument(
        "--no-leech",
        dest="use_leech",
        action="store_false",
        help="Desabilitar corre√ß√£o de Leech"
    )

    parser.add_argument(
        "--validate-energy",
        action="store_true",
        default=True,
        help="Validar conserva√ß√£o de energia"
    )

    parser.add_argument(
        "--no-validate-energy",
        dest="validate_energy",
        action="store_false",
        help="Desabilitar valida√ß√£o energ√©tica"
    )

    parser.add_argument(
        "--device",
        type=str,
        default="cpu",
        choices=["cpu", "cuda"],
        help="Dispositivo para processamento"
    )

    parser.add_argument(
        "--target-architecture",
        type=str,
        default="PsiQRHTransformerComplete",
        choices=["PsiQRHTransformer", "PsiQRHTransformerComplete"],
        help="Arquitetura alvo Œ®QRH"
    )

    args = parser.parse_args()

    print("="*70)
    print("üöÄ CONVERS√ÉO ESPECTRAL: Modelo ‚Üí Œ®QRH")
    print("="*70)
    print(f"üì¶ Fonte: {args.source}")
    print(f"üìÅ Sa√≠da: {args.output}")
    print(f"üéØ Arquitetura: {args.target_architecture}")
    print(f"üîß Corre√ß√£o Leech: {'‚úÖ Habilitada' if args.use_leech else '‚ùå Desabilitada'}")
    print(f"‚ö° Valida√ß√£o Energia: {'‚úÖ Habilitada' if args.validate_energy else '‚ùå Desabilitada'}")
    print("="*70)

    # Carregar modelo fonte
    try:
        source_model, tokenizer = load_source_model(args.source, args.device)
    except Exception as e:
        print(f"\n‚ùå ERRO ao carregar modelo: {e}")
        sys.exit(1)

    # Criar conversor
    print("\nüîß Inicializando Spectral Converter...")
    converter = SpectralModelConverter(
        alpha_min=args.alpha_min,
        alpha_max=args.alpha_max,
        lambda_coupling=args.lambda_coupling,
        use_leech_correction=args.use_leech,
        validate_energy=args.validate_energy
    )

    # Executar convers√£o
    print("\nüî¨ Executando Convers√£o F√≠sica (5 passos)...")
    try:
        if isinstance(source_model, dict):
            # Se for state_dict, converter diretamente usando spectral analysis
            print("üîÑ Convertendo state_dict usando an√°lise espectral...")

            # Criar conversor
            converter = SpectralModelConverter(
                alpha_min=args.alpha_min,
                alpha_max=args.alpha_max,
                lambda_coupling=args.lambda_coupling,
                use_leech_correction=args.use_leech,
                validate_energy=args.validate_energy
            )

            # Converter state_dict
            report = converter.convert_state_dict(
                source_model,
                target_architecture=args.target_architecture
            )
        else:
            # Modelo completo
            report = converter.convert_model(
                source_model,
                target_architecture=args.target_architecture
            )

    except Exception as e:
        print(f"\n‚ùå ERRO durante convers√£o: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    # Salvar modelo convertido
    print("\nüíæ Salvando Modelo Convertido...")
    output_path = Path(args.output)

    try:
        source_info = {
            'model_type': source_model.__class__.__name__ if hasattr(source_model, '__class__') else 'unknown',
            'source': args.source,
            'source_model': source_model  # ‚Üê modelo fonte
        }

        save_converted_model(report, output_path, source_info)

    except Exception as e:
        print(f"\n‚ùå ERRO ao salvar: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    # Resumo final
    print("\n" + "="*70)
    print("‚úÖ CONVERS√ÉO CONCLU√çDA COM SUCESSO!")
    print("="*70)
    print(f"üìä Dimens√£o Fractal M√©dia: {report['avg_fractal_dim']:.4f}")
    print(f"‚ö° Alpha M√©dio: {report['avg_alpha']:.4f}")
    print(f"üìä Camadas Convertidas: {report['n_layers_analyzed']}")
    print(f"üìÅ Modelo salvo em: {output_path}")
    print("="*70)

    print("\nüí° Pr√≥ximos passos:")
    print(f"   1. Treinar: python3 train_psiqrh_native.py --output_dir {output_path} --use_complete")
    print(f"   2. Validar: python3 validate_training_output.py --model_dir {output_path}")
    print(f"   3. Certificar: make model-certify MODEL={output_path.name}")
    print(f"   4. Ativar: make model-set-active MODEL={output_path.name}")


if __name__ == "__main__":
    main()
