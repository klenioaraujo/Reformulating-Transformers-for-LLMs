#!/usr/bin/env python3
"""
Autonomous Spectral Calibrator - Sistema de Auto-Acoplamento Espectral
=====================================================================

Implementa sistema de auto-acoplamento espectral din√¢mico que integra:
1. Calibra√ß√£o FCI com dados Œ®TWS
2. Convers√£o de embedding com modula√ß√£o sem√¢ntica
3. Auto-acoplamento espectral para diversifica√ß√£o de tokens

Baseado no padr√£o: Da Calibra√ß√£o √† Convers√£o F√≠sica
"""

import torch
import torch.nn as nn
import numpy as np
import json
import yaml
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass


@dataclass
class SemanticCategory:
    """Categoria sem√¢ntica para modula√ß√£o de embedding"""
    name: str
    target_fci: float
    alpha_modulation: float
    description: str


class AutonomousSpectralCalibrator:
    """
    Sistema de auto-acoplamento espectral din√¢mico

    Integra calibra√ß√£o FCI com convers√£o de embedding e auto-acoplamento
    para gerar tokens diversos via resson√¢ncia f√≠sica.
    """

    def __init__(self, config_path: str = None):
        """
        Args:
            config_path: Caminho para configura√ß√£o de calibra√ß√£o
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Carregar configura√ß√£o de calibra√ß√£o
        if config_path:
            self.calibration_config = self._load_calibration_config(config_path)
        else:
            self.calibration_config = self._load_default_calibration()

        # Inicializar categorias sem√¢nticas
        self.semantic_categories = self._initialize_semantic_categories()

        # Par√¢metros de auto-acoplamento
        self.alpha_range = (0.1, 3.0)
        self.beta_range = (0.5, 1.5)
        self.coupling_strength = 1.0

        print("üöÄ Sistema de Auto-Acoplamento Espectral Inicializado")
        print(f"üìä Categorias sem√¢nticas: {len(self.semantic_categories)}")
        print(f"üîß Configura√ß√£o: {self.calibration_config.get('state_thresholds', {})}")

    def _load_calibration_config(self, config_path: str) -> Dict:
        """Carrega configura√ß√£o de calibra√ß√£o FCI"""
        try:
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            print(f"‚úÖ Configura√ß√£o de calibra√ß√£o carregada: {config_path}")
            return config
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro ao carregar configura√ß√£o: {e}")
            return self._load_default_calibration()

    def _load_default_calibration(self) -> Dict:
        """Carrega configura√ß√£o padr√£o de calibra√ß√£o"""
        return {
            'state_thresholds': {
                'emergence': {'min_fci': 0.644},
                'meditation': {'min_fci': 0.636},
                'analysis': {'min_fci': 0.620}
            }
        }

    def _initialize_semantic_categories(self) -> Dict[str, SemanticCategory]:
        """Inicializa categorias sem√¢nticas baseadas na calibra√ß√£o"""
        thresholds = self.calibration_config['state_thresholds']

        return {
            'creative': SemanticCategory(
                name='creative',
                target_fci=thresholds['emergence']['min_fci'],
                alpha_modulation=1.2,  # Œ± mais alto para criatividade
                description='Estados criativos e emergentes'
            ),
            'analytical': SemanticCategory(
                name='analytical',
                target_fci=thresholds['analysis']['min_fci'],
                alpha_modulation=0.8,  # Œ± mais baixo para an√°lise
                description='Estados anal√≠ticos e focados'
            ),
            'meditative': SemanticCategory(
                name='meditative',
                target_fci=thresholds['meditation']['min_fci'],
                alpha_modulation=1.0,  # Œ± neutro para medita√ß√£o
                description='Estados meditativos e introspectivos'
            ),
            'neutral': SemanticCategory(
                name='neutral',
                target_fci=0.63,  # Valor intermedi√°rio
                alpha_modulation=1.0,
                description='Estados neutros e balanceados'
            )
        }

    def fci_to_alpha(self, target_fci: float, fractal_dim: float) -> float:
        """
        Converte FCI alvo para Œ± usando rela√ß√£o f√≠sica

        F√≥rmula: Œ±_target = Œ±‚ÇÄ * (1 + Œª * (D - D_eucl)/D_eucl)
        onde Œ±‚ÇÄ √© derivado do FCI alvo
        """
        # Mapear FCI para Œ± base (rela√ß√£o linear simplificada)
        alpha_base = 0.5 + (target_fci - 0.5) * 2.0  # FCI 0.5 ‚Üí Œ± 0.5, FCI 0.8 ‚Üí Œ± 1.1

        # Aplicar modula√ß√£o por dimens√£o fractal
        d_eucl = 1.0
        alpha_target = alpha_base * (1.0 + self.coupling_strength * (fractal_dim - d_eucl) / d_eucl)

        # Limitar ao intervalo permitido
        alpha_target = np.clip(alpha_target, self.alpha_range[0], self.alpha_range[1])

        return float(alpha_target)

    def modulate_embedding_with_calibration(
        self,
        embedding_weights: torch.Tensor,
        semantic_category: str = 'neutral',
        fractal_dim: float = 1.5
    ) -> Tuple[torch.Tensor, Dict]:
        """
        Modula embedding com base na calibra√ß√£o FCI

        Args:
            embedding_weights: Pesos de embedding originais
            semantic_category: Categoria sem√¢ntica
            fractal_dim: Dimens√£o fractal do contexto

        Returns:
            Tuple: (embedding_modulado, metadata)
        """
        print(f"üîß Modulando embedding para categoria: {semantic_category}")

        # Obter categoria sem√¢ntica
        category = self.semantic_categories.get(semantic_category, self.semantic_categories['neutral'])

        # Calcular Œ± geom√©trico puro
        alpha_geometric = self._compute_geometric_alpha(embedding_weights, fractal_dim)

        # Calcular Œ± calibrado baseado no FCI alvo
        alpha_calibrated = self.fci_to_alpha(category.target_fci, fractal_dim)

        # Interpolar entre Œ± geom√©trico e Œ± calibrado
        alpha_final = (0.7 * alpha_geometric + 0.3 * alpha_calibrated) * category.alpha_modulation
        alpha_final = np.clip(alpha_final, self.alpha_range[0], self.alpha_range[1])

        # Aplicar modula√ß√£o aos pesos de embedding
        modulated_weights = self._apply_alpha_modulation(embedding_weights, alpha_final)

        metadata = {
            'semantic_category': semantic_category,
            'target_fci': category.target_fci,
            'alpha_geometric': alpha_geometric,
            'alpha_calibrated': alpha_calibrated,
            'alpha_final': alpha_final,
            'fractal_dim': fractal_dim
        }

        print(f"   ‚Ä¢ Œ± geom√©trico: {alpha_geometric:.4f}")
        print(f"   ‚Ä¢ Œ± calibrado: {alpha_calibrated:.4f}")
        print(f"   ‚Ä¢ Œ± final: {alpha_final:.4f}")
        print(f"   ‚Ä¢ FCI alvo: {category.target_fci:.4f}")

        return modulated_weights, metadata

    def _compute_geometric_alpha(self, weights: torch.Tensor, fractal_dim: float) -> float:
        """Calcula Œ± puramente geom√©trico baseado na dimens√£o fractal"""
        alpha_0 = 1.0  # Valor central
        d_eucl = 1.0

        alpha_geometric = alpha_0 * (1.0 + self.coupling_strength * (fractal_dim - d_eucl) / d_eucl)
        return np.clip(alpha_geometric, self.alpha_range[0], self.alpha_range[1])

    def _apply_alpha_modulation(self, weights: torch.Tensor, alpha: float) -> torch.Tensor:
        """Aplica modula√ß√£o de Œ± aos pesos de embedding"""
        # Transformada de Fourier
        weights_fft = torch.fft.fft(weights, dim=-1)

        # Criar filtro espectral dependente de Œ±
        k = torch.arange(weights_fft.shape[-1], device=self.device, dtype=torch.float32)

        # Filtro: exp(iŒ±¬∑GELU(norm(ln(|k|+Œµ))))
        k_filter = torch.exp(
            1j * alpha * torch.nn.functional.gelu(
                torch.nn.functional.layer_norm(
                    torch.log(torch.abs(k) + 1e-8),
                    [k.shape[-1]]
                )
            )
        )

        # Aplicar filtro
        weights_filtered = weights_fft * k_filter

        # Transformada inversa
        modulated_weights = torch.fft.ifft(weights_filtered, dim=-1).real

        return modulated_weights

    def spectral_auto_coupling(
        self,
        psi_state: torch.Tensor,
        alpha: float,
        vocab_size: int,
        coupling_iterations: int = 3
    ) -> Tuple[int, List[float]]:
        """
        Auto-acoplamento espectral para diversifica√ß√£o de tokens

        Args:
            psi_state: Estado quaterni√¥nico
            alpha: Par√¢metro Œ± atual
            vocab_size: Tamanho do vocabul√°rio
            coupling_iterations: N√∫mero de itera√ß√µes de acoplamento

        Returns:
            Tuple: (token_ressonante, espectro_de_resson√¢ncia)
        """
        print(f"üîó Aplicando auto-acoplamento espectral ({coupling_iterations} itera√ß√µes)...")

        # Par√¢metros da sonda √≥ptica
        I0 = 1.0
        omega = 2 * np.pi
        t = 0.0

        # Espectro de resson√¢ncia acumulado
        resonance_accumulator = np.zeros(min(vocab_size, 100))

        for iteration in range(coupling_iterations):
            # Variar Œ± levemente para cada itera√ß√£o
            alpha_iter = alpha * (0.9 + 0.2 * np.random.random())
            beta_iter = alpha_iter / 2.0

            # Calcular espectro de resson√¢ncia para esta itera√ß√£o
            resonance_spectrum = []

            for lambda_token in range(len(resonance_accumulator)):
                # Equa√ß√£o de Padilha: f(Œª,t) = I‚ÇÄ sin(œât + Œ±Œª) ¬∑ e^(i(œât - kŒª + Œ≤Œª¬≤))
                phase = omega * t + alpha_iter * lambda_token
                f_lambda = I0 * np.sin(phase) * np.exp(
                    1j * (omega * t - 1.0 * lambda_token + beta_iter * lambda_token**2)
                )

                # Acoplamento: |‚ü®f(Œª,t), Œ®‚ü©|¬≤
                psi_mean = psi_state.mean().item()
                coupling = np.abs(f_lambda * psi_mean)**2

                resonance_spectrum.append(coupling)

            # Acumular espectro
            resonance_accumulator += np.array(resonance_spectrum)

            print(f"   ‚Ä¢ Itera√ß√£o {iteration+1}: Œ±={alpha_iter:.4f}, Œ≤={beta_iter:.4f}")

        # Normalizar espectro acumulado
        resonance_accumulator /= coupling_iterations

        # Encontrar token com m√°xima resson√¢ncia
        lambda_star = int(np.argmax(resonance_accumulator))
        max_resonance = resonance_accumulator[lambda_star]

        # Evitar token 0 (espa√ßo) se poss√≠vel
        if lambda_star == 0 and len(resonance_accumulator) > 1:
            resonance_copy = resonance_accumulator.copy()
            resonance_copy[0] = 0.0
            lambda_star = int(np.argmax(resonance_copy))
            max_resonance = resonance_accumulator[lambda_star]

        print(f"   ‚úÖ Token ressonante: Œª* = {lambda_star} (resson√¢ncia = {max_resonance:.6f})")

        return lambda_star, resonance_accumulator.tolist()

    def process_with_auto_coupling(
        self,
        input_text: str,
        embedding_weights: torch.Tensor,
        semantic_category: str = 'neutral'
    ) -> Dict:
        """
        Processa texto com auto-acoplamento espectral completo

        Args:
            input_text: Texto de entrada
            embedding_weights: Pesos de embedding
            semantic_category: Categoria sem√¢ntica

        Returns:
            Dict com resultados do processamento
        """
        print(f"\n{'='*70}")
        print(f"üì• PROCESSANDO COM AUTO-ACOPLAMENTO: '{input_text}'")
        print(f"{'='*70}")

        # 1. Estimar dimens√£o fractal do contexto
        fractal_dim = self._estimate_context_fractal_dim(input_text)

        # 2. Modular embedding com calibra√ß√£o
        modulated_embedding, modulation_metadata = self.modulate_embedding_with_calibration(
            embedding_weights, semantic_category, fractal_dim
        )

        # 3. Criar embedding quaterni√¥nico
        psi_state = self._create_quaternion_embedding(input_text, modulated_embedding)

        # 4. Aplicar auto-acoplamento espectral
        vocab_size = len(self.semantic_categories) * 10  # Simplifica√ß√£o
        next_token, resonance_spectrum = self.spectral_auto_coupling(
            psi_state, modulation_metadata['alpha_final'], vocab_size
        )

        # 5. Gerar texto
        generated_text = self._generate_text_from_token(next_token)

        result = {
            'input': input_text,
            'generated_text': generated_text,
            'next_token': next_token,
            'semantic_category': semantic_category,
            'fractal_dim': fractal_dim,
            'alpha_final': modulation_metadata['alpha_final'],
            'target_fci': modulation_metadata['target_fci'],
            'resonance_spectrum': resonance_spectrum,
            'modulation_metadata': modulation_metadata
        }

        print(f"\n{'='*70}")
        print("‚úÖ PROCESSAMENTO COM AUTO-ACOPLAMENTO CONCLU√çDO")
        print(f"{'='*70}")
        print(f"üì• Input: '{input_text}'")
        print(f"üì§ Output: '{generated_text}'")
        print(f"üî¨ Categoria: {semantic_category}")
        print(f"üìä FCI alvo: {result['target_fci']:.4f}")
        print(f"üîß Œ± final: {result['alpha_final']:.4f}")

        return result

    def _estimate_context_fractal_dim(self, text: str) -> float:
        """Estima dimens√£o fractal do contexto textual"""
        # Simplifica√ß√£o: usar comprimento do texto como proxy
        text_length = len(text)

        # Dimens√£o fractal estimada baseada em complexidade
        if text_length < 10:
            return 1.2  # Contexto simples
        elif text_length < 50:
            return 1.5  # Contexto m√©dio
        else:
            return 1.8  # Contexto complexo

    def _create_quaternion_embedding(self, text: str, embedding_weights: torch.Tensor) -> torch.Tensor:
        """Cria embedding quaterni√¥nico do texto"""
        # Simplifica√ß√£o: usar embedding m√©dio
        batch_size = 1
        seq_len = len(text)
        embed_dim = embedding_weights.shape[-1]

        # Criar tensor simulado
        psi_state = torch.randn(batch_size, seq_len, embed_dim, device=self.device)

        return psi_state

    def _generate_text_from_token(self, token_idx: int) -> str:
        """Gera texto a partir do token ressonante"""
        # Mapeamento simples token ‚Üí caractere
        chars = 'abcdefghijklmnopqrstuvwxyz '
        char_idx = token_idx % len(chars)

        return chars[char_idx] * 3  # Repetir caractere para demo


def main():
    """Demonstra√ß√£o do sistema de auto-acoplamento"""
    print("üöÄ DEMONSTRA√á√ÉO: Sistema de Auto-Acoplamento Espectral")
    print("=" * 70)

    # Inicializar calibrador
    calibrator = AutonomousSpectralCalibrator('calibrated_fci_thresholds.yaml')

    # Criar embedding de exemplo
    vocab_size = 100
    embed_dim = 256
    example_embedding = torch.randn(vocab_size, embed_dim)

    # Testar com diferentes categorias sem√¢nticas
    test_cases = [
        ("Hello world", "creative"),
        ("Quantum physics", "analytical"),
        ("Meditation", "meditative"),
        ("Test", "neutral")
    ]

    results = []

    for input_text, category in test_cases:
        result = calibrator.process_with_auto_coupling(
            input_text, example_embedding, category
        )
        results.append(result)

    # Salvar resultados
    output_file = "auto_coupling_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nüìÅ Resultados salvos em: {output_file}")
    print("‚úÖ Demonstra√ß√£o conclu√≠da!")


if __name__ == "__main__":
    main()