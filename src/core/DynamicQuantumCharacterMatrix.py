#!/usr/bin/env python3
"""
DynamicQuantumCharacterMatrix - Matriz qu√¢ntica din√¢mica baseada em caracteres

Implementa uma matriz qu√¢ntica que opera no n√≠vel de caracteres em vez de palavras,
permitindo gera√ß√£o mais granular e controle preciso sobre a sa√≠da de texto.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Dict, Any, Optional, List, Tuple


class DynamicQuantumCharacterMatrix(nn.Module):
    """
    Matriz qu√¢ntica din√¢mica baseada em caracteres.

    Esta matriz opera no n√≠vel de caracteres individuais, permitindo:
    - Gera√ß√£o mais granular de texto
    - Melhor controle sobre caracteres especiais
    - Vocabul√°rio menor e mais eficiente
    - Representa√ß√£o qu√¢ntica de caracteres ASCII/Unicode
    """

    def __init__(self,
                 char_vocab_size: int = 256,  # ASCII b√°sico + caracteres especiais
                 hidden_size: int = 256,      # Deve ser m√∫ltiplo de 4 para quaternions
                 device: str = 'cpu'):

        super().__init__()
        self.char_vocab_size = char_vocab_size
        self.hidden_size = hidden_size
        self.device = device

        # Verificar se hidden_size √© m√∫ltiplo de 4 para quaternions
        if hidden_size % 4 != 0:
            raise ValueError(f"hidden_size deve ser m√∫ltiplo de 4 para quaternions, recebido: {hidden_size}")

        # Dimens√£o quaterni√¥nica
        self.quaternion_dim = hidden_size // 4

        # Embeddings qu√¢nticos de caracteres
        self.char_embeddings = nn.Embedding(char_vocab_size, hidden_size)

        # Camada de rota√ß√£o SO(4) para quaternions
        self.rotation_layer = nn.Linear(hidden_size, hidden_size, bias=False)

        # Inicializar pesos com distribui√ß√£o normal
        self._initialize_weights()

        print(f"üî¨ Dynamic Quantum Character Matrix inicializada")
        print(f"   üìä Vocab: {char_vocab_size} caracteres, Hidden: {hidden_size} (quaternion_dim: {self.quaternion_dim})")
        print(f"   üîÑ Camada de rota√ß√£o SO(4): Implementada com multiplica√ß√£o quaterni√¥nica")

    def _initialize_weights(self):
        """Inicializa os pesos da matriz com distribui√ß√£o normal."""
        # Inicializar embeddings de caracteres
        nn.init.normal_(self.char_embeddings.weight, mean=0.0, std=0.02)

        # Inicializar camada de rota√ß√£o
        nn.init.orthogonal_(self.rotation_layer.weight)

    def _quaternion_multiply(self, q1: torch.Tensor, q2: torch.Tensor) -> torch.Tensor:
        """
        Multiplica√ß√£o quaterni√¥nica para dois tensores.

        Args:
            q1: Primeiro quaternion [batch_size, hidden_size]
            q2: Segundo quaternion [batch_size, hidden_size]

        Returns:
            Produto quaterni√¥nico [batch_size, hidden_size]
        """
        # Reorganizar para dimens√£o quaterni√¥nica
        q1_reshaped = q1.view(-1, self.quaternion_dim, 4)
        q2_reshaped = q2.view(-1, self.quaternion_dim, 4)

        # Extrair componentes
        a1, b1, c1, d1 = q1_reshaped.unbind(dim=2)
        a2, b2, c2, d2 = q2_reshaped.unbind(dim=2)

        # Multiplica√ß√£o quaterni√¥nica
        a = a1 * a2 - b1 * b2 - c1 * c2 - d1 * d2
        b = a1 * b2 + b1 * a2 + c1 * d2 - d1 * c2
        c = a1 * c2 - b1 * d2 + c1 * a2 + d1 * b2
        d = a1 * d2 + b1 * c2 - c1 * b2 + d1 * a2

        # Combinar componentes
        result = torch.stack([a, b, c, d], dim=2)
        return result.view(-1, self.hidden_size)

    def forward(self, char_ids: torch.Tensor) -> torch.Tensor:
        """
        Forward pass da matriz qu√¢ntica de caracteres.

        Args:
            char_ids: IDs de caracteres [batch_size, seq_len]

        Returns:
            Representa√ß√µes qu√¢nticas [batch_size, seq_len, hidden_size]
        """
        batch_size, seq_len = char_ids.shape

        # Obter embeddings de caracteres
        char_embeds = self.char_embeddings(char_ids)  # [batch_size, seq_len, hidden_size]

        # Aplicar rota√ß√£o quaterni√¥nica
        rotated_embeds = self.rotation_layer(char_embeds)

        # Aplicar multiplica√ß√£o quaterni√¥nica
        char_embeds_flat = char_embeds.view(-1, self.hidden_size)
        rotated_embeds_flat = rotated_embeds.view(-1, self.hidden_size)

        quantum_embeds = self._quaternion_multiply(char_embeds_flat, rotated_embeds_flat)
        quantum_embeds = quantum_embeds.view(batch_size, seq_len, self.hidden_size)

        return quantum_embeds

    def get_character_vocabulary(self) -> Dict[str, int]:
        """
        Retorna o vocabul√°rio de caracteres padr√£o.

        Returns:
            Dicion√°rio mapeando caracteres para IDs
        """
        # Caracteres ASCII b√°sicos + caracteres especiais
        char_vocab = {}

        # Caracteres ASCII imprim√≠veis (32-126) - come√ßando do ID 1
        for i in range(32, 127):
            char_vocab[chr(i)] = i - 31  # IDs de 1 a 95

        # Adicionar caracteres de controle importantes com IDs espec√≠ficos
        control_chars = {
            chr(0): 0,   # Null character
            chr(1): 1,   # Start of header
            chr(10): 96, # New line (\n)
            chr(9): 97,  # Tab (\t)
            chr(32): 98  # Space
        }
        char_vocab.update(control_chars)

        # Caracteres especiais adicionais - continuando dos IDs
        special_chars = {
            '√°': 99, '√©': 100, '√≠': 101, '√≥': 102, '√∫': 103,  # Acentos
            '√†': 104, '√®': 105, '√¨': 106, '√≤': 107, '√π': 108,
            '√¢': 109, '√™': 110, '√Æ': 111, '√¥': 112, '√ª': 113,
            '√£': 114, '√µ': 115, '√ß': 116,
            '√Å': 117, '√â': 118, '√ç': 119, '√ì': 120, '√ö': 121,
            '√Ä': 122, '√à': 123, '√å': 124, '√í': 125, '√ô': 126,
            '√Ç': 127, '√ä': 128, '√é': 129, '√î': 130, '√õ': 131,
            '√É': 132, '√ï': 133, '√á': 134
        }

        char_vocab.update(special_chars)

        # Garantir que n√£o exceda o tamanho do vocabul√°rio
        if len(char_vocab) > self.char_vocab_size:
            # Manter apenas os primeiros char_vocab_size caracteres
            char_vocab = dict(list(char_vocab.items())[:self.char_vocab_size])

        return char_vocab

    def encode_text(self, text: str) -> torch.Tensor:
        """
        Codifica texto em IDs de caracteres.

        Args:
            text: Texto para codificar

        Returns:
            Tensor de IDs de caracteres [1, seq_len]
        """
        char_vocab = self.get_character_vocabulary()

        # Converter texto para IDs
        char_ids = []
        for char in text:
            if char in char_vocab:
                char_ids.append(char_vocab[char])
            else:
                # Usar espa√ßo como fallback para caracteres desconhecidos
                char_ids.append(char_vocab.get(' ', 97))

        return torch.tensor([char_ids], dtype=torch.long, device=self.device)

    def decode_text(self, char_ids: torch.Tensor) -> str:
        """
        Decodifica IDs de caracteres em texto.

        Args:
            char_ids: Tensor de IDs de caracteres [batch_size, seq_len]

        Returns:
            Texto decodificado
        """
        char_vocab = self.get_character_vocabulary()

        # Inverter mapeamento
        id_to_char = {v: k for k, v in char_vocab.items()}

        # Converter IDs para texto
        text_chars = []
        for char_id in char_ids.cpu().numpy().flatten():
            if char_id in id_to_char:
                text_chars.append(id_to_char[char_id])
            else:
                text_chars.append('?')

        return ''.join(text_chars)

    def get_character_embeddings(self) -> torch.Tensor:
        """
        Retorna os embeddings de caracteres.

        Returns:
            Tensor de embeddings [char_vocab_size, hidden_size]
        """
        return self.char_embeddings.weight

    def analyze_character_distribution(self, text: str) -> Dict[str, Any]:
        """
        Analisa a distribui√ß√£o de caracteres no texto.

        Args:
            text: Texto para analisar

        Returns:
            Dicion√°rio com estat√≠sticas de caracteres
        """
        char_vocab = self.get_character_vocabulary()

        # Contar frequ√™ncia de caracteres
        char_counts = {}
        for char in text:
            if char in char_vocab:
                char_counts[char] = char_counts.get(char, 0) + 1

        # Calcular estat√≠sticas
        total_chars = len(text)
        unique_chars = len(char_counts)

        return {
            'total_characters': total_chars,
            'unique_characters': unique_chars,
            'character_frequencies': char_counts,
            'vocabulary_coverage': unique_chars / len(char_vocab) if char_vocab else 0
        }