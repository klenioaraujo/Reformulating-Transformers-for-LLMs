#!/usr/bin/env python3
"""
Embedding Spectral Converter - Convers√£o F√≠sica de Embeddings para Œ®QRH
=========================================================================

Converte embedding layer do GPT-2 (W_e ‚àà ‚Ñù^{V√ód}) para embeddings
quaterni√¥nicos (Œ®_e ‚àà ‚Ñç^{V√ód/4}) preservando sem√¢ntica atrav√©s de
an√°lise espectral por token.

Pipeline:
1. Para cada token i ‚àà [0, V):
   - Calcular FFT: ·∫Ω·µ¢ = F(e·µ¢)
   - Espectro: P·µ¢(k) = |·∫Ω·µ¢(k)|¬≤
   - Lei de pot√™ncia: P·µ¢(k) ~ k^(-Œ≤·µ¢)
   - Dimens√£o fractal: D·µ¢ = (3-Œ≤·µ¢)/2
   - Fase dominante: Œ∏·µ¢ = arg(·∫Ω·µ¢(k_dom))

2. Mapear e·µ¢ ‚Üí Œ®·µ¢ usando rota√ß√£o quaterni√¥nica baseada em D·µ¢ e Œ∏·µ¢

3. Salvar vocabul√°rio e tokenizer do GPT-2

Copyright (C) 2025 Klenio Araujo Padilha
Licensed under GNU GPLv3
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Tuple, Optional
import json
from pathlib import Path
from tqdm import tqdm
from collections import Counter


def fci_to_alpha(target_fci: float, fractal_dim: float, alpha_min: float = 0.1, alpha_max: float = 3.0) -> float:
    """
    Converte FCI alvo para Œ± usando rela√ß√£o f√≠sica.

    Args:
        target_fci: FCI alvo
        fractal_dim: Dimens√£o fractal
        alpha_min: Œ± m√≠nimo
        alpha_max: Œ± m√°ximo

    Returns:
        Œ± calibrado
    """
    # Mapear FCI para Œ± base (rela√ß√£o linear simplificada)
    alpha_base = 0.5 + (target_fci - 0.5) * 2.0  # FCI 0.5 ‚Üí Œ± 0.5, FCI 0.8 ‚Üí Œ± 1.1

    # Aplicar modula√ß√£o por dimens√£o fractal
    d_eucl = 1.0
    lambda_coupling = 1.0
    alpha_target = alpha_base * (1.0 + lambda_coupling * (fractal_dim - d_eucl) / d_eucl)

    # Limitar ao intervalo permitido
    alpha_target = np.clip(alpha_target, alpha_min, alpha_max)

    return float(alpha_target)


def fit_power_law_exponent(power_spectrum: torch.Tensor) -> float:
    """
    Ajusta lei de pot√™ncia P(k) ~ k^(-Œ≤) no espectro.

    Args:
        power_spectrum: Espectro de pot√™ncia |F(x)|¬≤

    Returns:
        Expoente Œ≤ da lei de pot√™ncia
    """
    # Converter para numpy
    ps = power_spectrum.cpu().numpy()

    # Frequ√™ncias
    k = np.arange(1, len(ps) + 1)

    # Remover zeros e valores muito pequenos
    valid_mask = ps > 1e-12
    k_valid = k[valid_mask]
    ps_valid = ps[valid_mask]

    if len(k_valid) < 10:
        # Poucos pontos v√°lidos, usar valor padr√£o
        return 1.0

    # Log-log space
    log_k = np.log(k_valid)
    log_ps = np.log(ps_valid + 1e-12)

    # Regress√£o linear
    try:
        coeffs = np.polyfit(log_k, log_ps, 1)
        beta = -coeffs[0]  # Inclina√ß√£o negativa
        return float(np.clip(beta, 0.5, 2.5))
    except:
        return 1.0


def spectral_quaternion_map(
    embedding: torch.Tensor,
    fractal_dim: float,
    theta: float,
    alpha: float
) -> torch.Tensor:
    """
    Mapeia embedding cl√°ssico para quaterni√¥nico usando par√¢metros espectrais.

    Transforma√ß√£o:
    e ‚àà ‚Ñù^d ‚Üí Œ® ‚àà ‚Ñç^{d/4}

    Args:
        embedding: Vetor de embedding [d]
        fractal_dim: Dimens√£o fractal D
        theta: Fase dominante Œ∏
        alpha: Par√¢metro espectral Œ±

    Returns:
        Embedding quaterni√¥nico [d/4, 4]
    """
    d = embedding.shape[0]
    assert d % 4 == 0, f"Dimens√£o {d} n√£o √© divis√≠vel por 4"

    # Reshape em grupos de 4 (componentes quaterni√¥nicos)
    # [d] ‚Üí [d/4, 4]
    quat_groups = embedding.reshape(-1, 4)

    # Normalizar cada quaternion
    norms = torch.norm(quat_groups, dim=-1, keepdim=True)
    quat_normalized = quat_groups / (norms + 1e-8)

    # Aplicar rota√ß√£o baseada em theta e alpha
    # q_rot = [cos(Œ∏/2), sin(Œ∏/2), 0, 0]
    half_theta = theta / 2.0
    q_rot = torch.tensor([
        np.cos(half_theta),
        np.sin(half_theta),
        0.0,
        0.0
    ], dtype=embedding.dtype, device=embedding.device)

    # Rota√ß√£o quaterni√¥nica simplificada
    # q' = q_rot * q (multiplica√ß√£o quaterni√¥nica aproximada)
    alpha_scale = torch.clamp(torch.tensor(alpha / 3.0), 0.0, 1.0)

    quat_rotated = (
        (1.0 - alpha_scale) * quat_normalized +
        alpha_scale * (quat_normalized * q_rot[0] + torch.roll(quat_normalized, 1, dims=-1) * q_rot[1])
    )

    # Re-normalizar
    quat_final_norms = torch.norm(quat_rotated, dim=-1, keepdim=True)
    quat_final = quat_rotated / (quat_final_norms + 1e-8)

    # Re-escalar para norma original (conserva√ß√£o de energia)
    quat_final = quat_final * norms

    return quat_final


def convert_gpt2_embedding_to_psiqrh(
    gpt2_embedding_weight: torch.Tensor,
    calibration_config: Dict = None,
    semantic_categories: Dict = None,
    alpha_min: float = 0.1,
    alpha_max: float = 3.0,
    lambda_coupling: float = 1.0,
    d_euclidean: float = 1.0,
    verbose: bool = True
) -> Tuple[torch.Tensor, Dict]:
    """
    Converte embedding do GPT-2 para Œ®QRH quaterni√¥nico com calibra√ß√£o FCI.

    W_e ‚àà ‚Ñù^{V√ód} ‚Üí Œ®_e ‚àà ‚Ñç^{V√ód/4}

    Args:
        gpt2_embedding_weight: Embedding GPT-2 [vocab_size, d_model]
        calibration_config: Configura√ß√£o de calibra√ß√£o FCI
        semantic_categories: Categorias sem√¢nticas para modula√ß√£o
        alpha_min: Œ± m√≠nimo
        alpha_max: Œ± m√°ximo
        lambda_coupling: Constante de acoplamento
        d_euclidean: Dimens√£o euclidiana de refer√™ncia
        verbose: Mostrar progresso

    Returns:
        Tuple (psi_embeddings, metadata)
    """
    V, d = gpt2_embedding_weight.shape

    if verbose:
        print(f"\nüîÑ Convertendo embedding GPT-2 ‚Üí Œ®QRH quaterni√¥nico")
        print(f"   ‚Ä¢ Vocabul√°rio: {V:,} tokens")
        print(f"   ‚Ä¢ Dimens√£o original: {d}")
        print(f"   ‚Ä¢ Dimens√£o quaterni√¥nica: {d//4} (4 componentes)")
        if calibration_config:
            print(f"   ‚Ä¢ Modula√ß√£o FCI: HABILITADA")
        if semantic_categories:
            print(f"   ‚Ä¢ Categorias sem√¢nticas: {len(semantic_categories)}")

    assert d % 4 == 0, f"Dimens√£o {d} n√£o √© divis√≠vel por 4"

    # Armazenar embeddings quaterni√¥nicos
    psi_embeddings = []

    # Metadados por token
    token_metadata = {
        'betas': [],
        'fractal_dims': [],
        'thetas': [],
        'alphas': []
    }

    # Processar cada token
    iterator = tqdm(range(V), desc="Converting tokens") if verbose else range(V)

    for i in iterator:
        e_i = gpt2_embedding_weight[i]  # [d]

        # 1. FFT
        fft_e = torch.fft.fft(e_i)

        # 2. Espectro de pot√™ncia (s√≥ frequ√™ncias positivas)
        power = torch.abs(fft_e[:len(fft_e)//2])**2

        # 3. Ajustar lei de pot√™ncia ‚Üí Œ≤ ‚Üí D
        beta_i = fit_power_law_exponent(power)
        D_i = (3.0 - beta_i) / 2.0
        D_i = np.clip(D_i, 1.0, 2.0)

        # 4. Fase dominante
        dominant_idx = torch.argmax(torch.abs(fft_e[:len(fft_e)//2]))
        theta_i = float(torch.angle(fft_e[dominant_idx]))

        # 5. Mapear D ‚Üí Œ± (com modula√ß√£o FCI se dispon√≠vel)
        alpha_0 = (alpha_min + alpha_max) / 2.0
        alpha_geometric = alpha_0 * (1.0 + lambda_coupling * (D_i - d_euclidean) / d_euclidean)

        # Aplicar modula√ß√£o FCI se dispon√≠vel
        if calibration_config and semantic_categories:
            # Obter categoria sem√¢ntica do token (simplificado)
            token_category = semantic_categories.get(str(i), 'neutral')
            if token_category in calibration_config.get('state_thresholds', {}):
                target_fci = calibration_config['state_thresholds'][token_category]['min_fci']
                # Converter FCI alvo para Œ±
                alpha_calibrated = fci_to_alpha(target_fci, D_i, alpha_min, alpha_max)
                # Interpolar entre Œ± geom√©trico e Œ± calibrado
                alpha_i = 0.7 * alpha_geometric + 0.3 * alpha_calibrated
                if verbose and i % 1000 == 0:
                    print(f"   ‚Ä¢ Token {i}: {token_category} ‚Üí FCI={target_fci:.3f}, Œ±={alpha_i:.3f}")
            else:
                alpha_i = alpha_geometric
        else:
            alpha_i = alpha_geometric

        alpha_i = np.clip(alpha_i, alpha_min, alpha_max)

        # 6. Mapear para quaterni√£o
        psi_i = spectral_quaternion_map(e_i, D_i, theta_i, alpha_i)

        psi_embeddings.append(psi_i)

        # Salvar metadata
        token_metadata['betas'].append(float(beta_i))
        token_metadata['fractal_dims'].append(float(D_i))
        token_metadata['thetas'].append(float(theta_i))
        token_metadata['alphas'].append(float(alpha_i))

        # Salvar categoria sem√¢ntica se dispon√≠vel
        if semantic_categories:
            token_category = semantic_categories.get(str(i), 'neutral')
            token_metadata.setdefault('semantic_categories', []).append(token_category)

    # Stack em tensor [V, d/4, 4]
    psi_embeddings_tensor = torch.stack(psi_embeddings)

    # üîë APLICA√á√ÉO √öNICA DA LEI DE BENFORD: AUDITORIA E CORRE√á√ÉO
    if verbose:
        print("\nüîç Aplicando auditoria espectral Benford ao embedding convertido...")
    audit = benford_spectral_audit(psi_embeddings_tensor)

    if verbose:
        print(f"   ‚Ä¢ Conformidade Benford: {audit['benford_conformity']:.4f}")

    if not audit['is_conformant']:
        if verbose:
            print("üîß Embedding n√£o conforme. Aplicando re-normaliza√ß√£o Benford...")
        psi_embeddings_tensor = spectral_benford_renormalization(psi_embeddings_tensor)
        # Re-auditar para confirmar
        audit = benford_spectral_audit(psi_embeddings_tensor)
        if verbose:
            print(f"   ‚Ä¢ Conformidade p√≥s-corre√ß√£o: {audit['benford_conformity']:.4f}")

    # Estat√≠sticas
    metadata = {
        'vocab_size': V,
        'd_model_original': d,
        'd_model_quaternion': d // 4,
        'mean_beta': float(np.mean(token_metadata['betas'])),
        'mean_fractal_dim': float(np.mean(token_metadata['fractal_dims'])),
        'mean_alpha': float(np.mean(token_metadata['alphas'])),
        'std_fractal_dim': float(np.std(token_metadata['fractal_dims'])),
        'token_metadata': token_metadata,
        'calibration_used': calibration_config is not None,
        'semantic_categories_used': semantic_categories is not None,
        'benford_audit': audit
    }

    # Adicionar estat√≠sticas de categorias sem√¢nticas se dispon√≠vel
    if semantic_categories and 'semantic_categories' in token_metadata:
        category_counts = Counter(token_metadata['semantic_categories'])
        metadata['semantic_category_distribution'] = dict(category_counts)

    if verbose:
        print(f"\n   ‚úÖ Convers√£o completa:")
        print(f"      ‚Ä¢ Œ≤ m√©dio: {metadata['mean_beta']:.4f}")
        print(f"      ‚Ä¢ D m√©dio: {metadata['mean_fractal_dim']:.4f} ¬± {metadata['std_fractal_dim']:.4f}")
        print(f"      ‚Ä¢ Œ± m√©dio: {metadata['mean_alpha']:.4f}")
        print(f"      ‚Ä¢ Shape: {psi_embeddings_tensor.shape}")
        print(f"      ‚Ä¢ Conformidade Benford: {audit['benford_conformity']:.4f}")
        if metadata['calibration_used']:
            print(f"      ‚Ä¢ Calibra√ß√£o FCI: HABILITADA")
        if metadata['semantic_categories_used']:
            print(f"      ‚Ä¢ Categorias sem√¢nticas: {len(metadata.get('semantic_category_distribution', {}))}")
            for category, count in metadata.get('semantic_category_distribution', {}).items():
                percentage = (count / V) * 100
                print(f"        ‚îî‚îÄ {category}: {count} ({percentage:.1f}%)")

    return psi_embeddings_tensor, metadata


def benford_spectral_audit(quaternion_embeddings: torch.Tensor) -> Dict[str, float]:
    """
    Auditoria espectral baseada na Lei de Benford generalizada.

    Verifica se a distribui√ß√£o das magnitudes dos componentes
    segue uma lei de pot√™ncia logar√≠tmica esperada.

    Args:
        quaternion_embeddings: Tensor [V, d/4, 4]

    Returns:
        Dict com m√©tricas de conformidade
    """
    # Magnitudes dos componentes quaterni√¥nicos
    magnitudes = torch.norm(quaternion_embeddings, dim=-1)  # [V, d/4]

    # Distribui√ß√£o logar√≠tmica das magnitudes
    log_mags = torch.log10(magnitudes + 1e-10)
    fractional_parts = log_mags - torch.floor(log_mags)

    # Histograma dos d√≠gitos significativos (Benford)
    hist = torch.histc(fractional_parts, bins=9, min=0, max=1)
    observed_probs = hist / hist.sum()

    # Lei de Benford te√≥rica: P(d) = log10(1 + 1/d)
    theoretical_probs = torch.tensor([np.log10(1 + 1/d) for d in range(1, 10)], device=magnitudes.device)

    # M√©trica de conformidade (KL divergence)
    kl_div = torch.sum(theoretical_probs * torch.log(theoretical_probs / (observed_probs + 1e-10)))

    return {
        'benford_conformity': float(kl_div.item()),
        'is_conformant': kl_div.item() < 0.5  # Limiar emp√≠rico
    }


def spectral_benford_renormalization(
    quaternion_embeddings: torch.Tensor,
    target_conformity: float = 0.3
) -> torch.Tensor:
    """
    Re-normaliza embeddings para respeitar a Lei de Benford espectral.

    Usa um mapeamento logar√≠tmico adaptativo para ajustar magnitudes.

    Args:
        quaternion_embeddings: Tensor [V, d/4, 4]
        target_conformity: Conformidade alvo

    Returns:
        Tensor re-normalizado
    """
    V, d_quat, _ = quaternion_embeddings.shape

    # Reshape para componentes individuais [V*d_quat, 4]
    components = quaternion_embeddings.reshape(-1, 4)

    # Calcular magnitudes atuais
    mags = torch.norm(components, dim=-1, keepdim=True)

    # Gerar magnitudes alvo seguindo Lei de Benford
    num_components = components.shape[0]
    # Amostrar d√≠gitos significativos da distribui√ß√£o de Benford
    benford_probs = torch.tensor([np.log10(1 + 1/d) for d in range(1, 10)])
    digits = torch.multinomial(benford_probs, num_components, replacement=True).float() + 1.0

    # Mapear para magnitudes log-uniformes
    log_mags_target = torch.log10(digits.unsqueeze(1)) + torch.rand(num_components, 1)  # Parte fracion√°ria aleat√≥ria
    mags_target = 10 ** log_mags_target

    # Re-normalizar componentes
    components_normalized = components / (mags + 1e-10)
    components_renorm = components_normalized * mags_target.to(components.device)

    # Reshape de volta
    return components_renorm.reshape(V, d_quat, 4)


def save_psiqrh_embedding(
    psi_embeddings: torch.Tensor,
    metadata: Dict,
    output_dir: Path
):
    """
    Salva embedding quaterni√¥nico e metadados (SEM depend√™ncias externas).

    Args:
        psi_embeddings: Tensor [V, d/4, 4]
        metadata: Metadados da convers√£o
        output_dir: Diret√≥rio de sa√≠da
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"\nüíæ Salvando embedding quaterni√¥nico...")

    # Salvar tensor (formato compacto)
    embedding_path = output_dir / "quaternion_embedding.pt"
    torch.save(psi_embeddings, embedding_path)
    print(f"   ‚úÖ Embedding salvo: {embedding_path}")
    print(f"      ‚Ä¢ Tamanho: {embedding_path.stat().st_size / (1024**2):.2f} MB")

    # Salvar metadata
    metadata_path = output_dir / "embedding_metadata.json"
    # Remover token_metadata para reduzir tamanho (pode ser muito grande)
    metadata_compact = {k: v for k, v in metadata.items() if k != 'token_metadata'}
    with open(metadata_path, 'w') as f:
        json.dump(metadata_compact, f, indent=2)
    print(f"   ‚úÖ Metadata salva: {metadata_path}")
    print(f"   ‚Ä¢ Vocabul√°rio: {metadata['vocab_size']:,} tokens GPT-2")
    print(f"   ‚Ä¢ Sistema 100% aut√¥nomo (sem transformers)")


def load_psiqrh_embedding(model_dir: Path) -> Tuple[torch.Tensor, Dict]:
    """
    Carrega embedding quaterni√¥nico convertido.

    Args:
        model_dir: Diret√≥rio do modelo

    Returns:
        Tuple (embedding, metadata)
    """
    model_dir = Path(model_dir)

    # Carregar embedding
    embedding_path = model_dir / "quaternion_embedding.pt"
    if not embedding_path.exists():
        raise FileNotFoundError(f"Embedding n√£o encontrado: {embedding_path}")

    psi_embeddings = torch.load(embedding_path, map_location='cpu')

    # Carregar metadata
    metadata_path = model_dir / "embedding_metadata.json"
    if metadata_path.exists():
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
    else:
        metadata = {}

    return psi_embeddings, metadata


if __name__ == "__main__":
    # Teste b√°sico
    print("üß™ Teste do Embedding Spectral Converter\n")

    # Simular embedding GPT-2 (pequeno para teste)
    V_test = 100
    d_test = 768

    print(f"Criando embedding de teste: [{V_test}, {d_test}]")
    gpt2_embedding_test = torch.randn(V_test, d_test)

    # Converter
    psi_emb, metadata = convert_gpt2_embedding_to_psiqrh(
        gpt2_embedding_test,
        verbose=True
    )

    print(f"\n‚úÖ Teste completo!")
    print(f"   ‚Ä¢ Shape original: {gpt2_embedding_test.shape}")
    print(f"   ‚Ä¢ Shape quaterni√¥nico: {psi_emb.shape}")
    print(f"   ‚Ä¢ Redu√ß√£o de dimens√£o: {d_test} ‚Üí {d_test//4} √ó 4")
    print(f"   ‚Ä¢ D m√©dio: {metadata['mean_fractal_dim']:.4f}")
