#!/usr/bin/env python3 """ Baseline Transformer Model ========================== Standard transformer implementation for comparison with HumanChatTest-v1.0 QRH model. Same architecture but with standard multi-head attention instead of QRHLayer. Author: Klenio Araujo Padilha License: GNU GPLv3 """ import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import math import logging from typing import Optional, Dict, Any from dataclasses import dataclass @dataclass class BaselineConfig: """Configuration for baseline transformer""" vocab_size: int = 10000 embed_dim: int = 256 num_layers: int = 6 num_heads: int = 8 hidden_dim: int = 1024 dropout: float = 0.1 max_length: int = 512 learning_rate: float = 1e-4 device: str = 'auto' class MultiHeadAttention(nn.Module): """Standard multi-head attention mechanism""" def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1): super().__init__() self.embed_dim = embed_dim self.num_heads = num_heads self.head_dim = embed_dim // num_heads assert self.head_dim * num_heads == embed_dim, "embed_dim must be divisible by num_heads" self.q_proj = nn.Linear(embed_dim, embed_dim) self.k_proj = nn.Linear(embed_dim, embed_dim) self.v_proj = nn.Linear(embed_dim, embed_dim) self.out_proj = nn.Linear(embed_dim, embed_dim) self.dropout = nn.Dropout(dropout) self.scale = math.sqrt(self.head_dim) def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor: batch_size, seq_len, embed_dim = x.shape # Project to Q, K, V q = self.q_proj(x) k = self.k_proj(x) v = self.v_proj(x) # Reshape for multi-head attention q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2) k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2) v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2) # Compute attention scores scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale # Apply attention mask if provided if attention_mask is not None: scores = scores.masked_fill(attention_mask == 0, -1e9) # Apply softmax attn_weights = F.softmax(scores, dim=-1) attn_weights = self.dropout(attn_weights) # Apply attention to values attn_output = torch.matmul(attn_weights, v) # Reshape and project output attn_output = attn_output.transpose(1, 2).contiguous().view( batch_size, seq_len, embed_dim ) output = self.out_proj(attn_output) return output class BaselineTransformerBlock(nn.Module): """Standard transformer block with multi-head attention""" def __init__(self, config: BaselineConfig): super().__init__() self.config = config # Multi-head attention self.attention = MultiHeadAttention( embed_dim=config.embed_dim, num_heads=config.num_heads, dropout=config.dropout ) # Feed forward network self.ffn = nn.Sequential( nn.Linear(config.embed_dim, config.hidden_dim), nn.GELU(), nn.Dropout(config.dropout), nn.Linear(config.hidden_dim, config.embed_dim), nn.Dropout(config.dropout) ) # Layer normalization self.ln1 = nn.LayerNorm(config.embed_dim) self.ln2 = nn.LayerNorm(config.embed_dim) def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor: # Self-attention with residual connection attn_out = self.attention(x, attention_mask) x = self.ln1(x + attn_out) # Feed forward with residual connection ffn_out = self.ffn(x) x = self.ln2(x + ffn_out) return x class BaselineTransformerModel(nn.Module): """ Baseline transformer model for comparison with QRH model Uses standard architecture with multi-head attention """ def __init__(self, config: BaselineConfig): super().__init__() self.config = config # Token embeddings self.token_embedding = nn.Embedding(config.vocab_size, config.embed_dim) self.position_embedding = nn.Embedding(config.max_length, config.embed_dim) # Transformer blocks self.blocks = nn.ModuleList([ BaselineTransformerBlock(config) for _ in range(config.num_layers) ]) # Output layer self.ln_final = nn.LayerNorm(config.embed_dim) self.lm_head = nn.Linear(config.embed_dim, config.vocab_size) # Initialize weights self.apply(self._init_weights) def _init_weights(self, module): """Initialize model weights""" if isinstance(module, nn.Linear): torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) if module.bias is not None: torch.nn.init.zeros_(module.bias) elif isinstance(module, nn.Embedding): torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor: batch_size, seq_len = input_ids.shape device = input_ids.device # Create position indices position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device) position_ids = position_ids.unsqueeze(0).expand(batch_size, -1) # Embeddings token_embeds = self.token_embedding(input_ids) pos_embeds = self.position_embedding(position_ids) x = token_embeds + pos_embeds # Pass through transformer blocks for block in self.blocks: x = block(x, attention_mask) # Final layer norm and output x = self.ln_final(x) logits = self.lm_head(x) return logits def count_parameters(self) -> int: """Count total number of parameters""" return sum(p.numel() for p in self.parameters() if p.requires_grad) class BaselineEngine: """Engine for baseline transformer model""" def __init__(self, config: Optional[BaselineConfig] = None): self.config = config or BaselineConfig() self.device = self._detect_device() self.config.device = self.device # Setup logging logging.basicConfig(level=logging.INFO) self.logger = logging.getLogger(__name__) # Initialize model self.model = BaselineTransformerModel(self.config).to(self.device) self.optimizer = optim.AdamW( self.model.parameters(), lr=self.config.learning_rate, weight_decay=0.01 ) self.logger.info(f"Baseline transformer initialized on {self.device}") self.logger.info(f"Model parameters: {self.model.count_parameters():,}") def _detect_device(self) -> str: """Detect optimal device""" if torch.cuda.is_available(): return "cuda" elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available(): return "mps" else: return "cpu" def train_step(self, batch: Dict[str, torch.Tensor]) -> float: """Single training step""" self.model.train() self.optimizer.zero_grad() input_ids = batch["input_ids"].to(self.device) targets = batch["targets"].to(self.device) attention_mask = batch.get("attention_mask", None) if attention_mask is not None: attention_mask = attention_mask.to(self.device) logits = self.model(input_ids, attention_mask) # Calculate loss loss = nn.CrossEntropyLoss()( logits.view(-1, logits.size(-1)), targets.view(-1) ) loss.backward() self.optimizer.step() return loss.item() def evaluate_perplexity(self, test_data: list) -> float: """Evaluate model perplexity""" self.model.eval() total_loss = 0.0 total_tokens = 0 with torch.no_grad(): for text in test_data: words = text.split() if len(words) < 2: continue # Simple tokenization vocab = list(set(words)) word_to_id = {word: i for i, word in enumerate(vocab)} token_ids = [word_to_id.get(word, 0) for word in words] input_ids = torch.tensor([token_ids[:-1]], device=self.device) targets = torch.tensor([token_ids[1:]], device=self.device) logits = self.model(input_ids) loss = nn.CrossEntropyLoss()( logits.view(-1, logits.size(-1)), targets.view(-1) ) total_loss += loss.item() * len(token_ids) total_tokens += len(token_ids) avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf') perplexity = torch.exp(torch.tensor(avg_loss)).item() return perplexity def generate(self, input_text: str, max_length: int = 50) -> str: """Generate text using the baseline model""" self.model.eval() with torch.no_grad(): words = input_text.split() vocab = list(set(words)) word_to_id = {word: i for i, word in enumerate(vocab)} id_to_word = {i: word for word, i in word_to_id.items()} token_ids = [word_to_id.get(word, 0) for word in words] input_ids = torch.tensor([token_ids], device=self.device) generated_ids = input_ids.clone() for _ in range(max_length): logits = self.model(generated_ids) next_token_logits = logits[0, -1, :] next_token_id = torch.argmax(next_token_logits, dim=-1) generated_ids = torch.cat([ generated_ids, next_token_id.unsqueeze(0).unsqueeze(0) ], dim=1) # Simple stopping criterion if generated_ids.size(1) >= self.config.max_length: break # Convert back to text generated_tokens = generated_ids[0].tolist() generated_words = [id_to_word.get(id, "<unk>") for id in generated_tokens] return " ".join(generated_words) def save_model(self, path: str): """Save model checkpoint""" torch.save({ 'model_state_dict': self.model.state_dict(), 'optimizer_state_dict': self.optimizer.state_dict(), 'config': self.config }, path) def load_model(self, path: str): """Load model checkpoint""" checkpoint = torch.load(path, map_location=self.device) self.model.load_state_dict(checkpoint['model_state_dict']) self.optimizer.load_state_dict(checkpoint['optimizer_state_dict']) def main(): """Test baseline transformer""" print("Baseline Transformer Model Testing") print("=" * 40) config = BaselineConfig( embed_dim=128, num_layers=4, vocab_size=5000 ) engine = BaselineEngine(config) # Test generation test_input = "The mathematical foundation of" generated = engine.generate(test_input, max_length=10) print(f"Input: {test_input}") print(f"Generated: {generated}") print(f"Parameters: {engine.model.count_parameters():,}") if __name__ == "__main__": main()