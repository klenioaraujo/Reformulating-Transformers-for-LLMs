#!/usr/bin/env python3 """ Model Comparison Script ====================== Compare HumanChatTest-v1.0 (QRH) with baseline transformer model. Measures perplexity, training efficiency, and generation quality. Author: Klenio Araujo Padilha License: GNU GPLv3 """ import os import sys import time import torch import numpy as np import matplotlib.pyplot as plt from pathlib import Path from typing import List, Dict, Any, Tuple import logging import json # Add project root to path PROJECT_ROOT = Path(__file__).parent.parent.parent sys.path.insert(0, str(PROJECT_ROOT)) from src.prompt_engine.humanchat_engine import HumanChatEngine, HumanChatConfig from src.prompt_engine.baseline_transformer import BaselineEngine, BaselineConfig class ModelComparison: """ Comprehensive comparison between QRH and baseline transformer models """ def __init__(self, model_size: str = "small"): """ Initialize comparison with specified model size Args: model_size: "small" (~10M), "medium" (~30M), or "large" (~100M) """ self.model_size = model_size self.device = self._detect_device() # Setup logging logging.basicConfig(level=logging.INFO) self.logger = logging.getLogger(__name__) # Model configurations self.qrh_config = self._get_qrh_config(model_size) self.baseline_config = self._get_baseline_config(model_size) # Initialize models self.qrh_engine = None self.baseline_engine = None # Results storage self.results = { "model_size": model_size, "device": self.device, "qrh_results": {}, "baseline_results": {}, "comparison": {} } def _detect_device(self) -> str: """Detect optimal device""" if torch.cuda.is_available(): return "cuda" elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available(): return "mps" else: return "cpu" def _get_qrh_config(self, size: str) -> HumanChatConfig: """Get QRH model configuration based on size""" configs = { "small": HumanChatConfig( embed_dim=128, num_layers=4, num_heads=8, hidden_dim=512, vocab_size=5000, max_length=256 ), "medium": HumanChatConfig( embed_dim=256, num_layers=6, num_heads=8, hidden_dim=1024, vocab_size=10000, max_length=512 ), "large": HumanChatConfig( embed_dim=512, num_layers=8, num_heads=16, hidden_dim=2048, vocab_size=20000, max_length=1024 ) } return configs[size] def _get_baseline_config(self, size: str) -> BaselineConfig: """Get baseline model configuration based on size""" configs = { "small": BaselineConfig( embed_dim=128, num_layers=4, num_heads=8, hidden_dim=512, vocab_size=5000, max_length=256 ), "medium": BaselineConfig( embed_dim=256, num_layers=6, num_heads=8, hidden_dim=1024, vocab_size=10000, max_length=512 ), "large": BaselineConfig( embed_dim=512, num_layers=8, num_heads=16, hidden_dim=2048, vocab_size=20000, max_length=1024 ) } return configs[size] def initialize_models(self): """Initialize both models for comparison""" self.logger.info("Initializing models...") # Initialize QRH model self.qrh_engine = HumanChatEngine(self.qrh_config) qrh_params = self.qrh_engine.model.count_parameters() # Initialize baseline model self.baseline_engine = BaselineEngine(self.baseline_config) baseline_params = self.baseline_engine.model.count_parameters() self.logger.info(f"QRH model parameters: {qrh_params:,}") self.logger.info(f"Baseline model parameters: {baseline_params:,}") # Store parameter counts self.results["qrh_results"]["parameters"] = qrh_params self.results["baseline_results"]["parameters"] = baseline_params def create_synthetic_dataset(self, size: int = 1000) -> List[str]: """Create synthetic dataset for training and evaluation""" templates = [ "The quantum mechanical principles of", "Mathematical foundations reveal that", "Consciousness emerges from complex", "Neural networks demonstrate how", "Information processing requires", "Harmonic oscillations show that", "Fractal patterns emerge when", "Quaternion algebra provides", "Wave equations describe the", "Computational systems exhibit" ] continuations = [ "energy conservation in isolated systems", "dimensional analysis provides deep insights", "interactions between multiple components", "learning occurs through gradient descent", "massive parallel computation capabilities", "resonance creates stable configurations", "self-similarity exists across scales", "elegant solutions to rotation problems", "propagation of information through space", "emergent behavior from simple rules" ] dataset = [] for i in range(size): template = templates[i % len(templates)] continuation = continuations[i % len(continuations)] text = f"{template} {continuation}" dataset.append(text) return dataset def benchmark_inference_speed(self, num_samples: int = 100) -> Dict[str, float]: """Benchmark inference speed for both models""" self.logger.info("Benchmarking inference speed...") test_texts = self.create_synthetic_dataset(num_samples) # Benchmark QRH model qrh_times = [] for text in test_texts[:num_samples]: start_time = time.time() result = self.qrh_engine.process_text(text) end_time = time.time() qrh_times.append(end_time - start_time) # Benchmark baseline model baseline_times = [] for text in test_texts[:num_samples]: start_time = time.time() generated = self.baseline_engine.generate(text, max_length=20) end_time = time.time() baseline_times.append(end_time - start_time) qrh_avg_time = np.mean(qrh_times) baseline_avg_time = np.mean(baseline_times) speedup = baseline_avg_time / qrh_avg_time results = { "qrh_avg_time": qrh_avg_time, "baseline_avg_time": baseline_avg_time, "speedup": speedup, "qrh_std": np.std(qrh_times), "baseline_std": np.std(baseline_times) } self.logger.info(f"QRH average time: {qrh_avg_time:.4f}s") self.logger.info(f"Baseline average time: {baseline_avg_time:.4f}s") self.logger.info(f"QRH speedup: {speedup:.2f}x") return results def measure_perplexity(self, test_size: int = 200) -> Dict[str, float]: """Measure perplexity on test dataset""" self.logger.info("Measuring perplexity...") test_data = self.create_synthetic_dataset(test_size) # Measure QRH perplexity qrh_perplexity = self.qrh_engine.evaluate_perplexity(test_data) # Measure baseline perplexity baseline_perplexity = self.baseline_engine.evaluate_perplexity(test_data) perplexity_improvement = baseline_perplexity / qrh_perplexity results = { "qrh_perplexity": qrh_perplexity, "baseline_perplexity": baseline_perplexity, "improvement": perplexity_improvement } self.logger.info(f"QRH perplexity: {qrh_perplexity:.2f}") self.logger.info(f"Baseline perplexity: {baseline_perplexity:.2f}") self.logger.info(f"QRH improvement: {perplexity_improvement:.2f}x") return results def analyze_generation_quality(self, num_samples: int = 50) -> Dict[str, Any]: """Analyze generation quality through various metrics""" self.logger.info("Analyzing generation quality...") prompts = [ "The mathematical relationship between", "Quantum consciousness emerges from", "Fractal patterns demonstrate that", "Information theory suggests that", "Neural networks learn by" ] qrh_generations = [] baseline_generations = [] for prompt in prompts * (num_samples // len(prompts)): # QRH generation qrh_result = self.qrh_engine.process_text(prompt) qrh_text = qrh_result.get("output", "") qrh_generations.append(qrh_text) # Baseline generation baseline_text = self.baseline_engine.generate(prompt, max_length=30) baseline_generations.append(baseline_text) # Calculate quality metrics qrh_avg_length = np.mean([len(text.split()) for text in qrh_generations]) baseline_avg_length = np.mean([len(text.split()) for text in baseline_generations]) qrh_diversity = self._calculate_diversity(qrh_generations) baseline_diversity = self._calculate_diversity(baseline_generations) results = { "qrh_avg_length": qrh_avg_length, "baseline_avg_length": baseline_avg_length, "qrh_diversity": qrh_diversity, "baseline_diversity": baseline_diversity, "sample_generations": { "qrh": qrh_generations[:5], "baseline": baseline_generations[:5] } } return results def _calculate_diversity(self, texts: List[str]) -> float: """Calculate lexical diversity (unique words / total words)""" all_words = [] for text in texts: all_words.extend(text.lower().split()) if not all_words: return 0.0 unique_words = len(set(all_words)) total_words = len(all_words) return unique_words / total_words def memory_usage_analysis(self) -> Dict[str, Any]: """Analyze memory usage of both models""" self.logger.info("Analyzing memory usage...") # QRH memory usage qrh_params = self.qrh_engine.model.count_parameters() qrh_memory_mb = qrh_params * 4 / (1024 * 1024) # Assuming float32 # Baseline memory usage baseline_params = self.baseline_engine.model.count_parameters() baseline_memory_mb = baseline_params * 4 / (1024 * 1024) memory_ratio = baseline_memory_mb / qrh_memory_mb results = { "qrh_memory_mb": qrh_memory_mb, "baseline_memory_mb": baseline_memory_mb, "memory_ratio": memory_ratio, "qrh_parameters": qrh_params, "baseline_parameters": baseline_params } self.logger.info(f"QRH memory usage: {qrh_memory_mb:.1f} MB") self.logger.info(f"Baseline memory usage: {baseline_memory_mb:.1f} MB") return results def run_comprehensive_comparison(self) -> Dict[str, Any]: """Run comprehensive comparison between models""" self.logger.info("Starting comprehensive model comparison...") # Initialize models self.initialize_models() # Run benchmarks speed_results = self.benchmark_inference_speed() perplexity_results = self.measure_perplexity() quality_results = self.analyze_generation_quality() memory_results = self.memory_usage_analysis() # Compile results self.results.update({ "speed_benchmark": speed_results, "perplexity_benchmark": perplexity_results, "quality_analysis": quality_results, "memory_analysis": memory_results }) # Calculate overall comparison scores self.results["comparison"] = self._calculate_comparison_scores() return self.results def _calculate_comparison_scores(self) -> Dict[str, float]: """Calculate overall comparison scores""" # Speed score (QRH faster = higher score) speed_score = self.results["speed_benchmark"]["speedup"] # Perplexity score (lower perplexity = higher score) perplexity_score = self.results["perplexity_benchmark"]["improvement"] # Memory efficiency score (less memory = higher score) memory_score = self.results["memory_analysis"]["memory_ratio"] # Quality score (higher diversity = higher score) qrh_diversity = self.results["quality_analysis"]["qrh_diversity"] baseline_diversity = self.results["quality_analysis"]["baseline_diversity"] quality_score = qrh_diversity / baseline_diversity if baseline_diversity > 0 else 1.0 # Overall score (weighted average) overall_score = ( 0.3 * speed_score + 0.4 * perplexity_score + 0.2 * memory_score + 0.1 * quality_score ) return { "speed_score": speed_score, "perplexity_score": perplexity_score, "memory_score": memory_score, "quality_score": quality_score, "overall_score": overall_score } def generate_report(self, output_path: str = "model_comparison_report.json"): """Generate comprehensive comparison report""" self.logger.info(f"Generating comparison report: {output_path}") # Add summary self.results["summary"] = { "winner": "QRH" if self.results["comparison"]["overall_score"] > 1.0 else "Baseline", "overall_score": self.results["comparison"]["overall_score"], "key_advantages": self._identify_key_advantages() } # Save results with open(output_path, 'w') as f: json.dump(self.results, f, indent=2) self.logger.info("Report generated successfully") def _identify_key_advantages(self) -> Dict[str, str]: """Identify key advantages of each model""" advantages = {} if self.results["comparison"]["speed_score"] > 1.0: advantages["qrh_speed"] = f"QRH is {self.results['comparison']['speed_score']:.1f}x faster" if self.results["comparison"]["perplexity_score"] > 1.0: advantages["qrh_perplexity"] = f"QRH has {self.results['comparison']['perplexity_score']:.1f}x better perplexity" if self.results["comparison"]["memory_score"] > 1.0: advantages["qrh_memory"] = f"QRH uses {self.results['comparison']['memory_score']:.1f}x less memory" return advantages def create_visualization(self, output_dir: str = "comparison_plots"): """Create visualization plots for comparison results""" os.makedirs(output_dir, exist_ok=True) # Performance comparison bar chart metrics = ['Speed', 'Perplexity', 'Memory Efficiency', 'Quality'] qrh_scores = [ self.results["comparison"]["speed_score"], self.results["comparison"]["perplexity_score"], self.results["comparison"]["memory_score"], self.results["comparison"]["quality_score"] ] plt.figure(figsize=(10, 6)) bars = plt.bar(metrics, qrh_scores, color=['blue', 'green', 'orange', 'purple']) plt.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Baseline') plt.ylabel('QRH vs Baseline Score (>1.0 = QRH better)') plt.title('Model Performance Comparison') plt.legend() # Add value labels on bars for bar, score in zip(bars, qrh_scores): plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, f'{score:.2f}', ha='center', va='bottom') plt.tight_layout() plt.savefig(f"{output_dir}/performance_comparison.png", dpi=300, bbox_inches='tight') plt.close() self.logger.info(f"Visualization saved to {output_dir}/") def main(): """Main execution for model comparison""" print("HumanChatTest-v1.0 vs Baseline Transformer Comparison") print("=" * 60) # Run comparison comparison = ModelComparison(model_size="small") results = comparison.run_comprehensive_comparison() # Generate report comparison.generate_report("model_comparison_results.json") comparison.create_visualization() # Print summary print("\nComparison Results Summary:") print("-" * 30) print(f"Overall winner: {results['summary']['winner']}") print(f"Overall score: {results['summary']['overall_score']:.2f}") print("\nKey metrics:") print(f"Speed improvement: {results['comparison']['speed_score']:.2f}x") print(f"Perplexity improvement: {results['comparison']['perplexity_score']:.2f}x") print(f"Memory efficiency: {results['comparison']['memory_score']:.2f}x") print(f"Quality score: {results['comparison']['quality_score']:.2f}") print("\nDetailed report saved to: model_comparison_results.json") print("Visualization saved to: comparison_plots/") if __name__ == "__main__": main()