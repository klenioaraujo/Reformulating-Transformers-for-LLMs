#!/usr/bin/env python3
"""
Neural Diffusion Engine - Calculadora do Coeficiente D
======================================================

Implementa o c√°lculo do coeficiente de difus√£o neural D que governa
a dispers√£o de informa√ß√£o na din√¢mica consciente.

Equa√ß√£o de difus√£o: D‚àá¬≤P
Onde D √© adaptativo baseado no estado de consci√™ncia e caracter√≠sticas fractais.
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, Optional, Tuple


class NeuralDiffusionEngine(nn.Module):
    """
    Engine para computa√ß√£o do coeficiente de difus√£o neural D
    que controla a dispers√£o de informa√ß√£o na consci√™ncia.
    """

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.device = config.device

        # Limites do coeficiente de difus√£o
        self.d_min = config.diffusion_coefficient_range[0]
        self.d_max = config.diffusion_coefficient_range[1]

        # Par√¢metros adaptativos aprend√≠veis
        self.register_parameter(
            'diffusion_weights',
            nn.Parameter(torch.tensor([1.0, 0.5, 0.3, 0.2]))  # Pesos para diferentes fatores
        )

        # Hist√≥rico de difus√£o para an√°lise temporal
        self.diffusion_history = []
        self.max_history = 100

        print(f"‚ö° NeuralDiffusionEngine inicializado com range D=[{self.d_min:.3f}, {self.d_max:.3f}]")

    def compute_diffusion(
        self,
        psi_distribution: torch.Tensor,
        fractal_field: torch.Tensor,
        fci: Optional[float] = None,
        spectral_energy: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Computa coeficiente de difus√£o neural D baseado no estado atual.

        O coeficiente D √© calculado considerando:
        1. Dispers√£o da distribui√ß√£o P(œà)
        2. Magnitude do campo fractal F(œà)
        3. Complexidade local do sistema
        4. Caracter√≠sticas temporais
        5. NOVO: Adapta√ß√£o din√¢mica baseada em FCI
        6. NOVO: Modula√ß√£o por energia espectral

        Args:
            psi_distribution: Distribui√ß√£o P(œà) [batch, embed_dim]
            fractal_field: Campo fractal F(œà) [batch, embed_dim]
            fci: Fractal Consciousness Index (opcional) [0, 1]
            spectral_energy: Energia espectral (opcional) [batch, embed_dim]

        Returns:
            Coeficiente de difus√£o D [batch, embed_dim]
        """
        batch_size, embed_dim = psi_distribution.shape

        # 1. Fator baseado na dispers√£o da distribui√ß√£o
        dispersion_factor = self._compute_dispersion_factor(psi_distribution)

        # 2. Fator baseado na magnitude do campo
        field_factor = self._compute_field_factor(fractal_field)

        # 3. Fator de complexidade local
        complexity_factor = self._compute_complexity_factor(psi_distribution, fractal_field)

        # 4. Fator de adapta√ß√£o temporal
        temporal_factor = self._compute_temporal_factor()

        # Combinar fatores com pesos aprend√≠veis
        combined_factor = (
            self.diffusion_weights[0] * dispersion_factor +
            self.diffusion_weights[1] * field_factor +
            self.diffusion_weights[2] * complexity_factor +
            self.diffusion_weights[3] * temporal_factor
        )

        # Normalizar e mapear para range de difus√£o
        diffusion_coefficient = self._normalize_to_diffusion_range(combined_factor)

        # 5. NOVO: Aplicar adapta√ß√£o din√¢mica baseada em FCI
        if fci is not None:
            diffusion_coefficient = self._apply_fci_adaptation(diffusion_coefficient, fci)
            # Aplicar for√ßa de repuls√£o para estados de baixa consci√™ncia
            diffusion_coefficient = self._apply_consciousness_repulsion(diffusion_coefficient, fci, psi_distribution)

        # 6. NOVO: Aplicar modula√ß√£o por energia espectral
        if spectral_energy is not None:
            diffusion_coefficient = self._apply_spectral_modulation(diffusion_coefficient, spectral_energy)

        # Aplicar suaviza√ß√£o espacial
        diffusion_coefficient = self._apply_spatial_smoothing(diffusion_coefficient)

        # Armazenar no hist√≥rico
        self._update_history(diffusion_coefficient)

        return diffusion_coefficient

    def _compute_dispersion_factor(self, psi_distribution: torch.Tensor) -> torch.Tensor:
        """
        Computa fator de dispers√£o baseado na distribui√ß√£o P(œà).

        Alta dispers√£o ‚Üí maior difus√£o (sistema mais "espalhado")
        Baixa dispers√£o ‚Üí menor difus√£o (sistema mais "concentrado")

        Args:
            psi_distribution: Distribui√ß√£o P(œà)

        Returns:
            Fator de dispers√£o normalizado [0, 1]
        """
        # Calcular entropia como medida de dispers√£o
        epsilon = float(self.config.epsilon) if hasattr(self.config, 'epsilon') else 1e-10
        psi_safe = torch.clamp(psi_distribution, min=epsilon)
        log_psi = torch.log(psi_safe)
        entropy_raw = -torch.sum(psi_distribution * log_psi, dim=-1, keepdim=True)
        # Prote√ß√£o contra NaN
        entropy = torch.where(torch.isnan(entropy_raw), torch.zeros_like(entropy_raw), entropy_raw)

        # Normalizar entropia (m√°ximo log(embed_dim))
        max_entropy = np.log(psi_distribution.shape[-1])
        normalized_entropy = entropy / (max_entropy + epsilon)

        # Calcular desvio padr√£o como medida adicional de dispers√£o
        psi_std = torch.std(psi_distribution, dim=-1, keepdim=True)
        # Prote√ß√£o contra NaN
        psi_std = torch.where(torch.isnan(psi_std), torch.zeros_like(psi_std), psi_std)

        # Combinar entropia e desvio padr√£o
        dispersion_factor = 0.7 * normalized_entropy + 0.3 * psi_std

        # Broadcast para todas as dimens√µes
        dispersion_factor = dispersion_factor.expand_as(psi_distribution)

        return torch.clamp(dispersion_factor, 0.0, 1.0)

    def _compute_field_factor(self, fractal_field: torch.Tensor) -> torch.Tensor:
        """
        Computa fator baseado na magnitude do campo fractal.

        Campo forte ‚Üí maior difus√£o (din√¢mica mais ativa)
        Campo fraco ‚Üí menor difus√£o (din√¢mica mais est√°vel)

        Args:
            fractal_field: Campo fractal F(œà)

        Returns:
            Fator de campo normalizado [0, 1]
        """
        # Magnitude local do campo
        field_magnitude = torch.abs(fractal_field)

        # Normalizar pela magnitude m√°xima global
        epsilon = float(self.config.epsilon) if hasattr(self.config, 'epsilon') else 1e-10
        max_magnitude = torch.max(field_magnitude) + epsilon
        normalized_magnitude = field_magnitude / max_magnitude

        # Aplicar fun√ß√£o sigmoidal para suavizar
        field_factor = torch.sigmoid(5 * (normalized_magnitude - 0.5))

        # Prote√ß√£o contra NaN
        field_factor = torch.where(torch.isnan(field_factor), torch.ones_like(field_factor) * 0.5, field_factor)

        return field_factor

    def _compute_complexity_factor(
        self,
        psi_distribution: torch.Tensor,
        fractal_field: torch.Tensor
    ) -> torch.Tensor:
        """
        Computa fator de complexidade local do sistema.

        Alta complexidade ‚Üí maior difus√£o (sistema mais din√¢mico)
        Baixa complexidade ‚Üí menor difus√£o (sistema mais ordenado)

        Args:
            psi_distribution: Distribui√ß√£o P(œà)
            fractal_field: Campo fractal F(œà)

        Returns:
            Fator de complexidade normalizado [0, 1]
        """
        batch_size, embed_dim = psi_distribution.shape
        epsilon = float(self.config.epsilon) if hasattr(self.config, 'epsilon') else 1e-10

        # Calcular gradientes espaciais como medida de complexidade
        psi_gradient = self._compute_spatial_gradient(psi_distribution)
        field_gradient = self._compute_spatial_gradient(fractal_field)

        # Complexidade baseada na varia√ß√£o espacial
        psi_complexity = torch.std(psi_gradient, dim=-1, keepdim=True)
        field_complexity = torch.std(field_gradient, dim=-1, keepdim=True)

        # Prote√ß√£o contra NaN
        psi_complexity = torch.where(torch.isnan(psi_complexity), torch.zeros_like(psi_complexity), psi_complexity)
        field_complexity = torch.where(torch.isnan(field_complexity), torch.zeros_like(field_complexity), field_complexity)

        # Combinar complexidades
        total_complexity = torch.sqrt(psi_complexity**2 + field_complexity**2 + epsilon)

        # Normalizar
        max_complexity = torch.max(total_complexity) + epsilon
        normalized_complexity = total_complexity / max_complexity

        # Broadcast para todas as dimens√µes
        complexity_factor = normalized_complexity.expand_as(psi_distribution)

        return torch.clamp(complexity_factor, 0.0, 1.0)

    def _compute_spatial_gradient(self, tensor: torch.Tensor) -> torch.Tensor:
        """Computa gradiente espacial usando diferen√ßas finitas."""
        batch_size, embed_dim = tensor.shape
        gradient = torch.zeros_like(tensor)

        for i in range(embed_dim):
            i_next = (i + 1) % embed_dim
            i_prev = (i - 1) % embed_dim
            gradient[:, i] = (tensor[:, i_next] - tensor[:, i_prev]) / 2.0

        return gradient

    def _compute_temporal_factor(self) -> torch.Tensor:
        """
        Computa fator de adapta√ß√£o temporal baseado no hist√≥rico.

        Returns:
            Fator temporal escalar [0, 1]
        """
        if len(self.diffusion_history) < 2:
            return torch.tensor(0.5, device=self.device)

        # Analisar tend√™ncia temporal
        recent_values = self.diffusion_history[-5:]  # √öltimos 5 valores
        if len(recent_values) >= 2:
            # Calcular varia√ß√£o temporal
            variations = []
            for i in range(1, len(recent_values)):
                variation = torch.std(recent_values[i] - recent_values[i-1])
                variations.append(variation.item())

            avg_variation = np.mean(variations)

            # Mapear varia√ß√£o para fator temporal
            # Alta varia√ß√£o ‚Üí maior difus√£o (sistema inst√°vel)
            temporal_factor = torch.sigmoid(torch.tensor(10 * avg_variation, device=self.device))
        else:
            temporal_factor = torch.tensor(0.5, device=self.device)

        return temporal_factor

    def _normalize_to_diffusion_range(self, combined_factor: torch.Tensor) -> torch.Tensor:
        """
        Normaliza fator combinado para range de difus√£o [d_min, d_max].

        Args:
            combined_factor: Fator combinado n√£o normalizado

        Returns:
            Coeficiente de difus√£o no range correto
        """
        epsilon = float(self.config.epsilon) if hasattr(self.config, 'epsilon') else 1e-10

        # Prote√ß√£o contra NaN no combined_factor
        combined_factor = torch.where(
            torch.isnan(combined_factor) | torch.isinf(combined_factor),
            torch.ones_like(combined_factor) * 0.5,
            combined_factor
        )

        # Normalizar factor para [0, 1]
        factor_min = torch.min(combined_factor)
        factor_max = torch.max(combined_factor)
        factor_range = factor_max - factor_min + epsilon

        normalized_factor = (combined_factor - factor_min) / factor_range

        # Prote√ß√£o contra NaN no resultado
        normalized_factor = torch.clamp(normalized_factor, 0.0, 1.0)
        normalized_factor = torch.where(
            torch.isnan(normalized_factor),
            torch.ones_like(normalized_factor) * 0.5,
            normalized_factor
        )

        # Mapear para range de difus√£o
        diffusion_range = self.d_max - self.d_min
        diffusion_coefficient = self.d_min + normalized_factor * diffusion_range

        return diffusion_coefficient

    def _apply_spatial_smoothing(self, diffusion_coeff: torch.Tensor) -> torch.Tensor:
        """Aplica suaviza√ß√£o espacial ao coeficiente de difus√£o."""
        batch_size, embed_dim = diffusion_coeff.shape
        smoothed_coeff = torch.zeros_like(diffusion_coeff)

        # Usar kernel do config se dispon√≠vel
        if hasattr(self.config, 'field_smoothing_kernel'):
            kernel = self.config.field_smoothing_kernel
        else:
            kernel = (0.25, 0.5, 0.25)

        for i in range(embed_dim):
            i_prev = (i - 1) % embed_dim
            i_next = (i + 1) % embed_dim

            # M√©dia ponderada com vizinhos usando kernel do config
            smoothed_coeff[:, i] = (
                kernel[0] * diffusion_coeff[:, i_prev] +
                kernel[1] * diffusion_coeff[:, i] +
                kernel[2] * diffusion_coeff[:, i_next]
            )

        return smoothed_coeff

    def _update_history(self, diffusion_coefficient: torch.Tensor):
        """Atualiza hist√≥rico de difus√£o."""
        # Armazenar m√©dia para reduzir mem√≥ria
        mean_diffusion = diffusion_coefficient.mean().detach()
        self.diffusion_history.append(mean_diffusion)

        # Manter tamanho m√°ximo do hist√≥rico
        if len(self.diffusion_history) > self.max_history:
            self.diffusion_history.pop(0)

    def get_diffusion_statistics(self) -> Dict[str, float]:
        """
        Retorna estat√≠sticas do coeficiente de difus√£o.

        Returns:
            Dicion√°rio com estat√≠sticas
        """
        if not self.diffusion_history:
            return {'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0, 'trend': 0.0}

        history_tensor = torch.stack(self.diffusion_history)

        # Estat√≠sticas b√°sicas
        mean_d = history_tensor.mean().item()
        std_d = history_tensor.std().item()
        min_d = history_tensor.min().item()
        max_d = history_tensor.max().item()

        # Tend√™ncia (slope da regress√£o linear simples)
        if len(self.diffusion_history) >= 3:
            x = torch.arange(len(self.diffusion_history), dtype=torch.float32)
            y = history_tensor

            # Regress√£o linear: y = ax + b
            n = len(x)
            sum_x = x.sum()
            sum_y = y.sum()
            sum_xy = (x * y).sum()
            sum_x2 = (x**2).sum()

            trend = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x**2)
            trend = trend.item()
        else:
            trend = 0.0

        return {
            'mean': mean_d,
            'std': std_d,
            'min': min_d,
            'max': max_d,
            'trend': trend,
            'range_utilization': (max_d - min_d) / (self.d_max - self.d_min),
            'stability': 1.0 / (1.0 + std_d)  # Estabilidade inversa √† varia√ß√£o
        }

    def adapt_diffusion_parameters(self, target_statistics: Dict[str, float]):
        """
        Adapta par√¢metros de difus√£o para atingir estat√≠sticas alvo.

        Args:
            target_statistics: Estat√≠sticas desejadas
        """
        current_stats = self.get_diffusion_statistics()

        # Ajustar pesos baseado na diferen√ßa das estat√≠sticas
        adjustments = []

        # Ajuste baseado na m√©dia
        mean_diff = target_statistics.get('mean', current_stats['mean']) - current_stats['mean']
        if abs(mean_diff) > 0.1:
            adjustment = 0.1 * np.sign(mean_diff)
            adjustments.append(adjustment)

        # Ajuste baseado na estabilidade
        target_stability = target_statistics.get('stability', current_stats['stability'])
        stability_diff = target_stability - current_stats['stability']
        if abs(stability_diff) > 0.05:
            adjustment = 0.05 * np.sign(stability_diff)
            adjustments.append(adjustment)

        # Aplicar ajustes aos pesos
        if adjustments:
            avg_adjustment = np.mean(adjustments)
            with torch.no_grad():
                self.diffusion_weights += avg_adjustment
                # Manter pesos positivos
                self.diffusion_weights.clamp_(min=0.1)

        print(f"‚ö° Difus√£o adaptada: ajuste={np.mean(adjustments) if adjustments else 0:.3f}")

    def reset_history(self):
        """Reseta hist√≥rico de difus√£o."""
        self.diffusion_history.clear()
        print("‚ö° Hist√≥rico de difus√£o resetado")

    def _apply_fci_adaptation(self, diffusion_coeff: torch.Tensor, fci: float) -> torch.Tensor:
        """
        Aplica adapta√ß√£o din√¢mica do coeficiente de difus√£o baseada no FCI.

        Estados de consci√™ncia superiores (FCI alto) requerem D MAIOR para
        permitir integra√ß√£o e converg√™ncia em n√≠veis mais elevados.
        Estados inferiores (FCI baixo) usam D m√≠nimo (modo emergencial).

        L√≥gica: FCI ‚Üë ‚Üí D ‚Üë

        Args:
            diffusion_coeff: Coeficiente atual [batch, embed_dim]
            fci: Fractal Consciousness Index [0, 1]

        Returns:
            Coeficiente modulado pelo FCI
        """
        # Mapear FCI para fator de escala exponencial
        # FCI = 0.0 ‚Üí factor ‚âà 1.0 (D m√≠nimo)
        # FCI = 1.0 ‚Üí factor ‚âà (d_max/d_min) (D m√°ximo)
        log_d_min = np.log(self.d_min + 1e-10)
        log_d_max = np.log(self.d_max + 1e-10)

        # Interpola√ß√£o logar√≠tmica: cresce exponencialmente com FCI
        log_factor = log_d_min + fci * (log_d_max - log_d_min)
        adaptation_factor = np.exp(log_factor) / self.d_min

        # Aplicar modula√ß√£o
        adapted_coeff = diffusion_coeff * adaptation_factor

        # Garantir limites
        adapted_coeff = torch.clamp(adapted_coeff, self.d_min, self.d_max)

        return adapted_coeff

    def _apply_consciousness_repulsion(self, diffusion_coeff: torch.Tensor,
                                       fci: float, psi_distribution: torch.Tensor) -> torch.Tensor:
        """
        Aplica for√ßa de repuls√£o para estados de baixa consci√™ncia (FCI pr√≥ximo de zero).

        Esta for√ßa impede que o sistema se estabilize em estados comatoso (FCI < 0.1),
        criando uma din√¢mica auto-corretiva que for√ßa evolu√ß√£o para estados de maior consci√™ncia.

        Princ√≠pio: Estados de baixa consci√™ncia recebem "repuls√£o" que aumenta a difus√£o,
        for√ßando explora√ß√£o de novos estados at√© encontrar equil√≠brio em FCI > 0.1.

        Args:
            diffusion_coeff: Coeficiente atual [batch, embed_dim]
            fci: Fractal Consciousness Index [0, 1]
            psi_distribution: Distribui√ß√£o atual P(œà) [batch, embed_dim]

        Returns:
            Coeficiente com for√ßa de repuls√£o aplicada
        """
        # Threshold de baixa consci√™ncia
        low_consciousness_threshold = 0.1

        if fci >= low_consciousness_threshold:
            # Estado saud√°vel - sem repuls√£o adicional
            return diffusion_coeff

        # Calcular intensidade da repuls√£o baseada na dist√¢ncia do threshold
        repulsion_intensity = (low_consciousness_threshold - fci) / low_consciousness_threshold
        repulsion_intensity = torch.clamp(torch.tensor(repulsion_intensity), 0.0, 1.0)

        # Fator de repuls√£o exponencial: mais intenso quanto mais pr√≥ximo de zero
        # FCI = 0.0 ‚Üí repulsion_factor ‚âà 3.0 (difus√£o triplicada)
        # FCI = 0.1 ‚Üí repulsion_factor ‚âà 1.0 (sem repuls√£o)
        repulsion_factor = 1.0 + 2.0 * repulsion_intensity ** 2

        # Aplicar repuls√£o ao coeficiente de difus√£o
        repulsed_coeff = diffusion_coeff * repulsion_factor

        # Garantir limites superiores para evitar instabilidade
        max_repulsed = min(self.d_max, self.d_min * 5.0)  # M√°ximo 5x o m√≠nimo
        repulsed_coeff = torch.clamp(repulsed_coeff, self.d_min, max_repulsed)

        # Log para debug
        if repulsion_intensity > 0.5:
            print(f"üõë REPULS√ÉO ATIVADA: FCI={fci:.3f}, intensidade={repulsion_intensity:.3f}, "
                  f"D aumentado {repulsion_factor:.2f}x")

        return repulsed_coeff

    def _apply_spectral_modulation(self, diffusion_coeff: torch.Tensor,
                                      spectral_energy: torch.Tensor) -> torch.Tensor:
        """
        Aplica modula√ß√£o do coeficiente de difus√£o baseada na energia espectral.

        Regi√µes com alta energia espectral (harm√¥nicos fortes) devem ter
        maior difus√£o para explorar estruturas complexas, enquanto regi√µes
        de baixa energia podem ter menor difus√£o.

        Args:
            diffusion_coeff: Coeficiente atual [batch, embed_dim]
            spectral_energy: Energia espectral [batch, embed_dim] ou [batch,]

        Returns:
            Coeficiente modulado pela energia espectral
        """
        epsilon = float(self.config.epsilon) if hasattr(self.config, 'epsilon') else 1e-10

        # Normalizar energia espectral para [0, 1]
        if spectral_energy.dim() == 1:
            # Se for vetor 1D, expandir para todas as dimens√µes
            spectral_energy = spectral_energy.unsqueeze(-1).expand_as(diffusion_coeff)

        # Calcular energia normalizada
        energy_min = spectral_energy.min()
        energy_max = spectral_energy.max()
        energy_range = energy_max - energy_min + epsilon

        normalized_energy = (spectral_energy - energy_min) / energy_range
        normalized_energy = torch.clamp(normalized_energy, 0.0, 1.0)

        # Fator de modula√ß√£o: proporcional √† energia
        # Alta energia ‚Üí fator alto (mais difus√£o para explorar estrutura)
        # Baixa energia ‚Üí fator baixo (menos difus√£o)
        spectral_modulation = 0.5 + 1.0 * normalized_energy

        # Aplicar modula√ß√£o
        modulated_coeff = diffusion_coeff * spectral_modulation

        # Garantir limites
        modulated_coeff = torch.clamp(modulated_coeff, self.d_min, self.d_max)

        return modulated_coeff