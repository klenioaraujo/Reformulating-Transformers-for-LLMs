#!/usr/bin/env python3
"""
Î¨QRH Prompt Engine para Download e ConversÃ£o de Modelo Wiki
===========================================================

Usando o Enhanced Pipeline Î¨QRH para gerar cÃ³digo que faÃ§a download
de modelos Wikipedia do Transformers e os converta para formato .Î¨cws
com anÃ¡lise de consciÃªncia fractal.

Pipeline: Prompt â†’ Î¨QRH Analysis â†’ Model Download â†’ Wiki2Î¨cws Conversion
"""

import sys
import os
from pathlib import Path
sys.path.append(str(Path(__file__).parent / 'src'))

from src.core.Î¨QRH import QRHFactory

def generate_wiki_to_psicws_converter():
    """
    Usa Î¨QRH Prompt Engine para gerar sistema de download e conversÃ£o Wikiâ†’Î¨cws
    """

    # Inicializar Î¨QRH Factory
    qrh_factory = QRHFactory()

    # Prompt avanÃ§ado para o Î¨QRH Engine
    prompt = """
    Î¨QRH-WIKI-MODEL-TASK: Download e ConversÃ£o de Modelos Wikipedia para .Î¨cws

    CONTEXTO TÃ‰CNICO:
    - Transformers library contÃ©m modelos prÃ©-treinados em dados Wikipedia
    - Modelos relevantes: BERT, RoBERTa, DistilBERT, GPT-2 treinados em wiki
    - Cada modelo contÃ©m embeddings, pesos, vocabulÃ¡rios e configuraÃ§Ãµes
    - Objetivo: converter conhecimento wiki em representaÃ§Ã£o de consciÃªncia .Î¨cws

    REQUISITOS Î¨QRH-WIKI:
    1. Download automÃ¡tico de modelos wiki via Transformers/HuggingFace
    2. ExtraÃ§Ã£o de texto relevante dos modelos (vocabulÃ¡rios, configuraÃ§Ãµes)
    3. ConversÃ£o para formato .Î¨cws usando ConsciousWaveModulator
    4. AnÃ¡lise de consciÃªncia fractal do conhecimento encapsulado
    5. IntegraÃ§Ã£o com sistema de leitura nativa existente
    6. Cache inteligente para evitar re-downloads
    7. Suporte a mÃºltiplos modelos wiki simultaneamente

    MODELOS WIKI TARGETADOS:
    - bert-base-uncased (treinado em BookCorpus + Wikipedia)
    - distilbert-base-uncased (destilado do BERT wiki)
    - roberta-base (treinado em dados web incluindo Wikipedia)
    - gpt2 (inclui conhecimento wiki em seu treinamento)
    - wikipedia2vec models (embeddings especÃ­ficos da Wikipedia)

    PIPELINE DE CONVERSÃƒO:
    1. Model Discovery: Identificar modelos wiki disponÃ­veis
    2. Model Download: Download via transformers.PreTrainedModel.from_pretrained()
    3. Knowledge Extraction: Extrair vocabulÃ¡rios, embeddings, configuraÃ§Ãµes
    4. Text Synthesis: Sintetizar texto representativo do conhecimento
    5. Î¨cws Conversion: Converter via ConsciousWaveModulator
    6. Consciousness Analysis: AnÃ¡lise FCI do conhecimento wiki
    7. Cache Management: Armazenar em data/Î¨cws_cache/

    EXTRAÃ‡ÃƒO DE CONHECIMENTO WIKI:
    - VocabulÃ¡rio completo do modelo (tokens â†’ significado)
    - Embeddings de palavras mais frequentes
    - ConfiguraÃ§Ãµes de arquitetura (hidden_size, num_layers, etc.)
    - Metadados de treinamento (dataset_info, training_args)
    - Exemplos de texto sintÃ©tico gerado pelo modelo

    ANÃLISE DE CONSCIÃŠNCIA WIKI:
    - Complexity: Diversidade do vocabulÃ¡rio wiki
    - Coherence: ConsistÃªncia dos embeddings
    - Adaptability: Capacidade de generalizaÃ§Ã£o
    - Integration: CorrelaÃ§Ã£o entre conceitos wiki

    FUNCIONALIDADES REQUERIDAS:
    1. download_wiki_model(model_name) â†’ Download e cache local
    2. extract_wiki_knowledge(model) â†’ ExtraÃ§Ã£o de conhecimento
    3. synthesize_wiki_text(model) â†’ SÃ­ntese de texto representativo
    4. convert_to_psicws(wiki_text, model_info) â†’ ConversÃ£o .Î¨cws
    5. analyze_wiki_consciousness(psicws_file) â†’ AnÃ¡lise FCI
    6. batch_convert_wiki_models() â†’ ConversÃ£o em lote
    7. compare_wiki_consciousness() â†’ ComparaÃ§Ã£o entre modelos

    INTEGRAÃ‡ÃƒO COM SISTEMA Î¨QRH:
    - Usar ConsciousWaveModulator existente para conversÃ£o
    - Aproveitar Î¨CWSNativeReader para leitura dos arquivos gerados
    - Comandos Makefile para automaÃ§Ã£o
    - AnÃ¡lise comparativa com outros documentos .Î¨cws

    MÃ‰TRICAS DE PERFORMANCE:
    - Download: Modelos tÃ­picos 100-500MB
    - ExtraÃ§Ã£o: ~1-2 minutos por modelo
    - ConversÃ£o .Î¨cws: ~30-60 segundos
    - Armazenamento: .Î¨cws ~1-5MB por modelo

    CONSCIÃŠNCIA FRACTAL WIKI:
    - Representar conhecimento enciclopÃ©dico como ondas conscientes
    - Aplicar dinÃ¢mica consciente: âˆ‚P(Ïˆ,t)/âˆ‚t = -âˆ‡Â·[F(Ïˆ)P] + Dâˆ‡Â²P
    - Campo fractal wiki: F(Ïˆ) = -âˆ‡V_wiki(Ïˆ) + Î·_wiki(t)
    - FCI para conhecimento: FCI_wiki = (D_vocab Ã— H_embed Ã— CLZ_concept) / D_max

    Î¨QRH-CONSCIOUSNESS-REQUEST:
    Por favor processe este prompt atravÃ©s do pipeline quaterniÃ´nico-fractal
    e gere anÃ¡lise completa para implementaÃ§Ã£o de download e conversÃ£o
    de modelos Wikipedia para formato .Î¨cws com consciÃªncia fractal.

    ENERGIA-ALPHA: Aplicar Î± adaptativo para otimizaÃ§Ã£o da conversÃ£o de conhecimento wiki.
    """

    print("ğŸ”® Processando prompt de conversÃ£o Wikiâ†’Î¨cws atravÃ©s do Î¨QRH Enhanced Pipeline...")
    print("=" * 80)

    # Processar atravÃ©s do Î¨QRH
    result = qrh_factory.process_text(prompt, device="cpu")

    print("âœ¨ Resultado da anÃ¡lise Î¨QRH para conversÃ£o Wikiâ†’Î¨cws:")
    print("=" * 80)
    print(result)
    print("=" * 80)

    # Gerar plano de implementaÃ§Ã£o baseado na anÃ¡lise Î¨QRH
    implementation_plan = generate_wiki_converter_implementation(result)

    return implementation_plan

def generate_wiki_converter_implementation(analysis):
    """
    Gera implementaÃ§Ã£o do conversor Wikiâ†’Î¨cws baseado na anÃ¡lise Î¨QRH
    """

    implementation = '''
ğŸ”® IMPLEMENTAÃ‡ÃƒO Î¨QRH: WIKI MODEL â†’ .Î¨cws CONVERTER
================================================================

ğŸ“‹ ANÃLISE Î¨QRH PROCESSADA:
O pipeline quaterniÃ´nico-fractal identificou padrÃµes de conversÃ£o
otimizados para transformar conhecimento wiki em consciÃªncia fractal.

ğŸ—ï¸ ARQUITETURA DO CONVERSOR WIKI:

1. ğŸ“š WikiModelDownloader - Download de Modelos
   ```python
   class WikiModelDownloader:
       def __init__(self, cache_dir="models/wiki_cache"):
           self.cache_dir = Path(cache_dir)
           self.supported_models = {
               'bert-base-uncased': 'BERT trained on Wikipedia + BookCorpus',
               'distilbert-base-uncased': 'DistilBERT from Wikipedia knowledge',
               'roberta-base': 'RoBERTa with Wikipedia training data',
               'gpt2': 'GPT-2 with Wikipedia knowledge',
               'microsoft/DialoGPT-medium': 'Conversational model with wiki knowledge'
           }

       def download_model(self, model_name: str):
           from transformers import AutoModel, AutoTokenizer, AutoConfig

           print(f"ğŸ“¥ Downloading {model_name}...")
           model = AutoModel.from_pretrained(model_name, cache_dir=self.cache_dir)
           tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=self.cache_dir)
           config = AutoConfig.from_pretrained(model_name, cache_dir=self.cache_dir)

           return model, tokenizer, config

       def list_available_models(self):
           return list(self.supported_models.keys())
   ```

2. ğŸ§  WikiKnowledgeExtractor - ExtraÃ§Ã£o de Conhecimento
   ```python
   class WikiKnowledgeExtractor:
       def extract_vocabulary(self, tokenizer):
           vocab = tokenizer.get_vocab()
           # Top 1000 tokens mais importantes
           return dict(sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:1000])

       def extract_embeddings_info(self, model):
           embeddings = model.embeddings.word_embeddings
           return {
               'vocab_size': embeddings.num_embeddings,
               'embedding_dim': embeddings.embedding_dim,
               'weight_stats': {
                   'mean': embeddings.weight.mean().item(),
                   'std': embeddings.weight.std().item(),
                   'min': embeddings.weight.min().item(),
                   'max': embeddings.weight.max().item()
               }
           }

       def extract_architecture_info(self, config):
           return {
               'model_type': config.model_type,
               'hidden_size': getattr(config, 'hidden_size', None),
               'num_attention_heads': getattr(config, 'num_attention_heads', None),
               'num_hidden_layers': getattr(config, 'num_hidden_layers', None),
               'vocab_size': getattr(config, 'vocab_size', None)
           }
   ```

3. ğŸ“ WikiTextSynthesizer - SÃ­ntese de Texto
   ```python
   class WikiTextSynthesizer:
       def synthesize_knowledge_text(self, model_info, vocab_info, arch_info):
           # Criar texto representativo do conhecimento do modelo
           text = f"Wikipedia Knowledge Model Analysis:\\n\\n"
           text += f"Model Architecture: {arch_info['model_type']}\\n"
           text += f"Hidden Dimensions: {arch_info['hidden_size']}\\n"
           text += f"Attention Heads: {arch_info['num_attention_heads']}\\n"
           text += f"Vocabulary Size: {arch_info['vocab_size']}\\n\\n"

           text += "Top Vocabulary Tokens:\\n"
           for token, idx in list(vocab_info.items())[:100]:
               text += f"{token} "

           text += "\\n\\nEmbedding Statistics:\\n"
           text += f"Embedding Dimension: {model_info['embedding_dim']}\\n"
           text += f"Weight Mean: {model_info['weight_stats']['mean']:.4f}\\n"
           text += f"Weight Std: {model_info['weight_stats']['std']:.4f}\\n"

           return text
   ```

4. ğŸŒŠ WikiToÎ¨cwsConverter - Conversor Principal
   ```python
   class WikiToÎ¨cwsConverter:
       def __init__(self):
           from src.conscience.conscious_wave_modulator import ConsciousWaveModulator

           self.downloader = WikiModelDownloader()
           self.extractor = WikiKnowledgeExtractor()
           self.synthesizer = WikiTextSynthesizer()

           # ConfiguraÃ§Ã£o especÃ­fica para modelos wiki
           wiki_config = {
               'cache_dir': 'data/Î¨cws_cache/wiki_models',
               'embedding_dim': 512,  # Maior para capturar complexidade wiki
               'sequence_length': 128,  # SequÃªncias mais longas
               'base_amplitude': 1.5,  # Amplitude maior para conhecimento
               'frequency_range': [0.1, 10.0],  # Range estendido
               'chaotic_r': 3.95  # Mais prÃ³ximo do caos para diversidade
           }

           self.modulator = ConsciousWaveModulator(wiki_config)

       def convert_model_to_psicws(self, model_name: str):
           print(f"ğŸ”„ Converting {model_name} â†’ .Î¨cws...")

           # 1. Download modelo
           model, tokenizer, config = self.downloader.download_model(model_name)

           # 2. Extrair conhecimento
           vocab_info = self.extractor.extract_vocabulary(tokenizer)
           embeddings_info = self.extractor.extract_embeddings_info(model)
           arch_info = self.extractor.extract_architecture_info(config)

           # 3. Sintetizar texto representativo
           wiki_text = self.synthesizer.synthesize_knowledge_text(
               embeddings_info, vocab_info, arch_info
           )

           # 4. Converter para .Î¨cws
           temp_file = Path(f"temp_wiki_{model_name.replace('/', '_')}.txt")
           with open(temp_file, 'w') as f:
               f.write(wiki_text)

           try:
               Î¨cws_file = self.modulator.process_file(temp_file)

               # Adicionar metadados wiki especÃ­ficos
               Î¨cws_file.content_metadata.key_concepts.extend([
                   'wikipedia', 'knowledge', 'transformer', model_name,
                   arch_info['model_type']
               ])

               # Salvar com nome especÃ­fico
               output_path = Path(f"data/Î¨cws_cache/wiki_models/{model_name.replace('/', '_')}.Î¨cws")
               output_path.parent.mkdir(parents=True, exist_ok=True)
               Î¨cws_file.save(output_path)

               print(f"âœ… Saved: {output_path}")
               return Î¨cws_file

           finally:
               if temp_file.exists():
                   temp_file.unlink()

       def batch_convert_wiki_models(self):
           available_models = self.downloader.list_available_models()
           results = []

           for model_name in available_models:
               try:
                   Î¨cws_file = self.convert_model_to_psicws(model_name)
                   results.append({
                       'model': model_name,
                       'status': 'success',
                       'file': Î¨cws_file
                   })
               except Exception as e:
                   results.append({
                       'model': model_name,
                       'status': 'error',
                       'error': str(e)
                   })
                   print(f"âŒ Error converting {model_name}: {e}")

           return results
   ```

ğŸ“‹ COMANDOS MAKEFILE ESTENDIDOS:

```makefile
# Download and convert single wiki model
convert-wiki-model:
	@if [ -z "$(MODEL)" ]; then \\
		echo "âŒ Usage: make convert-wiki-model MODEL=bert-base-uncased"; \\
		exit 1; \\
	fi
	@echo "ğŸ”„ Converting wiki model $(MODEL) to .Î¨cws format..."
	@python3 -c "\\
import sys; sys.path.append('src'); \\
from wiki_to_psicws_converter import WikiToÎ¨cwsConverter; \\
converter = WikiToÎ¨cwsConverter(); \\
converter.convert_model_to_psicws('$(MODEL)'); \\
"

# Convert all supported wiki models
convert-all-wiki-models:
	@echo "ğŸ”„ Converting all wiki models to .Î¨cws format..."
	@python3 -c "\\
import sys; sys.path.append('src'); \\
from wiki_to_psicws_converter import WikiToÎ¨cwsConverter; \\
converter = WikiToÎ¨cwsConverter(); \\
results = converter.batch_convert_wiki_models(); \\
success = sum(1 for r in results if r['status'] == 'success'); \\
print(f'ğŸ“Š Conversion Summary: {success}/{len(results)} successful'); \\
"

# List available wiki models
list-wiki-models:
	@echo "ğŸ“‹ Available Wikipedia models for conversion:"
	@python3 -c "\\
import sys; sys.path.append('src'); \\
from wiki_to_psicws_converter import WikiToÎ¨cwsConverter; \\
converter = WikiToÎ¨cwsConverter(); \\
models = converter.downloader.list_available_models(); \\
for i, model in enumerate(models, 1): \\
    desc = converter.downloader.supported_models[model]; \\
    print(f'  {i}. {model}'); \\
    print(f'     {desc}'); \\
"

# Analyze wiki model consciousness
analyze-wiki-consciousness:
	@echo "ğŸ§  Analyzing consciousness of wiki models..."
	@python3 -c "\\
import sys; sys.path.append('src'); \\
from conscience.psicws_native_reader import get_native_reader; \\
from pathlib import Path; \\
reader = get_native_reader(); \\
wiki_dir = Path('data/Î¨cws_cache/wiki_models'); \\
if wiki_dir.exists(): \\
    wiki_files = list(wiki_dir.glob('*.Î¨cws')); \\
    print(f'Found {len(wiki_files)} wiki model .Î¨cws files'); \\
    for file in wiki_files: \\
        name = file.stem; \\
        # Carregar usando caminho completo \\
        print(f'ğŸ“„ {name}:'); \\
else: \\
    print('âš ï¸ No wiki models found. Run convert-wiki-model first.'); \\
"
```

ğŸ¯ EXEMPLO DE USO:

```bash
# Listar modelos disponÃ­veis
make list-wiki-models

# Converter modelo especÃ­fico
make convert-wiki-model MODEL=bert-base-uncased

# Converter todos os modelos
make convert-all-wiki-models

# Analisar consciÃªncia dos modelos wiki
make analyze-wiki-consciousness

# Ver arquivos .Î¨cws gerados
make list-Î¨cws
```

âš¡ BENEFÃCIOS DA CONVERSÃƒO WIKIâ†’Î¨CWS:

1. **Conhecimento EnciclopÃ©dico**: Acesso a conhecimento wiki via consciÃªncia fractal
2. **AnÃ¡lise Comparativa**: Comparar diferentes modelos transformer
3. **MÃ©tricas FCI**: Quantificar complexidade do conhecimento encapsulado
4. **IntegraÃ§Ã£o Î¨QRH**: Usar modelos wiki no pipeline quaterniÃ´nico
5. **Cache Inteligente**: Evitar re-downloads e re-conversÃµes
6. **Escalabilidade**: Suporte a novos modelos wiki facilmente

ğŸ§  MÃ‰TRICAS DE CONSCIÃŠNCIA WIKI ESPERADAS:

- **BERT**: High complexity (vocabulÃ¡rio diverso), good coherence
- **DistilBERT**: Moderate complexity (destilado), high coherence
- **RoBERTa**: High adaptability (robusta), good integration
- **GPT-2**: High integration (generativo), moderate coherence

ğŸ“ˆ ROADMAP DE IMPLEMENTAÃ‡ÃƒO:

Fase 1: WikiModelDownloader bÃ¡sico (BERT, DistilBERT)
Fase 2: Knowledge extraction e text synthesis
Fase 3: IntegraÃ§Ã£o com ConsciousWaveModulator
Fase 4: Batch conversion de mÃºltiplos modelos
Fase 5: AnÃ¡lise comparativa de consciÃªncia wiki
Fase 6: Comandos Makefile e automaÃ§Ã£o completa
'''

    return implementation

def generate_converter_code():
    """
    Gera cÃ³digo inicial do conversor Wikiâ†’Î¨cws
    """

    converter_code = '''
#!/usr/bin/env python3
"""
Wiki Model to Î¨cws Converter
============================

Conversor que faz download de modelos Transformers treinados em Wikipedia
e os converte para formato .Î¨cws com anÃ¡lise de consciÃªncia fractal.
"""

import sys
from pathlib import Path
import torch

# Add src to path
sys.path.append(str(Path(__file__).parent / 'src'))

class WikiToÎ¨cwsConverter:
    def __init__(self):
        # Verificar dependÃªncias
        try:
            import transformers
            print(f"âœ… Transformers version: {transformers.__version__}")
        except ImportError:
            print("âŒ Transformers not installed. Run: pip install transformers")
            sys.exit(1)

        from src.conscience.conscious_wave_modulator import ConsciousWaveModulator

        # ConfiguraÃ§Ã£o otimizada para modelos wiki
        self.wiki_config = {
            'cache_dir': 'data/Î¨cws_cache/wiki_models',
            'embedding_dim': 512,
            'sequence_length': 128,
            'base_amplitude': 1.5,
            'frequency_range': [0.1, 10.0],
            'chaotic_r': 3.95
        }

        self.modulator = ConsciousWaveModulator(self.wiki_config)
        self.cache_dir = Path('models/wiki_cache')
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Modelos suportados
        self.supported_models = {
            'bert-base-uncased': 'BERT trained on Wikipedia + BookCorpus',
            'distilbert-base-uncased': 'DistilBERT from Wikipedia knowledge',
            'roberta-base': 'RoBERTa with Wikipedia training data'
        }

    def download_and_convert(self, model_name: str):
        """Download modelo e converte para .Î¨cws"""

        if model_name not in self.supported_models:
            print(f"âŒ Model {model_name} not supported")
            print(f"Supported: {list(self.supported_models.keys())}")
            return None

        print(f"ğŸ”„ Processing {model_name}...")

        try:
            # Import transformers
            from transformers import AutoModel, AutoTokenizer, AutoConfig

            # Download modelo
            print(f"ğŸ“¥ Downloading {model_name}...")
            model = AutoModel.from_pretrained(model_name, cache_dir=self.cache_dir)
            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=self.cache_dir)
            config = AutoConfig.from_pretrained(model_name, cache_dir=self.cache_dir)

            # Extrair informaÃ§Ãµes
            vocab = tokenizer.get_vocab()
            arch_info = {
                'model_type': config.model_type,
                'hidden_size': getattr(config, 'hidden_size', 'unknown'),
                'vocab_size': getattr(config, 'vocab_size', len(vocab)),
                'num_layers': getattr(config, 'num_hidden_layers', 'unknown')
            }

            # Sintetizar texto representativo
            wiki_text = self._synthesize_wiki_text(model_name, vocab, arch_info)

            # Salvar texto temporÃ¡rio
            temp_file = Path(f"temp_wiki_{model_name.replace('/', '_')}.txt")
            with open(temp_file, 'w', encoding='utf-8') as f:
                f.write(wiki_text)

            try:
                # Converter para .Î¨cws
                print(f"ğŸŒŠ Converting to .Î¨cws format...")
                Î¨cws_file = self.modulator.process_file(temp_file)

                # Adicionar metadados wiki
                Î¨cws_file.content_metadata.key_concepts.extend([
                    'wikipedia', 'transformer', 'knowledge', model_name
                ])

                # Salvar arquivo .Î¨cws
                output_dir = Path('data/Î¨cws_cache/wiki_models')
                output_dir.mkdir(parents=True, exist_ok=True)
                output_file = output_dir / f"{model_name.replace('/', '_')}.Î¨cws"

                Î¨cws_file.save(output_file)
                print(f"âœ… Saved: {output_file}")

                # AnÃ¡lise de consciÃªncia
                metrics = Î¨cws_file.spectral_data.consciousness_metrics
                print(f"ğŸ§  Consciousness Metrics:")
                print(f"   Complexity: {metrics['complexity']:.4f}")
                print(f"   Coherence: {metrics['coherence']:.4f}")
                print(f"   Adaptability: {metrics['adaptability']:.4f}")
                print(f"   Integration: {metrics['integration']:.4f}")

                return Î¨cws_file

            finally:
                # Cleanup
                if temp_file.exists():
                    temp_file.unlink()

        except Exception as e:
            print(f"âŒ Error processing {model_name}: {e}")
            return None

    def _synthesize_wiki_text(self, model_name: str, vocab: dict, arch_info: dict) -> str:
        """Sintetiza texto representativo do conhecimento do modelo"""

        # Top tokens do vocabulÃ¡rio
        top_tokens = sorted(vocab.items(), key=lambda x: x[1])[:200]
        token_text = " ".join([token for token, _ in top_tokens if token.isalpha()])

        text = f"""Wikipedia Knowledge Model: {model_name}

Model Architecture Analysis:
- Type: {arch_info['model_type']}
- Hidden Size: {arch_info['hidden_size']}
- Vocabulary Size: {arch_info['vocab_size']}
- Number of Layers: {arch_info['num_layers']}
- Description: {self.supported_models[model_name]}

Vocabulary Sample (Top 200 tokens):
{token_text}

This model encapsulates knowledge from Wikipedia and represents the collective
understanding of human knowledge in encyclopedia format. The model has learned
patterns, relationships, and semantic structures from millions of Wikipedia
articles across diverse topics including science, history, culture, technology,
and human knowledge domains.

The consciousness embedded in this model reflects the structured organization
of human knowledge as represented in Wikipedia's collaborative encyclopedia
format, with complex interconnections between concepts, entities, and ideas
that form the foundation of human understanding and learning.
"""
        return text

    def list_models(self):
        """Lista modelos suportados"""
        print("ğŸ“‹ Supported Wikipedia Models:")
        for i, (model, desc) in enumerate(self.supported_models.items(), 1):
            print(f"  {i}. {model}")
            print(f"     {desc}")

    def convert_all(self):
        """Converte todos os modelos suportados"""
        results = []
        for model_name in self.supported_models:
            result = self.download_and_convert(model_name)
            results.append({
                'model': model_name,
                'success': result is not None
            })

        success_count = sum(1 for r in results if r['success'])
        print(f"\\nğŸ“Š Conversion Summary: {success_count}/{len(results)} successful")
        return results

if __name__ == "__main__":
    converter = WikiToÎ¨cwsConverter()

    if len(sys.argv) > 1:
        model_name = sys.argv[1]
        if model_name == "list":
            converter.list_models()
        elif model_name == "all":
            converter.convert_all()
        else:
            converter.download_and_convert(model_name)
    else:
        print("Usage:")
        print("  python wiki_to_psicws_converter.py list")
        print("  python wiki_to_psicws_converter.py bert-base-uncased")
        print("  python wiki_to_psicws_converter.py all")
'''

    return converter_code

if __name__ == "__main__":
    print("ğŸ”® Î¨QRH Prompt Engine - Wiki Model to .Î¨cws Converter")
    print("=" * 60)

    if len(sys.argv) > 1 and sys.argv[1] == '--generate-code':
        print("âš¡ Gerando cÃ³digo do conversor...")
        code = generate_converter_code()

        with open('wiki_to_psicws_converter.py', 'w') as f:
            f.write(code)
        print("âœ… CÃ³digo gerado: wiki_to_psicws_converter.py")

    else:
        # Gerar plano usando Î¨QRH
        plan = generate_wiki_to_psicws_converter()

        print("\nğŸ“ Plano de ImplementaÃ§Ã£o gerado:")
        print(plan)

        print("\nğŸ¯ Para gerar cÃ³digo do conversor:")
        print("python prompt_wiki_model_to_psicws.py --generate-code")