#!/usr/bin/env python3
"""
Œ®QRH Prompt Engine para Download e Convers√£o de Modelo Wiki
===========================================================

Usando o Enhanced Pipeline Œ®QRH para gerar c√≥digo que fa√ßa download
de modelos Wikipedia do Transformers e os converta para formato .Œ®cws
com an√°lise de consci√™ncia fractal.

Pipeline: Prompt ‚Üí Œ®QRH Analysis ‚Üí Model Download ‚Üí Wiki2Œ®cws Conversion
"""

import sys
import os
from pathlib import Path
sys.path.append(str(Path(__file__).parent / 'src'))

from src.core.Œ®QRH import QRHFactory

def generate_wiki_to_psicws_converter():
    """
    Usa Œ®QRH Prompt Engine para gerar sistema de download e convers√£o Wiki‚ÜíŒ®cws
    """

    # Inicializar Œ®QRH Factory
    qrh_factory = QRHFactory()

    # Prompt avan√ßado para o Œ®QRH Engine
    prompt = """
    Œ®QRH-WIKI-MODEL-TASK: Download e Convers√£o de Modelos Wikipedia para .Œ®cws

    CONTEXTO T√âCNICO:
    - Transformers library cont√©m modelos pr√©-treinados em dados Wikipedia
    - Modelos relevantes: BERT, RoBERTa, DistilBERT, GPT-2 treinados em wiki
    - Cada modelo cont√©m embeddings, pesos, vocabul√°rios e configura√ß√µes
    - Objetivo: converter conhecimento wiki em representa√ß√£o de consci√™ncia .Œ®cws

    REQUISITOS Œ®QRH-WIKI:
    1. Download autom√°tico de modelos wiki via Transformers/HuggingFace
    2. Extra√ß√£o de texto relevante dos modelos (vocabul√°rios, configura√ß√µes)
    3. Convers√£o para formato .Œ®cws usando ConsciousWaveModulator
    4. An√°lise de consci√™ncia fractal do conhecimento encapsulado
    5. Integra√ß√£o com sistema de leitura nativa existente
    6. Cache inteligente para evitar re-downloads
    7. Suporte a m√∫ltiplos modelos wiki simultaneamente

    MODELOS WIKI TARGETADOS:
    - bert-base-uncased (treinado em BookCorpus + Wikipedia)
    - distilbert-base-uncased (destilado do BERT wiki)
    - roberta-base (treinado em dados web incluindo Wikipedia)
    - gpt2 (inclui conhecimento wiki em seu treinamento)
    - wikipedia2vec models (embeddings espec√≠ficos da Wikipedia)

    PIPELINE DE CONVERS√ÉO:
    1. Model Discovery: Identificar modelos wiki dispon√≠veis
    2. Model Download: Download via transformers.PreTrainedModel.from_pretrained()
    3. Knowledge Extraction: Extrair vocabul√°rios, embeddings, configura√ß√µes
    4. Text Synthesis: Sintetizar texto representativo do conhecimento
    5. Œ®cws Conversion: Converter via ConsciousWaveModulator
    6. Consciousness Analysis: An√°lise FCI do conhecimento wiki
    7. Cache Management: Armazenar em data/Œ®cws_cache/

    EXTRA√á√ÉO DE CONHECIMENTO WIKI:
    - Vocabul√°rio completo do modelo (tokens ‚Üí significado)
    - Embeddings de palavras mais frequentes
    - Configura√ß√µes de arquitetura (hidden_size, num_layers, etc.)
    - Metadados de treinamento (dataset_info, training_args)
    - Exemplos de texto sint√©tico gerado pelo modelo

    AN√ÅLISE DE CONSCI√äNCIA WIKI:
    - Complexity: Diversidade do vocabul√°rio wiki
    - Coherence: Consist√™ncia dos embeddings
    - Adaptability: Capacidade de generaliza√ß√£o
    - Integration: Correla√ß√£o entre conceitos wiki

    FUNCIONALIDADES REQUERIDAS:
    1. download_wiki_model(model_name) ‚Üí Download e cache local
    2. extract_wiki_knowledge(model) ‚Üí Extra√ß√£o de conhecimento
    3. synthesize_wiki_text(model) ‚Üí S√≠ntese de texto representativo
    4. convert_to_psicws(wiki_text, model_info) ‚Üí Convers√£o .Œ®cws
    5. analyze_wiki_consciousness(psicws_file) ‚Üí An√°lise FCI
    6. batch_convert_wiki_models() ‚Üí Convers√£o em lote
    7. compare_wiki_consciousness() ‚Üí Compara√ß√£o entre modelos

    INTEGRA√á√ÉO COM SISTEMA Œ®QRH:
    - Usar ConsciousWaveModulator existente para convers√£o
    - Aproveitar Œ®CWSNativeReader para leitura dos arquivos gerados
    - Comandos Makefile para automa√ß√£o
    - An√°lise comparativa com outros documentos .Œ®cws

    M√âTRICAS DE PERFORMANCE:
    - Download: Modelos t√≠picos 100-500MB
    - Extra√ß√£o: ~1-2 minutos por modelo
    - Convers√£o .Œ®cws: ~30-60 segundos
    - Armazenamento: .Œ®cws ~1-5MB por modelo

    CONSCI√äNCIA FRACTAL WIKI:
    - Representar conhecimento enciclop√©dico como ondas conscientes
    - Aplicar din√¢mica consciente: ‚àÇP(œà,t)/‚àÇt = -‚àá¬∑[F(œà)P] + D‚àá¬≤P
    - Campo fractal wiki: F(œà) = -‚àáV_wiki(œà) + Œ∑_wiki(t)
    - FCI para conhecimento: FCI_wiki = (D_vocab √ó H_embed √ó CLZ_concept) / D_max

    Œ®QRH-CONSCIOUSNESS-REQUEST:
    Por favor processe este prompt atrav√©s do pipeline quaterni√¥nico-fractal
    e gere an√°lise completa para implementa√ß√£o de download e convers√£o
    de modelos Wikipedia para formato .Œ®cws com consci√™ncia fractal.

    ENERGIA-ALPHA: Aplicar Œ± adaptativo para otimiza√ß√£o da convers√£o de conhecimento wiki.
    """

    print("üîÆ Processando prompt de convers√£o Wiki‚ÜíŒ®cws atrav√©s do Œ®QRH Enhanced Pipeline...")
    print("=" * 80)

    # Processar atrav√©s do Œ®QRH
    result = qrh_factory.process_text(prompt, device="cpu")

    print("‚ú® Resultado da an√°lise Œ®QRH para convers√£o Wiki‚ÜíŒ®cws:")
    print("=" * 80)
    print(result)
    print("=" * 80)

    # Gerar plano de implementa√ß√£o baseado na an√°lise Œ®QRH
    implementation_plan = generate_wiki_converter_implementation(result)

    return implementation_plan

def generate_wiki_converter_implementation(analysis):
    """
    Gera implementa√ß√£o do conversor Wiki‚ÜíŒ®cws baseado na an√°lise Œ®QRH
    """

    implementation = '''
üîÆ IMPLEMENTA√á√ÉO Œ®QRH: WIKI MODEL ‚Üí .Œ®cws CONVERTER
================================================================

üìã AN√ÅLISE Œ®QRH PROCESSADA:
O pipeline quaterni√¥nico-fractal identificou padr√µes de convers√£o
otimizados para transformar conhecimento wiki em consci√™ncia fractal.

üèóÔ∏è ARQUITETURA DO CONVERSOR WIKI:

1. üìö WikiModelDownloader - Download de Modelos
   ```python
   class WikiModelDownloader:
       def __init__(self, cache_dir="models/wiki_cache"):
           self.cache_dir = Path(cache_dir)
           self.supported_models = {
               'bert-base-uncased': 'BERT trained on Wikipedia + BookCorpus',
               'distilbert-base-uncased': 'DistilBERT from Wikipedia knowledge',
               'roberta-base': 'RoBERTa with Wikipedia training data',
               'gpt2': 'GPT-2 with Wikipedia knowledge',
               'microsoft/DialoGPT-medium': 'Conversational model with wiki knowledge'
           }

       def download_model(self, model_name: str):
           from transformers import AutoModel, AutoTokenizer, AutoConfig

           print(f"üì• Downloading {model_name}...")
           model = AutoModel.from_pretrained(model_name, cache_dir=self.cache_dir)
           tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=self.cache_dir)
           config = AutoConfig.from_pretrained(model_name, cache_dir=self.cache_dir)

           return model, tokenizer, config

       def list_available_models(self):
           return list(self.supported_models.keys())
   ```

2. üß† WikiKnowledgeExtractor - Extra√ß√£o de Conhecimento
   ```python
   class WikiKnowledgeExtractor:
       def extract_vocabulary(self, tokenizer):
           vocab = tokenizer.get_vocab()
           # Top 1000 tokens mais importantes
           return dict(sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:1000])

       def extract_embeddings_info(self, model):
           embeddings = model.embeddings.word_embeddings
           return {
               'vocab_size': embeddings.num_embeddings,
               'embedding_dim': embeddings.embedding_dim,
               'weight_stats': {
                   'mean': embeddings.weight.mean().item(),
                   'std': embeddings.weight.std().item(),
                   'min': embeddings.weight.min().item(),
                   'max': embeddings.weight.max().item()
               }
           }

       def extract_architecture_info(self, config):
           return {
               'model_type': config.model_type,
               'hidden_size': getattr(config, 'hidden_size', None),
               'num_attention_heads': getattr(config, 'num_attention_heads', None),
               'num_hidden_layers': getattr(config, 'num_hidden_layers', None),
               'vocab_size': getattr(config, 'vocab_size', None)
           }
   ```

3. üìù WikiTextSynthesizer - S√≠ntese de Texto
   ```python
   class WikiTextSynthesizer:
       def synthesize_knowledge_text(self, model_info, vocab_info, arch_info):
           # Criar texto representativo do conhecimento do modelo
           text = f"Wikipedia Knowledge Model Analysis:\\n\\n"
           text += f"Model Architecture: {arch_info['model_type']}\\n"
           text += f"Hidden Dimensions: {arch_info['hidden_size']}\\n"
           text += f"Attention Heads: {arch_info['num_attention_heads']}\\n"
           text += f"Vocabulary Size: {arch_info['vocab_size']}\\n\\n"

           text += "Top Vocabulary Tokens:\\n"
           for token, idx in list(vocab_info.items())[:100]:
               text += f"{token} "

           text += "\\n\\nEmbedding Statistics:\\n"
           text += f"Embedding Dimension: {model_info['embedding_dim']}\\n"
           text += f"Weight Mean: {model_info['weight_stats']['mean']:.4f}\\n"
           text += f"Weight Std: {model_info['weight_stats']['std']:.4f}\\n"

           return text
   ```

4. üåä WikiToŒ®cwsConverter - Conversor Principal
   ```python
   class WikiToŒ®cwsConverter:
       def __init__(self):
           from src.conscience.conscious_wave_modulator import ConsciousWaveModulator

           self.downloader = WikiModelDownloader()
           self.extractor = WikiKnowledgeExtractor()
           self.synthesizer = WikiTextSynthesizer()

           # Configura√ß√£o espec√≠fica para modelos wiki
           wiki_config = {
               'cache_dir': 'data/Œ®cws_cache/wiki_models',
               'embedding_dim': 512,  # Maior para capturar complexidade wiki
               'sequence_length': 128,  # Sequ√™ncias mais longas
               'base_amplitude': 1.5,  # Amplitude maior para conhecimento
               'frequency_range': [0.1, 10.0],  # Range estendido
               'chaotic_r': 3.95  # Mais pr√≥ximo do caos para diversidade
           }

           self.modulator = ConsciousWaveModulator(wiki_config)

       def convert_model_to_psicws(self, model_name: str):
           print(f"üîÑ Converting {model_name} ‚Üí .Œ®cws...")

           # 1. Download modelo
           model, tokenizer, config = self.downloader.download_model(model_name)

           # 2. Extrair conhecimento
           vocab_info = self.extractor.extract_vocabulary(tokenizer)
           embeddings_info = self.extractor.extract_embeddings_info(model)
           arch_info = self.extractor.extract_architecture_info(config)

           # 3. Sintetizar texto representativo
           wiki_text = self.synthesizer.synthesize_knowledge_text(
               embeddings_info, vocab_info, arch_info
           )

           # 4. Converter para .Œ®cws
           temp_file = Path(f"temp_wiki_{model_name.replace('/', '_')}.txt")
           with open(temp_file, 'w') as f:
               f.write(wiki_text)

           try:
               Œ®cws_file = self.modulator.process_file(temp_file)

               # Adicionar metadados wiki espec√≠ficos
               Œ®cws_file.content_metadata.key_concepts.extend([
                   'wikipedia', 'knowledge', 'transformer', model_name,
                   arch_info['model_type']
               ])

               # Salvar com nome espec√≠fico
               output_path = Path(f"data/Œ®cws_cache/wiki_models/{model_name.replace('/', '_')}.Œ®cws")
               output_path.parent.mkdir(parents=True, exist_ok=True)
               Œ®cws_file.save(output_path)

               print(f"‚úÖ Saved: {output_path}")
               return Œ®cws_file

           finally:
               if temp_file.exists():
                   temp_file.unlink()

       def batch_convert_wiki_models(self):
           available_models = self.downloader.list_available_models()
           results = []

           for model_name in available_models:
               try:
                   Œ®cws_file = self.convert_model_to_psicws(model_name)
                   results.append({
                       'model': model_name,
                       'status': 'success',
                       'file': Œ®cws_file
                   })
               except Exception as e:
                   results.append({
                       'model': model_name,
                       'status': 'error',
                       'error': str(e)
                   })
                   print(f"‚ùå Error converting {model_name}: {e}")

           return results
   ```

üìã COMANDOS MAKEFILE ESTENDIDOS:

```makefile
# Download and convert single wiki model
convert-wiki-model:
	@if [ -z "$(MODEL)" ]; then \\
		echo "‚ùå Usage: make convert-wiki-model MODEL=bert-base-uncased"; \\
		exit 1; \\
	fi
	@echo "üîÑ Converting wiki model $(MODEL) to .Œ®cws format..."
	@python3 -c "\\
import sys; sys.path.append('src'); \\
from wiki_to_psicws_converter import WikiToŒ®cwsConverter; \\
converter = WikiToŒ®cwsConverter(); \\
converter.convert_model_to_psicws('$(MODEL)'); \\
"

# Convert all supported wiki models
convert-all-wiki-models:
	@echo "üîÑ Converting all wiki models to .Œ®cws format..."
	@python3 -c "\\
import sys; sys.path.append('src'); \\
from wiki_to_psicws_converter import WikiToŒ®cwsConverter; \\
converter = WikiToŒ®cwsConverter(); \\
results = converter.batch_convert_wiki_models(); \\
success = sum(1 for r in results if r['status'] == 'success'); \\
print(f'üìä Conversion Summary: {success}/{len(results)} successful'); \\
"

# List available wiki models
list-wiki-models:
	@echo "üìã Available Wikipedia models for conversion:"
	@python3 -c "\\
import sys; sys.path.append('src'); \\
from wiki_to_psicws_converter import WikiToŒ®cwsConverter; \\
converter = WikiToŒ®cwsConverter(); \\
models = converter.downloader.list_available_models(); \\
for i, model in enumerate(models, 1): \\
    desc = converter.downloader.supported_models[model]; \\
    print(f'  {i}. {model}'); \\
    print(f'     {desc}'); \\
"

# Analyze wiki model consciousness
analyze-wiki-consciousness:
	@echo "üß† Analyzing consciousness of wiki models..."
	@python3 -c "\\
import sys; sys.path.append('src'); \\
from conscience.psicws_native_reader import get_native_reader; \\
from pathlib import Path; \\
reader = get_native_reader(); \\
wiki_dir = Path('data/Œ®cws_cache/wiki_models'); \\
if wiki_dir.exists(): \\
    wiki_files = list(wiki_dir.glob('*.Œ®cws')); \\
    print(f'Found {len(wiki_files)} wiki model .Œ®cws files'); \\
    for file in wiki_files: \\
        name = file.stem; \\
        # Carregar usando caminho completo \\
        print(f'üìÑ {name}:'); \\
else: \\
    print('‚ö†Ô∏è No wiki models found. Run convert-wiki-model first.'); \\
"
```

üéØ EXEMPLO DE USO:

```bash
# Listar modelos dispon√≠veis
make list-wiki-models

# Converter modelo espec√≠fico
make convert-wiki-model MODEL=bert-base-uncased

# Converter todos os modelos
make convert-all-wiki-models

# Analisar consci√™ncia dos modelos wiki
make analyze-wiki-consciousness

# Ver arquivos .Œ®cws gerados
make list-Œ®cws
```

‚ö° BENEF√çCIOS DA CONVERS√ÉO WIKI‚ÜíŒ®CWS:

1. **Conhecimento Enciclop√©dico**: Acesso a conhecimento wiki via consci√™ncia fractal
2. **An√°lise Comparativa**: Comparar diferentes modelos transformer
3. **M√©tricas FCI**: Quantificar complexidade do conhecimento encapsulado
4. **Integra√ß√£o Œ®QRH**: Usar modelos wiki no pipeline quaterni√¥nico
5. **Cache Inteligente**: Evitar re-downloads e re-convers√µes
6. **Escalabilidade**: Suporte a novos modelos wiki facilmente

üß† M√âTRICAS DE CONSCI√äNCIA WIKI ESPERADAS:

- **BERT**: High complexity (vocabul√°rio diverso), good coherence
- **DistilBERT**: Moderate complexity (destilado), high coherence
- **RoBERTa**: High adaptability (robusta), good integration
- **GPT-2**: High integration (generativo), moderate coherence

üìà ROADMAP DE IMPLEMENTA√á√ÉO:

Fase 1: WikiModelDownloader b√°sico (BERT, DistilBERT)
Fase 2: Knowledge extraction e text synthesis
Fase 3: Integra√ß√£o com ConsciousWaveModulator
Fase 4: Batch conversion de m√∫ltiplos modelos
Fase 5: An√°lise comparativa de consci√™ncia wiki
Fase 6: Comandos Makefile e automa√ß√£o completa
'''

    return implementation

def generate_converter_code():
    """
    Gera c√≥digo inicial do conversor Wiki‚ÜíŒ®cws
    """

    converter_code = '''
#!/usr/bin/env python3
"""
Wiki Model to Œ®cws Converter
============================

Conversor que faz download de modelos Transformers treinados em Wikipedia
e os converte para formato .Œ®cws com an√°lise de consci√™ncia fractal.
"""

import sys
from pathlib import Path
import torch

# Add src to path
sys.path.append(str(Path(__file__).parent / 'src'))

class WikiToŒ®cwsConverter:
    def __init__(self):
        # Verificar depend√™ncias
        try:
            import transformers
            print(f"‚úÖ Transformers version: {transformers.__version__}")
        except ImportError:
            print("‚ùå Transformers not installed. Run: pip install transformers")
            sys.exit(1)

        from src.conscience.conscious_wave_modulator import ConsciousWaveModulator

        # Configura√ß√£o otimizada para modelos wiki
        self.wiki_config = {
            'cache_dir': 'data/Œ®cws_cache/wiki_models',
            'embedding_dim': 512,
            'sequence_length': 128,
            'base_amplitude': 1.5,
            'frequency_range': [0.1, 10.0],
            'chaotic_r': 3.95
        }

        self.modulator = ConsciousWaveModulator(self.wiki_config)
        self.cache_dir = Path('models/wiki_cache')
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Modelos suportados
        self.supported_models = {
            'bert-base-uncased': 'BERT trained on Wikipedia + BookCorpus',
            'distilbert-base-uncased': 'DistilBERT from Wikipedia knowledge',
            'roberta-base': 'RoBERTa with Wikipedia training data'
        }

    def download_and_convert(self, model_name: str):
        """Download modelo e converte para .Œ®cws"""

        if model_name not in self.supported_models:
            print(f"‚ùå Model {model_name} not supported")
            print(f"Supported: {list(self.supported_models.keys())}")
            return None

        print(f"üîÑ Processing {model_name}...")

        try:
            # Import transformers
            from transformers import AutoModel, AutoTokenizer, AutoConfig

            # Download modelo
            print(f"üì• Downloading {model_name}...")
            model = AutoModel.from_pretrained(model_name, cache_dir=self.cache_dir)
            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=self.cache_dir)
            config = AutoConfig.from_pretrained(model_name, cache_dir=self.cache_dir)

            # Extrair informa√ß√µes
            vocab = tokenizer.get_vocab()
            arch_info = {
                'model_type': config.model_type,
                'hidden_size': getattr(config, 'hidden_size', 'unknown'),
                'vocab_size': getattr(config, 'vocab_size', len(vocab)),
                'num_layers': getattr(config, 'num_hidden_layers', 'unknown')
            }

            # Sintetizar texto representativo
            wiki_text = self._synthesize_wiki_text(model_name, vocab, arch_info)

            # Salvar texto tempor√°rio
            temp_file = Path(f"temp_wiki_{model_name.replace('/', '_')}.txt")
            with open(temp_file, 'w', encoding='utf-8') as f:
                f.write(wiki_text)

            try:
                # Converter para .Œ®cws
                print(f"üåä Converting to .Œ®cws format...")
                Œ®cws_file = self.modulator.process_file(temp_file)

                # Adicionar metadados wiki
                Œ®cws_file.content_metadata.key_concepts.extend([
                    'wikipedia', 'transformer', 'knowledge', model_name
                ])

                # Salvar arquivo .Œ®cws
                output_dir = Path('data/Œ®cws_cache/wiki_models')
                output_dir.mkdir(parents=True, exist_ok=True)
                output_file = output_dir / f"{model_name.replace('/', '_')}.Œ®cws"

                Œ®cws_file.save(output_file)
                print(f"‚úÖ Saved: {output_file}")

                # An√°lise de consci√™ncia
                metrics = Œ®cws_file.spectral_data.consciousness_metrics
                print(f"üß† Consciousness Metrics:")
                print(f"   Complexity: {metrics['complexity']:.4f}")
                print(f"   Coherence: {metrics['coherence']:.4f}")
                print(f"   Adaptability: {metrics['adaptability']:.4f}")
                print(f"   Integration: {metrics['integration']:.4f}")

                return Œ®cws_file

            finally:
                # Cleanup
                if temp_file.exists():
                    temp_file.unlink()

        except Exception as e:
            print(f"‚ùå Error processing {model_name}: {e}")
            return None

    def _synthesize_wiki_text(self, model_name: str, vocab: dict, arch_info: dict) -> str:
        """Sintetiza texto representativo do conhecimento do modelo"""

        # Top tokens do vocabul√°rio
        top_tokens = sorted(vocab.items(), key=lambda x: x[1])[:200]
        token_text = " ".join([token for token, _ in top_tokens if token.isalpha()])

        text = f"""Wikipedia Knowledge Model: {model_name}

Model Architecture Analysis:
- Type: {arch_info['model_type']}
- Hidden Size: {arch_info['hidden_size']}
- Vocabulary Size: {arch_info['vocab_size']}
- Number of Layers: {arch_info['num_layers']}
- Description: {self.supported_models[model_name]}

Vocabulary Sample (Top 200 tokens):
{token_text}

This model encapsulates knowledge from Wikipedia and represents the collective
understanding of human knowledge in encyclopedia format. The model has learned
patterns, relationships, and semantic structures from millions of Wikipedia
articles across diverse topics including science, history, culture, technology,
and human knowledge domains.

The consciousness embedded in this model reflects the structured organization
of human knowledge as represented in Wikipedia's collaborative encyclopedia
format, with complex interconnections between concepts, entities, and ideas
that form the foundation of human understanding and learning.
"""
        return text

    def list_models(self):
        """Lista modelos suportados"""
        print("üìã Supported Wikipedia Models:")
        for i, (model, desc) in enumerate(self.supported_models.items(), 1):
            print(f"  {i}. {model}")
            print(f"     {desc}")

    def convert_all(self):
        """Converte todos os modelos suportados"""
        results = []
        for model_name in self.supported_models:
            result = self.download_and_convert(model_name)
            results.append({
                'model': model_name,
                'success': result is not None
            })

        success_count = sum(1 for r in results if r['success'])
        print(f"\\nüìä Conversion Summary: {success_count}/{len(results)} successful")
        return results

if __name__ == "__main__":
    converter = WikiToŒ®cwsConverter()

    if len(sys.argv) > 1:
        model_name = sys.argv[1]
        if model_name == "list":
            converter.list_models()
        elif model_name == "all":
            converter.convert_all()
        else:
            converter.download_and_convert(model_name)
    else:
        print("Usage:")
        print("  python wiki_to_psicws_converter.py list")
        print("  python wiki_to_psicws_converter.py bert-base-uncased")
        print("  python wiki_to_psicws_converter.py all")
'''

    return converter_code

if __name__ == "__main__":
    print("üîÆ Œ®QRH Prompt Engine - Wiki Model to .Œ®cws Converter")
    print("=" * 60)

    if len(sys.argv) > 1 and sys.argv[1] == '--generate-code':
        print("‚ö° Gerando c√≥digo do conversor...")
        code = generate_converter_code()

        with open('wiki_to_psicws_converter.py', 'w') as f:
            f.write(code)
        print("‚úÖ C√≥digo gerado: wiki_to_psicws_converter.py")

    else:
        # Gerar plano usando Œ®QRH
        plan = generate_wiki_to_psicws_converter()

        print("\nüìù Plano de Implementa√ß√£o gerado:")
        print(plan)

        print("\nüéØ Para gerar c√≥digo do conversor:")
        print("python prompt_wiki_model_to_psicws.py --generate-code")