#!/usr/bin/env python3
"""
Memory Benchmark Test - Diagn√≥stico de Efici√™ncia do Œ®QRH Transformer
====================================================================

Script para quantificar e identificar a origem do consumo excessivo de mem√≥ria
e par√¢metros do Œ®QRH Transformer. Compara sistematicamente o modelo Œ®QRH
contra um Transformer padr√£o em ambientes de CPU e GPU.

Problema identificado: Œ®QRH consome ~388% mais mem√≥ria e tem ~5x mais
par√¢metros (215M vs 44M) que o baseline.

Hip√≥tese: A inefici√™ncia √© significativamente pior em CPU do que em GPU.
"""

import torch
import torch.nn as nn
from typing import Dict, Any, Optional
import gc
import sys
import os

# Adicionar o diret√≥rio src ao path para importar m√≥dulos locais
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))


def setup_model(model_type: str, device: str) -> nn.Module:
    """
    Instancia um modelo (Œ®QRH ou padr√£o) e move para o dispositivo de teste.

    Args:
        model_type: 'psiqrh' ou 'standard'
        device: 'cpu' ou 'cuda'

    Returns:
        Modelo instanciado no dispositivo
    """
    # Configura√ß√£o otimizada para CPU
    vocab_size = 5000   # Vocabul√°rio menor
    d_model = 256       # Dimens√£o menor
    n_layers = 4        # Menos camadas
    n_heads = 4         # Menos heads
    seq_len = 64        # Sequ√™ncia menor

    if model_type == 'psiqrh':
        try:
            from src.architecture.psiqrh_transformer import PsiQRHTransformer
            model = PsiQRHTransformer(
                vocab_size=vocab_size,
                d_model=d_model,
                n_layers=n_layers,
                n_heads=n_heads,
                max_seq_length=seq_len
            )
            print(f"‚úÖ Œ®QRH Transformer criado com sucesso")
        except ImportError as e:
            print(f"‚ùå Erro ao importar Œ®QRH Transformer: {e}")
            # Fallback para um modelo simplificado
            model = create_fallback_psiqrh_model(vocab_size, d_model, n_layers, n_heads)
            print(f"‚ö†Ô∏è  Usando modelo fallback para Œ®QRH")

    elif model_type == 'standard':
        model = create_standard_transformer(vocab_size, d_model, n_layers, n_heads)
        print(f"‚úÖ Standard Transformer criado com sucesso")

    else:
        raise ValueError(f"Tipo de modelo desconhecido: {model_type}")

    # Mover para dispositivo
    model = model.to(device)
    model.eval()  # Modo de avalia√ß√£o para consist√™ncia

    return model


def create_standard_transformer(vocab_size: int, d_model: int, n_layers: int, n_heads: int) -> nn.Module:
    """Cria um Transformer padr√£o para compara√ß√£o."""

    class StandardTransformer(nn.Module):
        def __init__(self, vocab_size, d_model, n_layers, n_heads):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, d_model)

            # Camadas de aten√ß√£o multi-head
            self.attention_layers = nn.ModuleList([
                nn.MultiheadAttention(d_model, n_heads, batch_first=True)
                for _ in range(n_layers)
            ])

            # Feed-forward layers
            self.ff_layers = nn.ModuleList([
                nn.Sequential(
                    nn.Linear(d_model, d_model * 4),
                    nn.ReLU(),
                    nn.Linear(d_model * 4, d_model)
                ) for _ in range(n_layers)
            ])

            self.layer_norms_1 = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_layers)])
            self.layer_norms_2 = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_layers)])

            self.output_layer = nn.Linear(d_model, vocab_size)

        def forward(self, x):
            x = self.embedding(x)

            for i in range(len(self.attention_layers)):
                # Self-attention
                attn_out, _ = self.attention_layers[i](x, x, x)
                x = self.layer_norms_1[i](x + attn_out)

                # Feed-forward
                ff_out = self.ff_layers[i](x)
                x = self.layer_norms_2[i](x + ff_out)

            return self.output_layer(x)

    return StandardTransformer(vocab_size, d_model, n_layers, n_heads)


def create_fallback_psiqrh_model(vocab_size: int, d_model: int, n_layers: int, n_heads: int) -> nn.Module:
    """
    Cria um modelo Œ®QRH simplificado como fallback.
    Esta implementa√ß√£o simula a estrutura b√°sica do Œ®QRH.
    """
    class FallbackPsiQRH(nn.Module):
        def __init__(self, vocab_size, d_model, n_layers, n_heads):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, d_model)

            # Camadas de transforma√ß√£o quaterni√¥nica simulada
            self.quaternion_layers = nn.ModuleList([
                nn.Sequential(
                    nn.Linear(d_model, d_model * 4),  # Simula opera√ß√µes quaterni√¥nicas
                    nn.ReLU(),
                    nn.Linear(d_model * 4, d_model)
                ) for _ in range(n_layers)
            ])

            # Camadas de consci√™ncia fractal simulada
            self.consciousness_layers = nn.ModuleList([
                nn.Sequential(
                    nn.Linear(d_model, d_model * 2),
                    nn.Tanh(),
                    nn.Linear(d_model * 2, d_model)
                ) for _ in range(n_layers // 2)
            ])

            self.output_layer = nn.Linear(d_model, vocab_size)

        def forward(self, x):
            x = self.embedding(x)

            # Aplicar camadas quaterni√¥nicas
            for layer in self.quaternion_layers:
                x = layer(x) + x  # Residual connection

            # Aplicar camadas de consci√™ncia
            for layer in self.consciousness_layers:
                x = layer(x) + x  # Residual connection

            return self.output_layer(x)

    return FallbackPsiQRH(vocab_size, d_model, n_layers, n_heads)


def analyze_model(model: nn.Module, device: str, batch_size: int = 8, seq_len: int = 128) -> Dict[str, float]:
    """
    Analisa par√¢metros e uso de mem√≥ria do modelo.

    Args:
        model: Modelo a ser analisado
        device: Dispositivo de teste
        batch_size: Tamanho do batch para teste
        seq_len: Comprimento da sequ√™ncia para teste

    Returns:
        Dicion√°rio com m√©tricas de an√°lise
    """
    # Usar o vocab_size correto do modelo
    vocab_size = getattr(model, 'vocab_size', 5000)
    results = {}

    print("\nüìä AN√ÅLISE DO MODELO:")

    # 1. Contagem de par√¢metros
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"- Par√¢metros Totais: {total_params:,}")
    print(f"- Par√¢metros Trein√°veis: {trainable_params:,}")

    results['total_params'] = total_params
    results['trainable_params'] = trainable_params

    # 2. Medi√ß√£o de mem√≥ria
    if device == 'cuda' and torch.cuda.is_available():
        # Medi√ß√£o GPU
        torch.cuda.reset_peak_memory_stats(device)
        torch.cuda.empty_cache()

        # Criar tensor de entrada com √≠ndices v√°lidos (range mais conservador)
        # Usar o vocab_size correto do modelo
        model_vocab_size = getattr(model, 'vocab_size', vocab_size)
        input_tensor = torch.randint(0, model_vocab_size - 1, (batch_size, seq_len), device=device)

        # Executar forward pass
        with torch.no_grad():
            model(input_tensor)

        peak_memory_bytes = torch.cuda.max_memory_allocated(device)
        peak_memory_mb = peak_memory_bytes / (1024 ** 2)

        print(f"- Pico de Mem√≥ria (GPU): {peak_memory_mb:.2f} MB")
        results['peak_memory_mb'] = peak_memory_mb

    else:
        # Medi√ß√£o CPU - usar psutil como alternativa ao memory_profiler
        try:
            import psutil
            import os

            process = psutil.Process(os.getpid())
            memory_before = process.memory_info().rss / (1024 ** 2)  # MB

            # Criar tensor de entrada com √≠ndices v√°lidos (range mais conservador)
            # Usar o vocab_size correto do modelo
            model_vocab_size = getattr(model, 'vocab_size', vocab_size)
            input_tensor = torch.randint(0, model_vocab_size - 1, (batch_size, seq_len), device=device)

            # Executar forward pass
            with torch.no_grad():
                model(input_tensor)

            memory_after = process.memory_info().rss / (1024 ** 2)  # MB
            peak_memory_mb = memory_after - memory_before

            print(f"- Pico de Mem√≥ria (CPU): {peak_memory_mb:.2f} MB")
            results['peak_memory_mb'] = peak_memory_mb

        except ImportError:
            print("‚ö†Ô∏è  psutil n√£o dispon√≠vel - pulando medi√ß√£o de mem√≥ria CPU")
            results['peak_memory_mb'] = 0.0

    # 3. An√°lise de camadas (opcional)
    layer_analysis = analyze_model_layers(model)
    results.update(layer_analysis)

    return results


def analyze_model_layers(model: nn.Module) -> Dict[str, Any]:
    """
    Analisa a distribui√ß√£o de par√¢metros por tipo de camada.
    """
    layer_stats = {}

    for name, module in model.named_modules():
        if len(list(module.children())) == 0:  # Leaf module
            params = sum(p.numel() for p in module.parameters())
            if params > 0:
                layer_type = type(module).__name__
                layer_stats[f"layer_{layer_type}"] = layer_stats.get(f"layer_{layer_type}", 0) + params

    return layer_stats


def main():
    """
    Executa a matriz de testes (2 modelos x 2 dispositivos).
    """
    print("üß† MEMORY BENCHMARK TEST - Œ®QRH vs Standard Transformer")
    print("=" * 60)

    # Configura√ß√µes de teste otimizadas para CPU
    batch_size = 4  # Batch menor para evitar problemas de mem√≥ria
    seq_len = 64    # Sequ√™ncia menor

    # Matriz de teste
    model_types = ['standard', 'psiqrh']
    devices = ['cpu']

    print(f"‚ö†Ô∏è  CUDA n√£o dispon√≠vel - testando apenas CPU com configura√ß√£o otimizada")
    print(f"   Batch size: {batch_size}, Seq len: {seq_len}")

    results = {}

    for device in devices:
        print(f"\n{'='*60}")
        print(f"üß™ TESTANDO NO DISPOSITIVO: {device.upper()}")
        print(f"{'='*60}")

        for model_type in model_types:
            print(f"\nüîç MODELO: {model_type.upper()}")
            print("-" * 40)

            try:
                # Setup do modelo
                model = setup_model(model_type, device)

                # An√°lise
                model_results = analyze_model(model, device, batch_size, seq_len)
                results[f"{model_type}_{device}"] = model_results

                # Cleanup
                del model
                if device == 'cuda':
                    torch.cuda.empty_cache()
                gc.collect()

            except Exception as e:
                print(f"‚ùå Erro ao testar {model_type} em {device}: {e}")
                import traceback
                traceback.print_exc()
                results[f"{model_type}_{device}"] = {'error': str(e)}

    # Relat√≥rio comparativo
    print("\n" + "="*60)
    print("üìà RELAT√ìRIO COMPARATIVO")
    print("="*60)

    generate_comparative_report(results)

    return results


def generate_comparative_report(results: Dict[str, Dict[str, float]]):
    """
    Gera relat√≥rio comparativo entre os modelos.
    """
    print("\nüìä COMPARA√á√ÉO DE EFICI√äNCIA:")

    # Encontrar resultados v√°lidos
    valid_results = {}
    for key, result in results.items():
        if 'error' not in result and 'total_params' in result:
            valid_results[key] = result

    if len(valid_results) < 2:
        print("‚ö†Ô∏è  Dados insuficientes para compara√ß√£o")
        return

    # Calcular m√©tricas comparativas
    standard_cpu = valid_results.get('standard_cpu')
    psiqrh_cpu = valid_results.get('psiqrh_cpu')
    standard_gpu = valid_results.get('standard_gpu')
    psiqrh_gpu = valid_results.get('psiqrh_gpu')

    if standard_cpu and psiqrh_cpu:
        param_ratio_cpu = psiqrh_cpu['total_params'] / standard_cpu['total_params']
        memory_ratio_cpu = psiqrh_cpu['peak_memory_mb'] / standard_cpu['peak_memory_mb'] if standard_cpu['peak_memory_mb'] > 0 else 0

        print(f"\nüíª CPU:")
        print(f"   Par√¢metros: Œ®QRH = {standard_cpu['total_params']:,} vs {psiqrh_cpu['total_params']:,}")
        print(f"   Ratio Par√¢metros: {param_ratio_cpu:.2f}x")
        if memory_ratio_cpu > 0:
            print(f"   Ratio Mem√≥ria: {memory_ratio_cpu:.2f}x")

    if standard_gpu and psiqrh_gpu:
        param_ratio_gpu = psiqrh_gpu['total_params'] / standard_gpu['total_params']
        memory_ratio_gpu = psiqrh_gpu['peak_memory_mb'] / standard_gpu['peak_memory_mb'] if standard_gpu['peak_memory_mb'] > 0 else 0

        print(f"\nüéÆ GPU:")
        print(f"   Par√¢metros: Œ®QRH = {standard_gpu['total_params']:,} vs {psiqrh_gpu['total_params']:,}")
        print(f"   Ratio Par√¢metros: {param_ratio_gpu:.2f}x")
        if memory_ratio_gpu > 0:
            print(f"   Ratio Mem√≥ria: {memory_ratio_gpu:.2f}x")

    # An√°lise de impacto
    print(f"\nüéØ DIAGN√ìSTICO:")
    if param_ratio_cpu > 3:
        print(f"   ‚ùå Œ®QRH tem {param_ratio_cpu:.1f}x mais par√¢metros - INEFICI√äNCIA CR√çTICA")
    elif param_ratio_cpu > 2:
        print(f"   ‚ö†Ô∏è  Œ®QRH tem {param_ratio_cpu:.1f}x mais par√¢metros - INEFICI√äNCIA MODERADA")
    else:
        print(f"   ‚úÖ Œ®QRH tem {param_ratio_cpu:.1f}x mais par√¢metros - EFICI√äNCIA ACEIT√ÅVEL")


if __name__ == "__main__":
    try:
        results = main()
        print("\n‚úÖ Benchmark conclu√≠do com sucesso!")
    except Exception as e:
        print(f"\n‚ùå Erro durante o benchmark: {e}")
        import traceback
        traceback.print_exc()