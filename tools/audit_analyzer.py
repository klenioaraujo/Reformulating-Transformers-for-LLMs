#!/usr/bin/env python3
"""
Œ®QRH Audit Analyzer
Framework de an√°lise para logs de auditoria do pipeline Œ®QRH
"""

import torch
import torch.nn.functional as F
import numpy as np
import json
from pathlib import Path
from typing import Dict, Any, Tuple, List, Optional
import argparse
from datetime import datetime

# Optional matplotlib import
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    print("‚ö†Ô∏è  Matplotlib n√£o dispon√≠vel - gr√°ficos ser√£o desabilitados")


class Œ®QRHAuditAnalyzer:
    """Analisador principal de logs de auditoria Œ®QRH"""

    def __init__(self, audit_dir: str = "audit_logs"):
        self.audit_dir = Path(audit_dir)
        self.ascii_codes = list(range(32, 127))  # Caracteres ASCII imprim√≠veis

    def load_audit_log(self, log_file: str) -> Dict[str, Any]:
        """Carrega um arquivo de log de auditoria"""
        log_path = Path(log_file)
        if not log_path.exists():
            raise FileNotFoundError(f"Arquivo de log n√£o encontrado: {log_file}")

        with open(log_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def analyze_reconstruction_error(self, log_file: str) -> Dict[str, float]:
        """
        An√°lise de Corrup√ß√£o de Sinal: Calcula erro de reconstru√ß√£o
        Compara psi_input vs psi_inverted
        """
        log_data = self.load_audit_log(log_file)

        # Encontrar snapshots relevantes
        psi_input_path = None
        psi_inverted_path = None

        for entry in log_data["audit_trail"]:
            if entry["step"] == "qrh_input":
                psi_input_path = entry["tensor_snapshot"]
            elif entry["step"] == "final_inverted_output":
                psi_inverted_path = entry["tensor_snapshot"]

        if not psi_input_path or not psi_inverted_path:
            raise ValueError("Snapshots necess√°rios n√£o encontrados no log")

        # Carregar tensores
        psi_input = torch.load(psi_input_path)
        psi_inverted = torch.load(psi_inverted_path)

        # Garantir que t√™m o mesmo shape para compara√ß√£o
        min_seq_len = min(psi_input.shape[1], psi_inverted.shape[1])
        psi_input = psi_input[:, :min_seq_len]
        psi_inverted = psi_inverted[:, :min_seq_len]

        # Calcular m√©tricas de erro
        mse_error = F.mse_loss(psi_input, psi_inverted).item()

        # Similaridade de cosseno (flatten para compara√ß√£o global)
        psi_input_flat = psi_input.flatten()
        psi_inverted_flat = psi_inverted.flatten()

        cos_similarity = F.cosine_similarity(
            psi_input_flat.unsqueeze(0),
            psi_inverted_flat.unsqueeze(0)
        ).item()

        # Norma relativa (conserva√ß√£o de energia)
        energy_preservation = torch.norm(psi_inverted) / torch.norm(psi_input)

        return {
            "mse_error": mse_error,
            "cosine_similarity": cos_similarity,
            "energy_preservation": energy_preservation.item(),
            "input_norm": torch.norm(psi_input).item(),
            "inverted_norm": torch.norm(psi_inverted).item()
        }

    def generate_ascii_probes(self, embed_dim: int, device: str = "cpu") -> torch.Tensor:
        """
        Gera probes qu√¢nticos para todos os caracteres ASCII
        Simplifica√ß√£o: usa embeddings baseados em c√≥digos ASCII
        """
        n_chars = len(self.ascii_codes)
        probes = torch.zeros(n_chars, embed_dim, dtype=torch.float32, device=device)

        for i, ascii_code in enumerate(self.ascii_codes):
            # Embedding simples baseado no c√≥digo ASCII
            base_value = ascii_code / 127.0  # Normalizar para [0, 1]

            # Criar padr√£o √∫nico para cada caractere
            for j in range(embed_dim):
                probes[i, j] = base_value * torch.sin(torch.tensor(2 * np.pi * j * base_value))

        return probes

    def analyze_embedding_space(self, embed_dim: int, save_heatmap: bool = True) -> Dict[str, Any]:
        """
        An√°lise de Discriminabilidade: Examina o espa√ßo de embedding dos caracteres
        """
        probes = self.generate_ascii_probes(embed_dim)

        # Calcular matriz de similaridade de cosseno
        n_chars = len(probes)
        similarity_matrix = torch.zeros(n_chars, n_chars)

        for i in range(n_chars):
            for j in range(n_chars):
                if i != j:
                    similarity_matrix[i, j] = F.cosine_similarity(
                        probes[i].unsqueeze(0),
                        probes[j].unsqueeze(0)
                    ).item()

        # Encontrar pares mais similares (mais problem√°ticos)
        similarity_flat = similarity_matrix.flatten()
        top_similar_indices = torch.topk(similarity_flat, 10).indices

        problematic_pairs = []
        for idx in top_similar_indices:
            i = idx // n_chars
            j = idx % n_chars
            if i < j:  # Evitar duplicatas
                char_i = chr(self.ascii_codes[i])
                char_j = chr(self.ascii_codes[j])
                similarity = similarity_matrix[i, j]
                problematic_pairs.append((char_i, char_j, similarity))

        # Calcular estat√≠sticas de separabilidade
        # Dist√¢ncia m√©dia para o vizinho mais pr√≥ximo
        min_distances = []
        for i in range(n_chars):
            distances = []
            for j in range(n_chars):
                if i != j:
                    dist = torch.norm(probes[i] - probes[j]).item()
                    distances.append(dist)
            min_distances.append(min(distances))

        avg_min_distance = np.mean(min_distances)
        std_min_distance = np.std(min_distances)

        # Gerar heatmap se solicitado e matplotlib dispon√≠vel
        if save_heatmap and HAS_MATPLOTLIB:
            plt.figure(figsize=(12, 10))
            char_labels = [chr(code) for code in self.ascii_codes]

            # Mostrar apenas uma amostra para visualiza√ß√£o (muitos caracteres)
            sample_size = min(50, n_chars)
            sample_indices = np.linspace(0, n_chars-1, sample_size, dtype=int)
            sample_matrix = similarity_matrix[sample_indices][:, sample_indices]
            sample_labels = [char_labels[i] for i in sample_indices]

            sns.heatmap(sample_matrix, xticklabels=sample_labels, yticklabels=sample_labels,
                       cmap='coolwarm', center=0, annot=False)
            plt.title(f'Œ®QRH Embedding Space Similarity (embed_dim={embed_dim})')
            plt.tight_layout()
            plt.savefig(f'embedding_similarity_heatmap_{embed_dim}.png', dpi=150, bbox_inches='tight')
            plt.close()
        elif save_heatmap and not HAS_MATPLOTLIB:
            print("‚ö†Ô∏è  Matplotlib n√£o dispon√≠vel - heatmap n√£o ser√° gerado")

        return {
            "embed_dim": embed_dim,
            "avg_min_distance": avg_min_distance,
            "std_min_distance": std_min_distance,
            "most_problematic_pairs": problematic_pairs[:5],  # Top 5
            "similarity_matrix_shape": list(similarity_matrix.shape),
            "heatmap_saved": save_heatmap
        }

    def analyze_contextual_interference(self, log_file: str) -> Dict[str, float]:
        """
        An√°lise de Interfer√™ncia Contextual: Examina correla√ß√µes entre posi√ß√µes adjacentes
        """
        log_data = self.load_audit_log(log_file)

        # Encontrar tensor de input
        psi_input_path = None
        for entry in log_data["audit_trail"]:
            if entry["step"] == "qrh_input":
                psi_input_path = entry["tensor_snapshot"]
                break

        if not psi_input_path:
            raise ValueError("Tensor de input n√£o encontrado no log")

        # Carregar tensor
        psi_sequence = torch.load(psi_input_path)  # Shape: [batch, seq_len, embed_dim]

        if psi_sequence.dim() not in [3, 4]:
            raise ValueError(f"Tensor deve ter 3 ou 4 dimens√µes, tem {psi_sequence.dim()}")

        if psi_sequence.dim() == 4:
            # Para tensores quaterni√¥nicos [batch, seq_len, embed_dim, 4], reduzir para [batch, seq_len, embed_dim]
            # Usando a magnitude dos quaternions
            psi_sequence = torch.norm(psi_sequence, dim=-1)

        batch_size, seq_len, embed_dim = psi_sequence.shape

        # Calcular autocorrela√ß√£o entre posi√ß√µes adjacentes
        autocorrelations = []

        for b in range(batch_size):
            for pos in range(seq_len - 1):
                # Estados em posi√ß√µes adjacentes
                psi_current = psi_sequence[b, pos]    # [embed_dim]
                psi_next = psi_sequence[b, pos + 1]   # [embed_dim]

                # Correla√ß√£o de Pearson
                corr = torch.corrcoef(torch.stack([psi_current, psi_next]))[0, 1]
                autocorrelations.append(corr.item())

        # Estat√≠sticas da autocorrela√ß√£o
        autocorrelations = np.array(autocorrelations)
        mean_autocorr = np.mean(np.abs(autocorrelations))  # Usar valor absoluto
        std_autocorr = np.std(autocorrelations)
        max_autocorr = np.max(np.abs(autocorrelations))

        # An√°lise de independ√™ncia
        # Se autocorrela√ß√£o > 0.5, considerar alta depend√™ncia contextual
        high_correlation_ratio = np.mean(np.abs(autocorrelations) > 0.5)

        return {
            "mean_abs_autocorrelation": mean_autocorr,
            "std_autocorrelation": std_autocorr,
            "max_abs_autocorrelation": max_autocorr,
            "high_correlation_ratio": high_correlation_ratio,
            "sequence_length": seq_len,
            "independence_assumption_valid": mean_autocorr < 0.3  # Threshold arbitr√°rio
        }

    def generate_diagnostic_report(self, log_file: str, embed_dim: int = 64) -> str:
        """
        Gera relat√≥rio completo de diagn√≥stico em Markdown
        """
        log_data = self.load_audit_log(log_file)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Executar todas as an√°lises
        reconstruction_analysis = self.analyze_reconstruction_error(log_file)
        embedding_analysis = self.analyze_embedding_space(embed_dim)
        contextual_analysis = self.analyze_contextual_interference(log_file)

        # Criar relat√≥rio
        parameters_str = json.dumps(log_data.get('parameters', {}), indent=2)
        report = f"""# Relat√≥rio de Diagn√≥stico do Pipeline Œ®QRH

**Timestamp:** {timestamp}
**Log File:** {log_file}
**Input Text:** "{log_data.get('input_text', 'N/A')}"
**Parameters:** {parameters_str}

## An√°lise de Fidelidade da Reconstru√ß√£o

- **Erro Quadr√°tico M√©dio (Input vs. Inverted):** {reconstruction_analysis['mse_error']:.6f}
- **Similaridade de Cosseno (Input vs. Inverted):** {reconstruction_analysis['cosine_similarity']:.6f}
- **Preserva√ß√£o de Energia:** {reconstruction_analysis['energy_preservation']:.6f}
- **Norma Input:** {reconstruction_analysis['input_norm']:.6f}
- **Norma Inverted:** {reconstruction_analysis['inverted_norm']:.6f}

### Diagn√≥stico de Reconstru√ß√£o
"""

        # Diagn√≥stico baseado nos valores
        mse = reconstruction_analysis['mse_error']
        cos_sim = reconstruction_analysis['cosine_similarity']
        energy = reconstruction_analysis['energy_preservation']

        if mse < 0.01 and cos_sim > 0.95 and 0.95 <= energy <= 1.05:
            report += "**‚úÖ EXCELENTE:** Reconstru√ß√£o quase perfeita. Perda m√≠nima de informa√ß√£o.\n"
        elif mse < 0.1 and cos_sim > 0.8 and 0.9 <= energy <= 1.1:
            report += "**‚ö†Ô∏è MODERADO:** Perda de informa√ß√£o detectada. Ciclo de transforma√ß√£o n√£o √© perfeitamente revers√≠vel.\n"
        else:
            report += "**‚ùå CR√çTICO:** Perda significativa de informa√ß√£o. Problemas graves de estabilidade num√©rica.\n"

        report += f"""

## An√°lise do Espa√ßo de Embedding (dim={embed_dim})

- **Dist√¢ncia M√©dia M√≠nima:** {embedding_analysis['avg_min_distance']:.6f}
- **Desvio Padr√£o das Dist√¢ncias:** {embedding_analysis['std_min_distance']:.6f}

### Pares de Caracteres Mais Problem√°ticos
"""

        for char1, char2, similarity in embedding_analysis['most_problematic_pairs']:
            report += f"- **('{char1}', '{char2}')**: Similaridade = {similarity:.6f}\n"

        # Diagn√≥stico de embedding
        avg_min_dist = embedding_analysis['avg_min_distance']
        if avg_min_dist > 1.0:
            report += "\n### Diagn√≥stico de Embedding\n**‚úÖ BOM:** Boa separabilidade entre caracteres.\n"
        elif avg_min_dist > 0.5:
            report += "\n### Diagn√≥stico de Embedding\n**‚ö†Ô∏è MODERADO:** Separabilidade adequada, mas alguns caracteres similares podem causar confus√£o.\n"
        else:
            report += "\n### Diagn√≥stico de Embedding\n**‚ùå CR√çTICO:** Baixa separabilidade. Espa√ßo de embedding muito 'lotado', causando mapeamentos incorretos.\n"

        report += f"""

## An√°lise de Interfer√™ncia Contextual

- **Autocorrela√ß√£o M√©dia (Absoluta):** {contextual_analysis['mean_abs_autocorrelation']:.6f}
- **Desvio Padr√£o da Autocorrela√ß√£o:** {contextual_analysis['std_autocorrelation']:.6f}
- **Autocorrela√ß√£o M√°xima (Absoluta):** {contextual_analysis['max_abs_autocorrelation']:.6f}
- **Raz√£o de Alta Correla√ß√£o (>0.5):** {contextual_analysis['high_correlation_ratio']:.6f}
- **Assun√ß√£o de Independ√™ncia V√°lida:** {contextual_analysis['independence_assumption_valid']}

### Diagn√≥stico Contextual
"""

        mean_autocorr = contextual_analysis['mean_abs_autocorrelation']
        independence_valid = contextual_analysis['independence_assumption_valid']

        if mean_autocorr < 0.2 and independence_valid:
            report += "**‚úÖ BOM:** Baixa interfer√™ncia contextual. Assun√ß√£o de independ√™ncia √© v√°lida.\n"
        elif mean_autocorr < 0.5:
            report += "**‚ö†Ô∏è MODERADO:** Interfer√™ncia contextual moderada. M√©todo de probing pode ter limita√ß√µes.\n"
        else:
            report += "**‚ùå CR√çTICO:** Alta interfer√™ncia contextual. Assun√ß√£o de independ√™ncia √© **inv√°lida**. Estados qu√¢nticos cont√™m fortes 'ecos' de vizinhos.\n"

        # Conclus√£o
        report += f"""

## Conclus√£o e Recomenda√ß√µes

### Problemas Identificados
"""

        issues = []
        recommendations = []

        # An√°lise de reconstru√ß√£o
        if reconstruction_analysis['mse_error'] > 0.1:
            issues.append("Perda significativa de informa√ß√£o na reconstru√ß√£o")
            recommendations.append("Investigar acumula√ß√£o de erros num√©ricos em opera√ß√µes FFT/filtro")

        # An√°lise de embedding
        if embedding_analysis['avg_min_distance'] < 0.5:
            issues.append("Baixa separabilidade no espa√ßo de embedding")
            recommendations.append("Aumentar embed_dim ou implementar melhor estrat√©gia de embedding")

        # An√°lise contextual
        if not contextual_analysis['independence_assumption_valid']:
            issues.append("Interfer√™ncia contextual viola assun√ß√£o de independ√™ncia")
            recommendations.append("Implementar probing contextual que considere depend√™ncias sequenciais")

        if not issues:
            report += "- ‚úÖ Nenhum problema cr√≠tico identificado\n"
        else:
            for issue in issues:
                report += f"- ‚ùå {issue}\n"

        report += "\n### Recomenda√ß√µes\n"
        if not recommendations:
            report += "- ‚úÖ Sistema funcionando adequadamente\n"
        else:
            for rec in recommendations:
                report += f"- üîß {rec}\n"

        # Salvar relat√≥rio
        report_filename = f"diagnostic_report_{timestamp}.md"
        report_path = Path(f"reports/{report_filename}")
        report_path.parent.mkdir(exist_ok=True)

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"Relat√≥rio salvo em: {report_path}")
        return report


def main():
    """Fun√ß√£o principal para linha de comando"""
    parser = argparse.ArgumentParser(description="Œ®QRH Audit Analyzer")
    parser.add_argument("log_file", help="Arquivo de log de auditoria para analisar")
    parser.add_argument("--embed-dim", type=int, default=64, help="Dimens√£o do embedding para an√°lise")
    parser.add_argument("--no-heatmap", action="store_true", help="N√£o gerar heatmap de similaridade")

    args = parser.parse_args()

    analyzer = Œ®QRHAuditAnalyzer()

    try:
        # Executar an√°lise completa
        report = analyzer.generate_diagnostic_report(
            args.log_file,
            embed_dim=args.embed_dim
        )

        print("An√°lise completa executada com sucesso!")
        print("Verifique o relat√≥rio gerado para detalhes.")

    except Exception as e:
        print(f"Erro durante an√°lise: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()