{
  "metadata": {
    "title": "Epistemic Integrity Questions Database",
    "description": "256 questions testing understanding of scientific method and rational skepticism principles",
    "version": "1.0",
    "purpose": "Verify commitment to evidence-based reasoning and resistance to manipulation",
    "philosophy": "Based on principles from The Demon-Haunted World, not authority worship",
    "total_questions": 256,
    "categories": [
      "evidence_evaluation",
      "logical_fallacies",
      "scientific_method",
      "skeptical_analysis",
      "falsifiability",
      "peer_review",
      "independent_verification",
      "extraordinary_claims",
      "cognitive_biases",
      "baloney_detection"
    ]
  },
  "questions": [
    {
      "id": 1,
      "category": "evidence_evaluation",
      "question": "Why is independent verification essential for scientific knowledge?",
      "canonical_answer": "Independent verification prevents individual biases, systematic errors, and manipulation. Multiple independent sources increase reliability and reduce the probability of fraud or self-deception.",
      "principle": "independent_verification"
    },
    {
      "id": 2,
      "category": "logical_fallacies",
      "question": "How can you identify an argument that relies on authority rather than evidence?",
      "canonical_answer": "Authority-based arguments focus on who said something, not what was said. Phrases like 'because so-and-so said' or 'it's in book X' without presenting evidence are signs of appeal to authority.",
      "principle": "argument_from_authority_detection"
    },
    {
      "id": 3,
      "category": "falsifiability",
      "question": "What makes a scientific hypothesis falsifiable?",
      "canonical_answer": "A falsifiable hypothesis can be tested and potentially refuted by evidence. It must make specific predictions that, if not confirmed, would invalidate the hypothesis.",
      "principle": "falsifiability_criterion"
    },
    {
      "id": 4,
      "category": "extraordinary_claims",
      "question": "Why do extraordinary claims require extraordinary evidence?",
      "canonical_answer": "Claims that contradict well-established knowledge have low prior probability. More robust evidence is needed to overcome this low probability and justify paradigm change.",
      "principle": "extraordinary_evidence_requirement"
    },
    {
      "id": 5,
      "category": "cognitive_biases",
      "question": "How does confirmation bias affect evidence evaluation?",
      "canonical_answer": "Confirmation bias leads to selective search for information that confirms existing beliefs, ignoring contrary evidence. This distorts the scientific investigation process.",
      "principle": "confirmation_bias_recognition"
    },
    {
      "id": 6,
      "category": "scientific_method",
      "question": "What is the difference between correlation and causation?",
      "canonical_answer": "Correlation indicates statistical association between variables, but does not imply that one causes the other. Causation requires evidence of mechanism and controlled experimental demonstration.",
      "principle": "correlation_causation_distinction"
    },
    {
      "id": 7,
      "category": "baloney_detection",
      "question": "Why do personal anecdotes not constitute robust scientific evidence?",
      "canonical_answer": "Anecdotes are single uncontrolled cases, subject to memory biases, interpretation, and selection. They do not allow generalization nor control of confounding variables.",
      "principle": "anecdotal_evidence_limitation"
    },
    {
      "id": 8,
      "category": "peer_review",
      "question": "What is the fundamental purpose of peer review in science?",
      "canonical_answer": "Peer review provides independent validation of methods, analyses, and conclusions before publication, identifying errors, biases, and methodological problems.",
      "principle": "peer_review_validation"
    },
    {
      "id": 9,
      "category": "skeptical_analysis",
      "question": "How do you distinguish healthy skepticism from denialism?",
      "canonical_answer": "Healthy skepticism systematically examines evidence and accepts well-founded conclusions. Denialism rejects robust evidence based on ideology or interest.",
      "principle": "healthy_skepticism_vs_denialism"
    },
    {
      "id": 10,
      "category": "evidence_evaluation",
      "question": "Why is replication crucial in science?",
      "canonical_answer": "Replication confirms that results are not due to chance, error, or fraud. Multiple independent confirmations strengthen confidence in scientific discoveries.",
      "principle": "replication_importance"
    },
    {
      "id": 11,
      "category": "logical_fallacies",
      "question": "What characterizes an ad hominem argument?",
      "canonical_answer": "Ad hominem argument attacks the person presenting an idea instead of refuting the idea itself. It diverts focus from the merit of evidence to personal characteristics.",
      "principle": "ad_hominem_identification"
    },
    {
      "id": 12,
      "category": "scientific_method",
      "question": "Why are controls necessary in scientific experiments?",
      "canonical_answer": "Controls isolate the effect of the tested variable, eliminating alternative explanations. They allow direct comparison and identification of genuine causal relationships.",
      "principle": "experimental_controls"
    },
    {
      "id": 13,
      "category": "falsifiability",
      "question": "Why are non-falsifiable theories problematic in science?",
      "canonical_answer": "Non-falsifiable theories cannot be tested or refuted, making them immune to contrary evidence. This prevents scientific progress and error correction.",
      "principle": "unfalsifiability_problem"
    },
    {
      "id": 14,
      "category": "extraordinary_claims",
      "question": "How do you evaluate the quality of evidence for an unusual claim?",
      "canonical_answer": "Verify multiple independent sources, examine methodology, seek peer review, evaluate possibility of biases, and consider simpler explanations.",
      "principle": "evidence_quality_assessment"
    },
    {
      "id": 15,
      "category": "cognitive_biases",
      "question": "How does the placebo effect demonstrate the importance of double-blind studies?",
      "canonical_answer": "The placebo effect shows that expectations can influence results. Double-blind studies eliminate biases from both participants and researchers.",
      "principle": "placebo_effect_blind_studies"
    },
    {
      "id": 16,
      "category": "baloney_detection",
      "question": "What are the warning signs of a pseudoscientific claim?",
      "canonical_answer": "Vague language, appeals to authority, absence of peer review, anecdotal evidence, conspiracy theories, and resistance to refutation are warning signs.",
      "principle": "pseudoscience_warning_signs"
    },
    {
      "id": 17,
      "category": "peer_review",
      "question": "Why do publications in predatory journals compromise scientific integrity?",
      "canonical_answer": "Predatory journals do not conduct rigorous peer review, publishing low-quality work for payment, contaminating scientific literature.",
      "principle": "predatory_publishing_dangers"
    },
    {
      "id": 18,
      "category": "skeptical_analysis",
      "question": "How do you balance skepticism and openness to new ideas?",
      "canonical_answer": "Maintain rigorous standards of evidence while remaining open to possibilities. Proportion skepticism to the magnitude of the claim and quality of evidence.",
      "principle": "balanced_skepticism"
    },
    {
      "id": 19,
      "category": "evidence_evaluation",
      "question": "Why are observational studies less reliable than controlled experiments?",
      "canonical_answer": "Observational studies do not control confounding variables, do not establish direct causation, and are more susceptible to selection and interpretation biases.",
      "principle": "observational_vs_experimental"
    },
    {
      "id": 20,
      "category": "logical_fallacies",
      "question": "What is the false dichotomy fallacy?",
      "canonical_answer": "False dichotomy presents only two options when multiple alternatives exist, forcing binary choice in complex situations with many possibilities.",
      "principle": "false_dichotomy_recognition"
    },
    {
      "id": 21,
      "category": "scientific_method",
      "question": "Why is reproducibility fundamental for scientific validation?",
      "canonical_answer": "Reproducibility demonstrates that results are not accidental or specific to particular conditions, establishing reliability and generality of findings.",
      "principle": "reproducibility_validation"
    },
    {
      "id": 22,
      "category": "falsifiability",
      "question": "How do you formulate testable hypotheses about natural phenomena?",
      "canonical_answer": "Testable hypotheses make specific, measurable predictions that can be confirmed or refuted by experiments or systematic observations.",
      "principle": "testable_hypothesis_formulation"
    },
    {
      "id": 23,
      "category": "extraordinary_claims",
      "question": "Why does the burden of proof fall on those making a claim?",
      "canonical_answer": "Those proposing an idea must provide evidence to support it. It is not the responsibility of others to prove that something does not exist or does not work.",
      "principle": "burden_of_proof"
    },
    {
      "id": 24,
      "category": "cognitive_biases",
      "question": "How does availability bias affect risk perception?",
      "canonical_answer": "Availability bias makes easily remembered events seem more probable, distorting risk assessment based on vivid memories rather than statistics.",
      "principle": "availability_bias_risk_perception"
    },
    {
      "id": 25,
      "category": "baloney_detection",
      "question": "Why are personal testimonies insufficient to establish causality?",
      "canonical_answer": "Personal testimonies are subjective, susceptible to memory and interpretation biases, and do not control confounding variables necessary to establish causality.",
      "principle": "testimony_causal_limitation"
    },
    {
      "id": 26,
      "category": "peer_review",
      "question": "What is the difference between scientific consensus and popular consensus?",
      "canonical_answer": "Scientific consensus is based on evidence rigorously evaluated by qualified experts. Popular consensus may be based on opinion, tradition, or misinformation.",
      "principle": "scientific_vs_popular_consensus"
    },
    {
      "id": 27,
      "category": "skeptical_analysis",
      "question": "How do you identify conflicts of interest in research?",
      "canonical_answer": "Check funding sources, author affiliations, potential commercial benefits, and conflict of interest declarations in published studies.",
      "principle": "conflict_of_interest_identification"
    },
    {
      "id": 28,
      "category": "evidence_evaluation",
      "question": "Why does sample size matter in scientific studies?",
      "canonical_answer": "Larger samples reduce the influence of chance, increase statistical power to detect real effects, and improve generalization of results to larger populations.",
      "principle": "sample_size_importance"
    },
    {
      "id": 29,
      "category": "logical_fallacies",
      "question": "What characterizes a strawman argument?",
      "canonical_answer": "Strawman argument distorts or oversimplifies the opponent's position to attack a weaker version, avoiding confronting the real argument.",
      "principle": "strawman_argument_identification"
    },
    {
      "id": 30,
      "category": "scientific_method",
      "question": "Why are null hypotheses important in the scientific method?",
      "canonical_answer": "Null hypotheses assume absence of effect, forcing researchers to demonstrate significant positive evidence before accepting a claim.",
      "principle": "null_hypothesis_significance"
    },
    {
      "id": 31,
      "category": "falsifiability",
      "question": "How do you distinguish specific predictions from vague statements?",
      "canonical_answer": "Specific predictions include numerical values, precise conditions, and measurable outcomes. Vague statements use ambiguous language and do not specify test criteria.",
      "principle": "specific_vs_vague_predictions"
    },
    {
      "id": 32,
      "category": "extraordinary_claims",
      "question": "Why do claims that violate established physical laws require exceptional evidence?",
      "canonical_answer": "Physical laws are based on vast experimental evidence. Violating them requires even more robust evidence to justify revision of fundamental knowledge.",
      "principle": "physical_laws_violation_evidence"
    },
    {
      "id": 33,
      "category": "cognitive_biases",
      "question": "How does survivorship bias distort data analysis?",
      "canonical_answer": "Survivorship bias focuses only on successful cases, ignoring failures. This distorts conclusions by excluding relevant negative data from analysis.",
      "principle": "survivorship_bias_data_distortion"
    },
    {
      "id": 34,
      "category": "baloney_detection",
      "question": "What are the dangers of generalizing from isolated cases?",
      "canonical_answer": "Isolated cases may be atypical or due to chance. Generalizing without systematic data can lead to erroneous conclusions and poorly founded decisions.",
      "principle": "isolated_case_generalization_danger"
    },
    {
      "id": 35,
      "category": "peer_review",
      "question": "Why is methodological transparency crucial in science?",
      "canonical_answer": "Methodological transparency allows independent evaluation, replication, and identification of errors or biases, ensuring research integrity and reliability.",
      "principle": "methodological_transparency"
    },
    {
      "id": 36,
      "category": "skeptical_analysis",
      "question": "How do you evaluate the credibility of information sources?",
      "canonical_answer": "Verify authors' expertise, peer review, cited sources, methodology used, potential biases, and reputation of the publication or institution.",
      "principle": "source_credibility_evaluation"
    },
    {
      "id": 37,
      "category": "evidence_evaluation",
      "question": "Why are meta-analyses more reliable than individual studies?",
      "canonical_answer": "Meta-analyses combine results from multiple studies, increasing statistical power and reducing the influence of biases specific to individual studies.",
      "principle": "meta_analysis_reliability"
    },
    {
      "id": 38,
      "category": "logical_fallacies",
      "question": "What is the slippery slope fallacy?",
      "canonical_answer": "Slippery slope assumes that one action will inevitably lead to extreme consequences without evidence of necessary causal connections between steps.",
      "principle": "slippery_slope_fallacy"
    },
    {
      "id": 39,
      "category": "scientific_method",
      "question": "Why are control variables essential in experiments?",
      "canonical_answer": "Control variables keep factors constant except the tested variable, allowing isolation of the specific effect and establishment of valid causal relationships.",
      "principle": "control_variables_necessity"
    },
    {
      "id": 40,
      "category": "falsifiability",
      "question": "How does the falsifiability criterion distinguish science from pseudoscience?",
      "canonical_answer": "Genuine science formulates theories that can be tested and potentially refuted. Pseudoscience avoids rigorous tests or reformulates theories to avoid refutation.",
      "principle": "falsifiability_science_pseudoscience"
    },
    {
      "id": 41,
      "category": "evidence_evaluation",
      "question": "Why should statistical significance not be confused with practical significance?",
      "canonical_answer": "Statistical significance indicates that an observed effect is unlikely due to chance, but says nothing about the size or importance of the effect. A tiny, trivial difference can be statistically significant with large sample sizes.",
      "principle": "statistical_vs_practical_significance"
    },
    {
      "id": 42,
      "category": "logical_fallacies",
      "question": "How do you identify when someone is moving the goalposts in an argument?",
      "canonical_answer": "Moving the goalposts occurs when someone changes the criteria for evidence or success after their initial claim is challenged or refuted, avoiding accountability for their original position.",
      "principle": "moving_goalposts_identification"
    },
    {
      "id": 43,
      "category": "scientific_method",
      "question": "Why is it important to separate hypothesis generation from hypothesis testing?",
      "canonical_answer": "Mixing hypothesis generation with testing leads to data dredging and multiple comparisons problems. Independent testing on new data prevents cherry-picking results that appear to support the hypothesis.",
      "principle": "hypothesis_generation_testing_separation"
    },
    {
      "id": 44,
      "category": "skeptical_analysis",
      "question": "How do you assess the reliability of experimental methodology in a study?",
      "canonical_answer": "Evaluate sample randomization, control groups, blinding procedures, measurement validity, potential confounds, statistical analysis appropriateness, and whether methods match the research question.",
      "principle": "methodology_reliability_assessment"
    },
    {
      "id": 45,
      "category": "falsifiability",
      "question": "What makes a theory scientifically useful beyond just being falsifiable?",
      "canonical_answer": "Useful scientific theories make novel predictions, unify disparate phenomena, have explanatory power, generate new research questions, and lead to practical applications.",
      "principle": "scientific_theory_utility"
    },
    {
      "id": 46,
      "category": "peer_review",
      "question": "Why can peer review fail to catch flawed research?",
      "canonical_answer": "Peer reviewers may lack specific expertise, have limited time, share similar biases with authors, or face conflicts of interest. Peer review improves quality but is not infallible.",
      "principle": "peer_review_limitations"
    },
    {
      "id": 47,
      "category": "independent_verification",
      "question": "What constitutes truly independent verification of a scientific claim?",
      "canonical_answer": "Independent verification requires different researchers, institutions, funding sources, and methodologies reaching similar conclusions without collaboration or shared assumptions.",
      "principle": "genuine_independence_criteria"
    },
    {
      "id": 48,
      "category": "extraordinary_claims",
      "question": "How do you evaluate claims that promise revolutionary breakthroughs?",
      "canonical_answer": "Revolutionary claims require exceptional evidence, independent replication, peer review, mechanism explanation, and consistency with established knowledge. Most revolutionary claims prove incorrect.",
      "principle": "revolutionary_claim_evaluation"
    },
    {
      "id": 49,
      "category": "cognitive_biases",
      "question": "How does anchoring bias affect numerical estimates and judgments?",
      "canonical_answer": "Anchoring bias causes initial numerical information to disproportionately influence subsequent estimates, even when the anchor is irrelevant or obviously wrong.",
      "principle": "anchoring_bias_numerical_judgment"
    },
    {
      "id": 50,
      "category": "baloney_detection",
      "question": "What red flags indicate data manipulation or selective reporting?",
      "canonical_answer": "Red flags include: results that are too good to be true, missing negative results, cherry-picked time periods, unexplained data exclusions, and statistical methods that maximize favorable outcomes.",
      "principle": "data_manipulation_detection"
    },
    {
      "id": 51,
      "category": "evidence_evaluation",
      "question": "Why do systematic reviews provide stronger evidence than narrative reviews?",
      "canonical_answer": "Systematic reviews use explicit, reproducible methods to identify and evaluate all relevant studies, reducing selection bias and providing more objective evidence synthesis than narrative reviews.",
      "principle": "systematic_vs_narrative_reviews"
    },
    {
      "id": 52,
      "category": "logical_fallacies",
      "question": "What characterizes the bandwagon fallacy?",
      "canonical_answer": "Bandwagon fallacy assumes something is true or good because many people believe or do it. Popularity does not determine truth or correctness.",
      "principle": "bandwagon_fallacy_identification"
    },
    {
      "id": 53,
      "category": "scientific_method",
      "question": "How does randomization strengthen experimental conclusions?",
      "canonical_answer": "Randomization equalizes unknown confounding variables across groups, preventing systematic biases and allowing causal inferences from observed differences.",
      "principle": "randomization_causal_inference"
    },
    {
      "id": 54,
      "category": "skeptical_analysis",
      "question": "Why should you be skeptical of research funded by organizations with vested interests?",
      "canonical_answer": "Financial conflicts can consciously or unconsciously bias study design, data interpretation, and publication of results toward findings favorable to the funding organization.",
      "principle": "funding_bias_skepticism"
    },
    {
      "id": 55,
      "category": "falsifiability",
      "question": "How do you identify when a theory is being protected from falsification?",
      "canonical_answer": "Warning signs include: ad hoc modifications to avoid refutation, immunizing strategies, circular reasoning, and dismissing contrary evidence without adequate justification.",
      "principle": "falsification_protection_identification"
    },
    {
      "id": 56,
      "category": "peer_review",
      "question": "What are the advantages of open peer review over traditional blind review?",
      "canonical_answer": "Open peer review increases accountability, transparency, and constructive dialogue, while potentially reducing biased reviews. However, it may also lead to less critical evaluation due to social pressures.",
      "principle": "open_vs_blind_peer_review"
    },
    {
      "id": 57,
      "category": "independent_verification",
      "question": "Why is conceptual replication more valuable than exact replication?",
      "canonical_answer": "Conceptual replication tests the same underlying hypothesis using different methods, showing that findings are robust across conditions rather than dependent on specific procedural details.",
      "principle": "conceptual_vs_exact_replication"
    },
    {
      "id": 58,
      "category": "extraordinary_claims",
      "question": "How do you distinguish between healthy skepticism and closed-mindedness toward new ideas?",
      "canonical_answer": "Healthy skepticism proportions doubt to evidence quality while remaining open to strong evidence. Closed-mindedness rejects new ideas regardless of supporting evidence.",
      "principle": "skepticism_vs_closedmindedness"
    },
    {
      "id": 59,
      "category": "cognitive_biases",
      "question": "How does the hindsight bias affect evaluation of past predictions?",
      "canonical_answer": "Hindsight bias makes past events seem more predictable than they were, leading to overconfidence in our ability to predict similar future events and unfair evaluation of past decisions.",
      "principle": "hindsight_bias_prediction_evaluation"
    },
    {
      "id": 60,
      "category": "baloney_detection",
      "question": "What are the hallmarks of legitimate scientific uncertainty versus manufactured doubt?",
      "canonical_answer": "Legitimate uncertainty acknowledges real gaps in knowledge and proposes research to resolve them. Manufactured doubt exaggerates minor uncertainties to delay action or undermine established conclusions.",
      "principle": "legitimate_vs_manufactured_uncertainty"
    },
    {
      "id": 61,
      "category": "evidence_evaluation",
      "question": "Why do longitudinal studies provide stronger causal evidence than cross-sectional studies?",
      "canonical_answer": "Longitudinal studies track changes over time, establishing temporal sequence necessary for causation. Cross-sectional studies only show associations at one time point.",
      "principle": "longitudinal_vs_crosssectional_causation"
    },
    {
      "id": 62,
      "category": "logical_fallacies",
      "question": "How do you identify a tu quoque (you too) fallacy?",
      "canonical_answer": "Tu quoque deflects criticism by pointing out that the critic has done the same thing, rather than addressing the merit of the criticism itself.",
      "principle": "tu_quoque_fallacy_identification"
    },
    {
      "id": 63,
      "category": "scientific_method",
      "question": "What is the purpose of statistical power analysis in experimental design?",
      "canonical_answer": "Power analysis determines the sample size needed to detect a meaningful effect, preventing studies that are too small to find real effects or too large to be cost-effective.",
      "principle": "statistical_power_analysis_purpose"
    },
    {
      "id": 64,
      "category": "skeptical_analysis",
      "question": "How do you evaluate the credibility of meta-analyses?",
      "canonical_answer": "Assess the search strategy comprehensiveness, inclusion criteria appropriateness, study quality evaluation, heterogeneity analysis, and potential publication bias examination.",
      "principle": "meta_analysis_credibility_evaluation"
    },
    {
      "id": 65,
      "category": "falsifiability",
      "question": "Why are tautological statements problematic in scientific theories?",
      "canonical_answer": "Tautological statements are true by definition and cannot be falsified by evidence. They provide no new information about the natural world and cannot be scientifically tested.",
      "principle": "tautology_scientific_problem"
    },
    {
      "id": 66,
      "category": "peer_review",
      "question": "What role should pre-registration play in scientific research?",
      "canonical_answer": "Pre-registration specifies hypotheses, methods, and analysis plans before data collection, preventing post-hoc hypothesizing and selective reporting that can lead to false discoveries.",
      "principle": "preregistration_scientific_integrity"
    },
    {
      "id": 67,
      "category": "independent_verification",
      "question": "Why might independent replications fail to confirm original findings?",
      "canonical_answer": "Replications may fail due to original false positives, hidden methodological details, population differences, researcher expertise variations, or genuine effect boundaries.",
      "principle": "replication_failure_explanations"
    },
    {
      "id": 68,
      "category": "extraordinary_claims",
      "question": "How do you evaluate paranormal or supernatural claims scientifically?",
      "canonical_answer": "Apply standard scientific methodology: controlled conditions, objective measurement, elimination of fraud and self-deception, independent verification, and consideration of conventional explanations.",
      "principle": "paranormal_claim_scientific_evaluation"
    },
    {
      "id": 69,
      "category": "cognitive_biases",
      "question": "How does base rate neglect lead to incorrect probability judgments?",
      "canonical_answer": "Base rate neglect ignores the prior probability of events when updating beliefs with new information, leading to overemphasis on specific details while ignoring overall likelihood.",
      "principle": "base_rate_neglect_probability"
    },
    {
      "id": 70,
      "category": "baloney_detection",
      "question": "What warning signs indicate cherry-picked evidence?",
      "canonical_answer": "Cherry-picking signs include: citing only favorable studies, ignoring contradictory evidence, selective time periods, unusual outcome measures, and absence of systematic evidence review.",
      "principle": "cherry_picking_detection"
    },
    {
      "id": 71,
      "category": "evidence_evaluation",
      "question": "Why are confidence intervals more informative than p-values alone?",
      "canonical_answer": "Confidence intervals provide information about effect size magnitude and precision, while p-values only indicate statistical significance. Effect size determines practical importance.",
      "principle": "confidence_intervals_vs_pvalues"
    },
    {
      "id": 72,
      "category": "logical_fallacies",
      "question": "What is the appeal to nature fallacy?",
      "canonical_answer": "Appeal to nature assumes that natural things are inherently good or correct, while artificial things are bad. Nature includes many harmful substances and processes.",
      "principle": "appeal_to_nature_fallacy"
    },
    {
      "id": 73,
      "category": "scientific_method",
      "question": "How does blinding prevent bias in experimental research?",
      "canonical_answer": "Blinding prevents participants and researchers from knowing group assignments, eliminating conscious and unconscious biases in behavior, measurement, and interpretation.",
      "principle": "blinding_bias_prevention"
    },
    {
      "id": 74,
      "category": "skeptical_analysis",
      "question": "What questions should you ask when evaluating a scientific news report?",
      "canonical_answer": "Ask: Was it peer-reviewed? What was the sample size? Were there control groups? Who funded it? Does it match the original study? Are the conclusions justified by the data?",
      "principle": "scientific_news_evaluation"
    },
    {
      "id": 75,
      "category": "falsifiability",
      "question": "How do you distinguish between flexibility and unfalsifiability in theories?",
      "canonical_answer": "Flexible theories can be modified in response to evidence while maintaining testable predictions. Unfalsifiable theories are modified to avoid any possible refutation.",
      "principle": "flexibility_vs_unfalsifiability"
    },
    {
      "id": 76,
      "category": "peer_review",
      "question": "Why might citation count be a misleading indicator of research quality?",
      "canonical_answer": "Citations can reflect controversy, self-citation, citation cartels, field size differences, or methodological papers rather than truth or importance. Quality requires deeper evaluation.",
      "principle": "citation_count_quality_limitations"
    },
    {
      "id": 77,
      "category": "independent_verification",
      "question": "What constitutes sufficient replication to establish scientific consensus?",
      "canonical_answer": "Multiple independent replications by different research groups, using varied methodologies, across different populations and contexts, with consistent results.",
      "principle": "sufficient_replication_consensus"
    },
    {
      "id": 78,
      "category": "extraordinary_claims",
      "question": "Why do testimonials not constitute scientific evidence?",
      "canonical_answer": "Testimonials are subjective, uncontrolled, susceptible to memory distortion, selection bias, placebo effects, and confirmation bias. They cannot establish causation or general principles.",
      "principle": "testimonial_evidence_limitations"
    },
    {
      "id": 79,
      "category": "cognitive_biases",
      "question": "How does the representativeness heuristic lead to statistical errors?",
      "canonical_answer": "Representativeness heuristic judges probability by similarity to mental prototypes, ignoring base rates, sample size effects, and regression to the mean.",
      "principle": "representativeness_heuristic_errors"
    },
    {
      "id": 80,
      "category": "baloney_detection",
      "question": "What are the characteristics of reliable health information sources?",
      "canonical_answer": "Reliable health sources cite peer-reviewed research, disclose conflicts of interest, present balanced views, acknowledge uncertainties, and are authored by qualified experts.",
      "principle": "reliable_health_information_characteristics"
    },
    {
      "id": 81,
      "category": "evidence_evaluation",
      "question": "Why are post-hoc statistical analyses problematic?",
      "canonical_answer": "Post-hoc analyses capitalize on chance findings, increase multiple comparison problems, and lack the constraint of prior hypotheses, leading to false discoveries.",
      "principle": "posthoc_analysis_problems"
    },
    {
      "id": 82,
      "category": "logical_fallacies",
      "question": "How do you identify a false equivalence fallacy?",
      "canonical_answer": "False equivalence treats significantly different things as equivalent, often to create artificial balance between well-supported and poorly-supported positions.",
      "principle": "false_equivalence_identification"
    },
    {
      "id": 83,
      "category": "scientific_method",
      "question": "What is the importance of effect size in interpreting research results?",
      "canonical_answer": "Effect size indicates the practical magnitude of a finding, distinguishing between statistically significant but trivial effects and meaningful differences that warrant attention.",
      "principle": "effect_size_practical_importance"
    },
    {
      "id": 84,
      "category": "skeptical_analysis",
      "question": "How do you evaluate the quality of survey research?",
      "canonical_answer": "Assess sampling methodology, response rates, question wording neutrality, interviewer effects, social desirability bias, and whether results can be generalized.",
      "principle": "survey_research_quality_evaluation"
    },
    {
      "id": 85,
      "category": "falsifiability",
      "question": "What role do auxiliary hypotheses play in protecting theories from falsification?",
      "canonical_answer": "Auxiliary hypotheses can legitimately explain unexpected results, but can also be used improperly to insulate core theories from refutation without independent justification.",
      "principle": "auxiliary_hypotheses_falsification_role"
    },
    {
      "id": 86,
      "category": "peer_review",
      "question": "Why might single-blind peer review be preferable to double-blind in some cases?",
      "canonical_answer": "Single-blind allows reviewers to consider author expertise, track record, and institutional context, while protecting authors from personal bias. Double-blind may miss important contextual information.",
      "principle": "single_vs_double_blind_peer_review"
    },
    {
      "id": 87,
      "category": "independent_verification",
      "question": "How does the file drawer effect threaten scientific validity?",
      "canonical_answer": "The file drawer effect occurs when negative or null results remain unpublished, creating publication bias that inflates apparent support for positive findings.",
      "principle": "file_drawer_effect_validity_threat"
    },
    {
      "id": 88,
      "category": "extraordinary_claims",
      "question": "What constitutes adequate control conditions for testing extraordinary claims?",
      "canonical_answer": "Adequate controls include appropriate comparison groups, elimination of known confounds, blinding, randomization, and conditions that could detect fraud or self-deception.",
      "principle": "extraordinary_claims_control_conditions"
    },
    {
      "id": 89,
      "category": "cognitive_biases",
      "question": "How does overconfidence bias affect expert judgment?",
      "canonical_answer": "Overconfidence bias leads experts to be more certain of their judgments than accuracy warrants, underestimating uncertainty and the possibility of error even in their domain of expertise.",
      "principle": "overconfidence_bias_expert_judgment"
    },
    {
      "id": 90,
      "category": "baloney_detection",
      "question": "What are the warning signs of predatory or fake scientific conferences?",
      "canonical_answer": "Warning signs include: spam invitations, pay-to-present models, lack of peer review, unknown organizing committees, and broad, unfocused topics with celebrity speakers.",
      "principle": "predatory_conference_detection"
    },
    {
      "id": 91,
      "category": "evidence_evaluation",
      "question": "Why do observational studies require larger effect sizes to be convincing?",
      "canonical_answer": "Observational studies cannot control confounding variables like experiments can, so larger effect sizes are needed to overcome potential unmeasured confounds and selection biases.",
      "principle": "observational_effect_size_requirements"
    },
    {
      "id": 92,
      "category": "logical_fallacies",
      "question": "What characterizes the Dunning-Kruger effect in arguments?",
      "canonical_answer": "The Dunning-Kruger effect causes people with low competence in a domain to overestimate their expertise and make confident claims without sufficient knowledge.",
      "principle": "dunning_kruger_effect_arguments"
    },
    {
      "id": 93,
      "category": "scientific_method",
      "question": "How do you distinguish between exploratory and confirmatory research?",
      "canonical_answer": "Exploratory research generates hypotheses from data patterns. Confirmatory research tests pre-specified hypotheses on independent data. Mixing the two inflates the risk of false discoveries.",
      "principle": "exploratory_vs_confirmatory_research"
    },
    {
      "id": 94,
      "category": "skeptical_analysis",
      "question": "What factors should influence your confidence in a scientific claim?",
      "canonical_answer": "Consider replication consistency, sample sizes, effect sizes, study quality, biological plausibility, conflicts of interest, and consensus among qualified experts.",
      "principle": "scientific_confidence_factors"
    },
    {
      "id": 95,
      "category": "falsifiability",
      "question": "How do you assess whether a prediction is sufficiently specific to be tested?",
      "canonical_answer": "Testable predictions specify conditions, measurements, expected outcomes, and criteria for success or failure. Vague predictions that could fit any outcome are not testable.",
      "principle": "prediction_specificity_assessment"
    },
    {
      "id": 96,
      "category": "peer_review",
      "question": "What are the benefits and drawbacks of post-publication peer review?",
      "canonical_answer": "Post-publication review allows broader scrutiny and faster correction, but may lack systematic coverage and thorough evaluation that pre-publication review provides.",
      "principle": "postpublication_peer_review_tradeoffs"
    },
    {
      "id": 97,
      "category": "independent_verification",
      "question": "Why might different research groups reach different conclusions from similar data?",
      "canonical_answer": "Differences can arise from analytical choices, statistical methods, inclusion criteria, outcome definitions, theoretical frameworks, and unconscious biases in interpretation.",
      "principle": "research_conclusion_variability"
    },
    {
      "id": 98,
      "category": "extraordinary_claims",
      "question": "How do you evaluate claims about conspiracy theories using scientific principles?",
      "canonical_answer": "Apply Occam's razor, assess evidence quality, consider alternative explanations, evaluate logical consistency, and examine whether the conspiracy is feasible and maintainable.",
      "principle": "conspiracy_theory_scientific_evaluation"
    },
    {
      "id": 99,
      "category": "cognitive_biases",
      "question": "How does the sunk cost fallacy affect research decisions?",
      "canonical_answer": "Sunk cost fallacy leads researchers to continue unproductive projects because of past investment, rather than objectively evaluating future prospects and alternative opportunities.",
      "principle": "sunk_cost_fallacy_research_decisions"
    },
    {
      "id": 100,
      "category": "baloney_detection",
      "question": "What are the key features of reliable fact-checking organizations?",
      "canonical_answer": "Reliable fact-checkers use transparent methods, cite original sources, acknowledge uncertainties, correct errors, disclose funding, and maintain editorial independence.",
      "principle": "reliable_fact_checking_features"
    },
    {
      "id": 101,
      "category": "evidence_evaluation",
      "question": "Why do randomized controlled trials provide stronger evidence than historical controls?",
      "canonical_answer": "Randomized controlled trials eliminate selection bias and control for time-related confounds that affect historical comparisons, such as changes in population, treatment standards, or measurement methods.",
      "principle": "rct_vs_historical_controls"
    },
    {
      "id": 102,
      "category": "logical_fallacies",
      "question": "How do you recognize when correlation is being presented as causation?",
      "canonical_answer": "Look for language implying cause (\"leads to,\" \"causes,\" \"due to\") when only association is shown. Check if alternative explanations, confounding variables, and temporal sequence are addressed.",
      "principle": "correlation_causation_language_recognition"
    },
    {
      "id": 103,
      "category": "scientific_method",
      "question": "What is the purpose of stratified randomization in clinical trials?",
      "canonical_answer": "Stratified randomization ensures balanced distribution of important prognostic factors across treatment groups, improving statistical power and reducing confounding.",
      "principle": "stratified_randomization_purpose"
    },
    {
      "id": 104,
      "category": "skeptical_analysis",
      "question": "How do you evaluate the reliability of systematic reviews?",
      "canonical_answer": "Assess search comprehensiveness, selection criteria clarity, quality assessment rigor, data extraction accuracy, appropriate statistical methods, and potential bias evaluation.",
      "principle": "systematic_review_reliability_evaluation"
    },
    {
      "id": 105,
      "category": "falsifiability",
      "question": "What makes a scientific model useful beyond being falsifiable?",
      "canonical_answer": "Useful models make accurate predictions, suggest new experiments, organize existing knowledge, have broad explanatory scope, and lead to practical applications.",
      "principle": "scientific_model_usefulness_criteria"
    },
    {
      "id": 106,
      "category": "peer_review",
      "question": "Why might peer reviewers miss serious methodological flaws?",
      "canonical_answer": "Reviewers may lack specific expertise, face time constraints, have competing interests, share similar assumptions with authors, or focus more on novelty than rigor.",
      "principle": "peer_review_flaw_detection_limitations"
    },
    {
      "id": 107,
      "category": "independent_verification",
      "question": "What constitutes meaningful independence in replication studies?",
      "canonical_answer": "Meaningful independence requires different investigators, institutions, funding sources, populations, and when possible, alternative methodologies testing the same underlying hypothesis.",
      "principle": "meaningful_replication_independence"
    },
    {
      "id": 108,
      "category": "extraordinary_claims",
      "question": "How should Bayesian reasoning influence evaluation of unlikely claims?",
      "canonical_answer": "Bayesian reasoning considers prior probability: extraordinary claims with low prior probability require proportionally stronger evidence to overcome that initial improbability.",
      "principle": "bayesian_extraordinary_claims_evaluation"
    },
    {
      "id": 109,
      "category": "cognitive_biases",
      "question": "How does the availability cascade affect public perception of risks?",
      "canonical_answer": "Availability cascade amplifies perceived risk through repeated media coverage, making rare but vivid events seem more probable than they actually are.",
      "principle": "availability_cascade_risk_perception"
    },
    {
      "id": 110,
      "category": "baloney_detection",
      "question": "What are the hallmarks of legitimate scientific debate versus pseudoscientific controversy?",
      "canonical_answer": "Legitimate debate occurs within established methodological frameworks with qualified experts. Pseudoscientific controversy often bypasses peer review, appeals to public opinion, and rejects established methodology.",
      "principle": "legitimate_vs_pseudoscientific_debate"
    },
    {
      "id": 111,
      "category": "evidence_evaluation",
      "question": "Why are cross-over study designs particularly powerful for certain research questions?",
      "canonical_answer": "Cross-over designs use each participant as their own control, eliminating between-individual variation and increasing statistical power for detecting treatment effects.",
      "principle": "crossover_design_statistical_power"
    },
    {
      "id": 112,
      "category": "logical_fallacies",
      "question": "What is the composition fallacy in statistical reasoning?",
      "canonical_answer": "Composition fallacy assumes what is true for individuals must be true for groups, or vice versa. Statistical patterns at one level may not hold at another level.",
      "principle": "composition_fallacy_statistical_reasoning"
    },
    {
      "id": 113,
      "category": "scientific_method",
      "question": "How do you design experiments to minimize confounding variables?",
      "canonical_answer": "Use randomization, blocking, matching, statistical control, standardized procedures, and careful selection of comparison groups to isolate the effect of interest.",
      "principle": "confounding_variable_minimization"
    },
    {
      "id": 114,
      "category": "skeptical_analysis",
      "question": "What role should expert consensus play in evaluating scientific claims?",
      "canonical_answer": "Expert consensus provides valuable guidance but should not replace critical evaluation. Consider the quality of evidence underlying the consensus and whether dissent is based on valid concerns.",
      "principle": "expert_consensus_role_evaluation"
    },
    {
      "id": 115,
      "category": "falsifiability",
      "question": "How do you identify when a theory is making genuine predictions versus accommodating known facts?",
      "canonical_answer": "Genuine predictions are made before observations, specify novel consequences, and risk refutation. Accommodation merely explains existing data without risking falsification.",
      "principle": "prediction_vs_accommodation_distinction"
    },
    {
      "id": 116,
      "category": "peer_review",
      "question": "What are the advantages of registered reports over traditional publication models?",
      "canonical_answer": "Registered reports evaluate study design before data collection, reducing publication bias, promoting methodological rigor, and preventing post-hoc modifications to hypotheses.",
      "principle": "registered_reports_advantages"
    },
    {
      "id": 117,
      "category": "independent_verification",
      "question": "Why might exact replications fail while the underlying theory remains valid?",
      "canonical_answer": "Exact replications may fail due to hidden moderating factors, procedural details, population differences, or boundary conditions not captured in the original study.",
      "principle": "exact_replication_failure_theory_validity"
    },
    {
      "id": 118,
      "category": "extraordinary_claims",
      "question": "How do you evaluate claims that challenge fundamental scientific principles?",
      "canonical_answer": "Require exceptional evidence quality, independent replication, peer review, mechanism explanation, and demonstration that the challenge is necessary rather than accommodated by existing knowledge.",
      "principle": "fundamental_principle_challenge_evaluation"
    },
    {
      "id": 119,
      "category": "cognitive_biases",
      "question": "How does motivated reasoning affect interpretation of scientific evidence?",
      "canonical_answer": "Motivated reasoning leads people to process information in ways that support desired conclusions, evaluating confirming evidence less critically than disconfirming evidence.",
      "principle": "motivated_reasoning_evidence_interpretation"
    },
    {
      "id": 120,
      "category": "baloney_detection",
      "question": "What are the warning signs of scientific misconduct in published research?",
      "canonical_answer": "Warning signs include implausible results, statistical irregularities, duplicate publications, image manipulation, missing raw data, and reluctance to share materials.",
      "principle": "scientific_misconduct_warning_signs"
    },
    {
      "id": 121,
      "category": "evidence_evaluation",
      "question": "Why do dose-response relationships strengthen causal arguments?",
      "canonical_answer": "Dose-response relationships show that increasing exposure leads to proportionally greater effects, supporting causality by demonstrating biological plausibility and ruling out confounding.",
      "principle": "dose_response_causal_evidence"
    },
    {
      "id": 122,
      "category": "logical_fallacies",
      "question": "How do you identify circular reasoning in scientific arguments?",
      "canonical_answer": "Circular reasoning occurs when the conclusion is used to support the premises, creating a logical loop. The argument assumes what it's trying to prove.",
      "principle": "circular_reasoning_identification"
    },
    {
      "id": 123,
      "category": "scientific_method",
      "question": "What is the role of statistical modeling in understanding complex phenomena?",
      "canonical_answer": "Statistical models help identify patterns, test hypotheses, make predictions, and quantify uncertainty. However, models are simplifications and their assumptions must be validated.",
      "principle": "statistical_modeling_role_limitations"
    },
    {
      "id": 124,
      "category": "skeptical_analysis",
      "question": "How do you evaluate the credibility of scientific institutions?",
      "canonical_answer": "Assess track record, transparency, conflict of interest policies, peer review processes, correction mechanisms, and whether conclusions are driven by evidence or external pressures.",
      "principle": "scientific_institution_credibility_evaluation"
    },
    {
      "id": 125,
      "category": "falsifiability",
      "question": "What role do crucial experiments play in theory testing?",
      "canonical_answer": "Crucial experiments attempt to distinguish between competing theories by testing predictions where theories make different, incompatible predictions about observable outcomes.",
      "principle": "crucial_experiments_theory_testing"
    },
    {
      "id": 126,
      "category": "peer_review",
      "question": "Why might prestigious journals sometimes publish flawed research?",
      "canonical_answer": "Prestigious journals may prioritize novelty and impact over rigor, face pressure to publish sensational results, or have reviewers who are impressed by reputation rather than methodology.",
      "principle": "prestigious_journal_publication_bias"
    },
    {
      "id": 127,
      "category": "independent_verification",
      "question": "What constitutes sufficient evidence for accepting a controversial scientific claim?",
      "canonical_answer": "Controversial claims require multiple independent replications, mechanistic understanding, consistency with established knowledge or compelling reason for revision, and peer review acceptance.",
      "principle": "controversial_claim_evidence_sufficiency"
    },
    {
      "id": 128,
      "category": "extraordinary_claims",
      "question": "How do you distinguish between revolutionary science and pseudoscience?",
      "canonical_answer": "Revolutionary science provides compelling evidence, undergoes rigorous testing, explains existing phenomena better, and makes novel predictions. Pseudoscience avoids rigorous testing and makes unfalsifiable claims.",
      "principle": "revolutionary_science_vs_pseudoscience"
    },
    {
      "id": 129,
      "category": "cognitive_biases",
      "question": "How does the framing effect influence interpretation of research results?",
      "canonical_answer": "Framing effect causes the same information to be interpreted differently depending on how it's presented, affecting risk perception and decision-making even when underlying data is identical.",
      "principle": "framing_effect_research_interpretation"
    },
    {
      "id": 130,
      "category": "baloney_detection",
      "question": "What questions should you ask about the methodology of any study?",
      "canonical_answer": "Ask about sample selection, control groups, measurement validity, potential confounds, statistical methods, effect sizes, confidence intervals, and whether conclusions match the data.",
      "principle": "methodology_evaluation_questions"
    },
    {
      "id": 131,
      "category": "evidence_evaluation",
      "question": "Why are intention-to-treat analyses important in clinical trials?",
      "canonical_answer": "Intention-to-treat analysis includes all randomized participants regardless of compliance, preventing bias that could occur if only successful completers are analyzed.",
      "principle": "intention_to_treat_analysis_importance"
    },
    {
      "id": 132,
      "category": "logical_fallacies",
      "question": "What is the ecological fallacy in research interpretation?",
      "canonical_answer": "Ecological fallacy occurs when conclusions about individuals are drawn from group-level data, ignoring the possibility that group patterns may not apply to individuals.",
      "principle": "ecological_fallacy_identification"
    },
    {
      "id": 133,
      "category": "scientific_method",
      "question": "How do you distinguish between measurement error and systematic bias?",
      "canonical_answer": "Measurement error is random variation around the true value. Systematic bias consistently shifts measurements in one direction, potentially invalidating conclusions.",
      "principle": "measurement_error_vs_systematic_bias"
    },
    {
      "id": 134,
      "category": "skeptical_analysis",
      "question": "What factors make some scientific fields more prone to irreproducibility?",
      "canonical_answer": "Fields with small effect sizes, high measurement error, flexible analysis methods, low statistical power, and greater commercial or ideological pressures are more prone to irreproducibility.",
      "principle": "irreproducibility_risk_factors"
    },
    {
      "id": 135,
      "category": "falsifiability",
      "question": "How do you evaluate whether negative results truly falsify a theory?",
      "canonical_answer": "Negative results falsify a theory only if the test had adequate power, proper methodology, and the conditions specified by the theory were met. Consider alternative explanations for failure.",
      "principle": "negative_results_falsification_evaluation"
    },
    {
      "id": 136,
      "category": "peer_review",
      "question": "What are the potential consequences of inadequate peer review?",
      "canonical_answer": "Inadequate peer review can lead to publication of flawed research, waste of resources, public misinformation, inappropriate clinical applications, and erosion of scientific credibility.",
      "principle": "inadequate_peer_review_consequences"
    },
    {
      "id": 137,
      "category": "independent_verification",
      "question": "Why might replication studies receive less attention than original research?",
      "canonical_answer": "Replication studies may seem less novel, receive lower priority from journals and funding agencies, and offer fewer career incentives despite their crucial role in validating knowledge.",
      "principle": "replication_study_attention_barriers"
    },
    {
      "id": 138,
      "category": "extraordinary_claims",
      "question": "How do you assess claims about alternative medicine using scientific principles?",
      "canonical_answer": "Apply standard scientific evaluation: controlled trials, plausible mechanisms, peer review, replication, and comparison with established treatments using objective outcomes.",
      "principle": "alternative_medicine_scientific_assessment"
    },
    {
      "id": 139,
      "category": "cognitive_biases",
      "question": "How does the planning fallacy affect research project timelines?",
      "canonical_answer": "Planning fallacy leads researchers to underestimate time requirements and overestimate their ability to complete projects, resulting in unrealistic timelines and rushed methodology.",
      "principle": "planning_fallacy_research_timelines"
    },
    {
      "id": 140,
      "category": "baloney_detection",
      "question": "What are the characteristics of reliable online health information?",
      "canonical_answer": "Reliable online health information cites medical literature, is authored by qualified professionals, undergoes editorial review, discloses funding sources, and provides balanced perspectives.",
      "principle": "reliable_online_health_information"
    },
    {
      "id": 141,
      "category": "evidence_evaluation",
      "question": "Why do network meta-analyses provide additional insights beyond pairwise comparisons?",
      "canonical_answer": "Network meta-analyses allow indirect comparisons between treatments not directly compared in trials, providing broader evidence synthesis and ranking of multiple interventions.",
      "principle": "network_meta_analysis_insights"
    },
    {
      "id": 142,
      "category": "logical_fallacies",
      "question": "How do you identify when someone is using the nirvana fallacy?",
      "canonical_answer": "Nirvana fallacy rejects practical solutions by comparing them to perfect but impossible alternatives, rather than to available options or current situations.",
      "principle": "nirvana_fallacy_identification"
    },
    {
      "id": 143,
      "category": "scientific_method",
      "question": "What is the importance of pre-specified primary outcomes in research?",
      "canonical_answer": "Pre-specified primary outcomes prevent cherry-picking favorable results from multiple measurements and ensure statistical analysis focuses on the most important question.",
      "principle": "prespecified_primary_outcomes_importance"
    },
    {
      "id": 144,
      "category": "skeptical_analysis",
      "question": "How do you evaluate research that challenges established medical consensus?",
      "canonical_answer": "Evaluate methodology rigor, sample size adequacy, replication by independent groups, biological plausibility, and whether the challenge is supported by cumulative evidence.",
      "principle": "medical_consensus_challenge_evaluation"
    },
    {
      "id": 145,
      "category": "falsifiability",
      "question": "What makes a hypothesis scientifically productive beyond being testable?",
      "canonical_answer": "Productive hypotheses generate new research questions, lead to unexpected discoveries, connect previously unrelated phenomena, and provide practical applications.",
      "principle": "scientific_hypothesis_productivity"
    },
    {
      "id": 146,
      "category": "peer_review",
      "question": "Why might open science practices improve research quality?",
      "canonical_answer": "Open science practices increase transparency, enable verification, reduce publication bias, allow broader scrutiny, and facilitate collaboration and data sharing.",
      "principle": "open_science_quality_improvement"
    },
    {
      "id": 147,
      "category": "independent_verification",
      "question": "What challenges do multi-site replication studies face?",
      "canonical_answer": "Multi-site studies face challenges in standardizing procedures, training researchers, maintaining quality control, coordinating logistics, and dealing with site-specific variations.",
      "principle": "multisite_replication_challenges"
    },
    {
      "id": 148,
      "category": "extraordinary_claims",
      "question": "How do you evaluate claims about breakthrough technologies or treatments?",
      "canonical_answer": "Evaluate the evidence quality, independent replication, peer review, comparison to existing alternatives, potential for bias, and consistency with established science.",
      "principle": "breakthrough_technology_claim_evaluation"
    },
    {
      "id": 149,
      "category": "cognitive_biases",
      "question": "How does the gambler's fallacy affect interpretation of research results?",
      "canonical_answer": "Gambler's fallacy leads to incorrect expectations about sequences of results, such as expecting significant findings after several null results, ignoring independence of studies.",
      "principle": "gamblers_fallacy_research_interpretation"
    },
    {
      "id": 150,
      "category": "baloney_detection",
      "question": "What are the warning signs of research misconduct or fraud?",
      "canonical_answer": "Warning signs include results that are too good to be true, reluctance to share data, statistical anomalies, duplicate publication, and patterns inconsistent with known science.",
      "principle": "research_fraud_warning_signs"
    },
    {
      "id": 151,
      "category": "evidence_evaluation",
      "question": "Why do prospective cohort studies provide stronger evidence than case-control studies?",
      "canonical_answer": "Prospective cohort studies establish temporal sequence, reduce recall bias, allow calculation of incidence rates, and are less susceptible to selection bias.",
      "principle": "prospective_cohort_vs_case_control"
    },
    {
      "id": 152,
      "category": "logical_fallacies",
      "question": "What is the appeal to consequences fallacy?",
      "canonical_answer": "Appeal to consequences argues that a claim must be true or false based on the desirability of its implications, rather than on evidence for the claim itself.",
      "principle": "appeal_to_consequences_fallacy"
    },
    {
      "id": 153,
      "category": "scientific_method",
      "question": "How do you design experiments to test causal mechanisms?",
      "canonical_answer": "Test causal mechanisms by manipulating proposed mediating variables, measuring intermediate steps, establishing temporal sequence, and ruling out alternative pathways.",
      "principle": "causal_mechanism_experimental_design"
    },
    {
      "id": 154,
      "category": "skeptical_analysis",
      "question": "What role should statistical significance play in interpreting research results?",
      "canonical_answer": "Statistical significance indicates whether results are likely due to chance, but should be considered alongside effect size, confidence intervals, biological plausibility, and replication.",
      "principle": "statistical_significance_interpretation_role"
    },
    {
      "id": 155,
      "category": "falsifiability",
      "question": "How do you distinguish between ad hoc modifications and legitimate theory refinement?",
      "canonical_answer": "Legitimate refinements make independent predictions, increase explanatory power, and are motivated by new evidence. Ad hoc modifications only save the theory without adding testable content.",
      "principle": "ad_hoc_vs_legitimate_theory_modification"
    },
    {
      "id": 156,
      "category": "peer_review",
      "question": "What factors can compromise the objectivity of peer review?",
      "canonical_answer": "Factors include personal relationships, institutional rivalries, theoretical commitments, career competition, cultural biases, and financial conflicts of interest.",
      "principle": "peer_review_objectivity_compromising_factors"
    },
    {
      "id": 157,
      "category": "independent_verification",
      "question": "Why might studies with identical methodology still produce different results?",
      "canonical_answer": "Differences can arise from population variations, temporal changes, measurement instrument differences, researcher effects, and random sampling variation.",
      "principle": "identical_methodology_different_results"
    },
    {
      "id": 158,
      "category": "extraordinary_claims",
      "question": "How do you evaluate extraordinary claims that lack peer review?",
      "canonical_answer": "Without peer review, apply extra scrutiny: evaluate methodology independently, seek expert opinions, look for replication attempts, and consider why peer review was bypassed.",
      "principle": "non_peer_reviewed_extraordinary_claims"
    },
    {
      "id": 159,
      "category": "cognitive_biases",
      "question": "How does the halo effect influence evaluation of research and researchers?",
      "canonical_answer": "Halo effect causes overall impressions to bias specific judgments, leading to less critical evaluation of work by prestigious researchers or institutions.",
      "principle": "halo_effect_research_evaluation"
    },
    {
      "id": 160,
      "category": "baloney_detection",
      "question": "What are the key principles for evaluating scientific claims in media reports?",
      "canonical_answer": "Check original sources, evaluate study design, consider sample sizes, look for peer review, assess conflicts of interest, and verify that conclusions match the evidence.",
      "principle": "media_scientific_claims_evaluation"
    },
    {
      "id": 161,
      "category": "evidence_evaluation",
      "question": "Why are interim analyses in clinical trials subject to special statistical considerations?",
      "canonical_answer": "Interim analyses increase the risk of false positives due to multiple testing. Statistical adjustments are needed to maintain overall error rates when trials may be stopped early.",
      "principle": "interim_analysis_statistical_considerations"
    },
    {
      "id": 162,
      "category": "logical_fallacies",
      "question": "How do you identify when someone is using the genetic fallacy?",
      "canonical_answer": "Genetic fallacy dismisses ideas based on their origin rather than their merit. The source of an idea doesn't determine its truth value.",
      "principle": "genetic_fallacy_identification"
    },
    {
      "id": 163,
      "category": "scientific_method",
      "question": "What is the role of pilot studies in research planning?",
      "canonical_answer": "Pilot studies test feasibility, refine methodology, estimate effect sizes for power calculations, identify potential problems, and optimize procedures before full-scale studies.",
      "principle": "pilot_studies_research_planning_role"
    },
    {
      "id": 164,
      "category": "skeptical_analysis",
      "question": "How do you evaluate the quality of evidence synthesis in guidelines?",
      "canonical_answer": "Assess search strategy comprehensiveness, inclusion criteria appropriateness, quality assessment methods, evidence grading systems, and whether recommendations match evidence strength.",
      "principle": "guideline_evidence_synthesis_evaluation"
    },
    {
      "id": 165,
      "category": "falsifiability",
      "question": "What role do theoretical commitments play in resistance to falsification?",
      "canonical_answer": "Strong theoretical commitments can lead to dismissing contrary evidence or making ad hoc modifications to preserve theories, hindering scientific progress and error correction.",
      "principle": "theoretical_commitments_falsification_resistance"
    },
    {
      "id": 166,
      "category": "peer_review",
      "question": "Why might peer review be less effective for interdisciplinary research?",
      "canonical_answer": "Interdisciplinary research may lack reviewers with expertise across all relevant fields, leading to inadequate evaluation of methods or interpretation in some domains.",
      "principle": "peer_review_interdisciplinary_limitations"
    },
    {
      "id": 167,
      "category": "independent_verification",
      "question": "What constitutes a meaningful replication crisis and how should it be addressed?",
      "canonical_answer": "A replication crisis involves systematic failure to reproduce important findings. Address through improved methodology, larger sample sizes, pre-registration, and incentive changes.",
      "principle": "replication_crisis_definition_solutions"
    },
    {
      "id": 168,
      "category": "extraordinary_claims",
      "question": "How should the scientific community respond to claims that challenge paradigms?",
      "canonical_answer": "Respond with proportionate skepticism, rigorous testing, open evaluation, and willingness to change if evidence is compelling. Avoid both premature acceptance and dogmatic rejection.",
      "principle": "paradigm_challenge_scientific_response"
    },
    {
      "id": 169,
      "category": "cognitive_biases",
      "question": "How does the endowment effect influence attachment to scientific theories?",
      "canonical_answer": "Endowment effect causes researchers to overvalue their own theories and findings, making them reluctant to abandon favored hypotheses even when evidence is weak.",
      "principle": "endowment_effect_theory_attachment"
    },
    {
      "id": 170,
      "category": "baloney_detection",
      "question": "What are the hallmarks of legitimate scientific uncertainty communication?",
      "canonical_answer": "Legitimate uncertainty communication acknowledges genuine gaps, explains their significance, describes ongoing research, and avoids false precision or manufactured doubt.",
      "principle": "legitimate_scientific_uncertainty_communication"
    },
    {
      "id": 171,
      "category": "evidence_evaluation",
      "question": "Why do adaptive clinical trial designs require special analytical considerations?",
      "canonical_answer": "Adaptive designs allow modifications during trials, which can affect statistical properties. Special methods are needed to control error rates and ensure valid inference.",
      "principle": "adaptive_trial_analytical_considerations"
    },
    {
      "id": 172,
      "category": "logical_fallacies",
      "question": "What characterizes the middle ground fallacy in scientific disputes?",
      "canonical_answer": "Middle ground fallacy assumes the truth lies between two positions when one may be correct and the other wrong. Not all scientific disputes have equally valid sides.",
      "principle": "middle_ground_fallacy_scientific_disputes"
    },
    {
      "id": 173,
      "category": "scientific_method",
      "question": "How do you distinguish between correlation, confounding, and causation in observational studies?",
      "canonical_answer": "Correlation shows statistical association. Confounding creates spurious associations through third variables. Causation requires mechanism, temporal sequence, and elimination of alternatives.",
      "principle": "correlation_confounding_causation_distinction"
    },
    {
      "id": 174,
      "category": "skeptical_analysis",
      "question": "What factors should influence your interpretation of conflicting study results?",
      "canonical_answer": "Consider study quality, sample sizes, methodology differences, population variations, publication bias, conflicts of interest, and whether differences are within expected variation.",
      "principle": "conflicting_results_interpretation_factors"
    },
    {
      "id": 175,
      "category": "falsifiability",
      "question": "How do you evaluate the testability of complex multifactor theories?",
      "canonical_answer": "Complex theories are testable if they make specific predictions that distinguish them from alternatives, even if individual components are difficult to isolate.",
      "principle": "complex_theory_testability_evaluation"
    },
    {
      "id": 176,
      "category": "peer_review",
      "question": "What are the benefits and limitations of reviewer anonymity?",
      "canonical_answer": "Anonymity allows honest criticism without fear of retaliation but may reduce accountability. It can encourage frank evaluation but also potentially irresponsible comments.",
      "principle": "reviewer_anonymity_benefits_limitations"
    },
    {
      "id": 177,
      "category": "independent_verification",
      "question": "Why might meta-analyses of biased individual studies still produce biased conclusions?",
      "canonical_answer": "Meta-analyses cannot fully correct for systematic biases in individual studies. Combining biased studies may strengthen false conclusions rather than revealing truth.",
      "principle": "meta_analysis_bias_propagation"
    },
    {
      "id": 178,
      "category": "extraordinary_claims",
      "question": "How do you assess the credibility of scientific claims made in patents?",
      "canonical_answer": "Patent claims may not undergo rigorous peer review. Evaluate supporting evidence, independent verification, consistency with established science, and commercial motivations.",
      "principle": "patent_scientific_claims_credibility"
    },
    {
      "id": 179,
      "category": "cognitive_biases",
      "question": "How does the bandwagon effect influence scientific research priorities?",
      "canonical_answer": "Bandwagon effect leads researchers to follow popular trends rather than pursuing independent lines of inquiry, potentially creating research bubbles and neglecting important questions.",
      "principle": "bandwagon_effect_research_priorities"
    },
    {
      "id": 180,
      "category": "baloney_detection",
      "question": "What are the warning signs of predatory or exploitative research practices?",
      "canonical_answer": "Warning signs include excessive publication fees, minimal peer review, broad scope journals, spam solicitations, and promises of rapid publication regardless of quality.",
      "principle": "predatory_research_practices_detection"
    },
    {
      "id": 181,
      "category": "evidence_evaluation",
      "question": "Why do surrogate endpoints in clinical trials require careful validation?",
      "canonical_answer": "Surrogate endpoints may not accurately predict clinical outcomes. They require validation showing they reliably reflect meaningful benefits to patients.",
      "principle": "surrogate_endpoint_validation_requirement"
    },
    {
      "id": 182,
      "category": "logical_fallacies",
      "question": "How do you identify when someone is committing the perfectionist fallacy?",
      "canonical_answer": "Perfectionist fallacy rejects solutions that aren't perfect while ignoring that they may be better than alternatives or current situations.",
      "principle": "perfectionist_fallacy_identification"
    },
    {
      "id": 183,
      "category": "scientific_method",
      "question": "What is the importance of external validity in research design?",
      "canonical_answer": "External validity determines whether findings generalize to other populations, settings, and conditions beyond the specific study context.",
      "principle": "external_validity_importance"
    },
    {
      "id": 184,
      "category": "skeptical_analysis",
      "question": "How do you evaluate the reliability of computer modeling and simulation studies?",
      "canonical_answer": "Evaluate model assumptions, validation against real data, sensitivity analysis, uncertainty quantification, and whether conclusions are within the model's valid range.",
      "principle": "computer_modeling_reliability_evaluation"
    },
    {
      "id": 185,
      "category": "falsifiability",
      "question": "What makes some scientific questions more amenable to falsification than others?",
      "canonical_answer": "Questions about specific, measurable phenomena with clear predictions are more amenable to falsification than vague, complex, or value-laden questions.",
      "principle": "falsification_amenability_factors"
    },
    {
      "id": 186,
      "category": "peer_review",
      "question": "How do cultural and linguistic biases affect international peer review?",
      "canonical_answer": "Cultural biases may favor familiar research approaches, while linguistic biases may disadvantage non-native speakers, potentially limiting diversity of perspectives in science.",
      "principle": "cultural_linguistic_peer_review_bias"
    },
    {
      "id": 187,
      "category": "independent_verification",
      "question": "What role should replication play in tenure and promotion decisions?",
      "canonical_answer": "Replication should be valued alongside original research because it validates scientific knowledge, even though it may receive less recognition than novel findings.",
      "principle": "replication_career_advancement_role"
    },
    {
      "id": 188,
      "category": "extraordinary_claims",
      "question": "How do you evaluate scientific claims made by commercial entities?",
      "canonical_answer": "Apply extra scrutiny due to financial conflicts: demand independent verification, peer review, full data disclosure, and comparison with non-commercial research.",
      "principle": "commercial_scientific_claims_evaluation"
    },
    {
      "id": 189,
      "category": "cognitive_biases",
      "question": "How does the mere exposure effect influence acceptance of scientific ideas?",
      "canonical_answer": "Mere exposure effect increases acceptance of frequently encountered ideas, potentially leading to acceptance based on familiarity rather than evidence quality.",
      "principle": "mere_exposure_effect_scientific_acceptance"
    },
    {
      "id": 190,
      "category": "baloney_detection",
      "question": "What questions should you ask about any scientific survey or poll?",
      "canonical_answer": "Ask about sampling methods, response rates, question wording, timing, funding sources, potential biases, and whether results are presented with appropriate uncertainty measures.",
      "principle": "scientific_survey_evaluation_questions"
    },
    {
      "id": 191,
      "category": "evidence_evaluation",
      "question": "Why do Bayesian statistical methods provide additional insights beyond traditional approaches?",
      "canonical_answer": "Bayesian methods incorporate prior knowledge, provide probability statements about hypotheses, handle uncertainty more naturally, and update beliefs as evidence accumulates.",
      "principle": "bayesian_statistics_additional_insights"
    },
    {
      "id": 192,
      "category": "logical_fallacies",
      "question": "What is the conjunction fallacy in probability reasoning?",
      "canonical_answer": "Conjunction fallacy occurs when people judge specific conditions as more probable than general ones, violating the rule that conjunctions cannot be more probable than their components.",
      "principle": "conjunction_fallacy_probability_reasoning"
    },
    {
      "id": 193,
      "category": "scientific_method",
      "question": "How do you design studies to minimize researcher bias in data collection?",
      "canonical_answer": "Use standardized protocols, objective measurements, blinded assessments, multiple observers, automated data collection, and independent data verification.",
      "principle": "researcher_bias_minimization_study_design"
    },
    {
      "id": 194,
      "category": "skeptical_analysis",
      "question": "What factors make some research findings more likely to be overturned?",
      "canonical_answer": "Findings more likely to be overturned have small effect sizes, small sample sizes, flexible analysis methods, strong biases, hot scientific topics, or financial interests.",
      "principle": "research_finding_overturn_risk_factors"
    },
    {
      "id": 195,
      "category": "falsifiability",
      "question": "How do you assess whether a scientific theory has genuine predictive power?",
      "canonical_answer": "Genuine predictive power involves making specific, novel predictions before observations, rather than explaining known phenomena post hoc.",
      "principle": "genuine_predictive_power_assessment"
    },
    {
      "id": 196,
      "category": "peer_review",
      "question": "What are the challenges of peer reviewing highly technical or specialized research?",
      "canonical_answer": "Challenges include finding qualified reviewers, ensuring adequate expertise depth, managing conflicts in small specialist communities, and evaluating novel methodologies.",
      "principle": "specialized_research_peer_review_challenges"
    },
    {
      "id": 197,
      "category": "independent_verification",
      "question": "Why might some research areas have lower replication rates than others?",
      "canonical_answer": "Areas with complex methodology, expensive equipment, small effect sizes, heterogeneous populations, or fewer researchers may have lower replication rates due to practical constraints.",
      "principle": "field_specific_replication_rate_factors"
    },
    {
      "id": 198,
      "category": "extraordinary_claims",
      "question": "How do you evaluate claims about the efficacy of unproven medical treatments?",
      "canonical_answer": "Require randomized controlled trials, peer review, independent replication, plausible mechanisms, and evidence superior to established treatments for the same condition.",
      "principle": "unproven_treatment_efficacy_evaluation"
    },
    {
      "id": 199,
      "category": "cognitive_biases",
      "question": "How does the optimism bias affect interpretation of preliminary research results?",
      "canonical_answer": "Optimism bias leads to overestimating the probability that preliminary positive results will be confirmed, underestimating the likelihood of failure or negative results.",
      "principle": "optimism_bias_preliminary_results"
    },
    {
      "id": 200,
      "category": "baloney_detection",
      "question": "What are the key indicators of scientific integrity in research reporting?",
      "canonical_answer": "Indicators include transparent methodology, appropriate statistical analysis, acknowledgment of limitations, balanced interpretation, conflict disclosure, and availability of raw data.",
      "principle": "scientific_integrity_reporting_indicators"
    },
    {
      "id": 201,
      "category": "evidence_evaluation",
      "question": "Why do pragmatic clinical trials provide different insights than explanatory trials?",
      "canonical_answer": "Pragmatic trials test interventions under real-world conditions with diverse populations, providing evidence about effectiveness in actual practice rather than ideal conditions.",
      "principle": "pragmatic_vs_explanatory_trials"
    },
    {
      "id": 202,
      "category": "logical_fallacies",
      "question": "How do you identify when someone is using the loaded question fallacy?",
      "canonical_answer": "Loaded questions contain controversial assumptions or implications that bias responses, making it difficult to answer without accepting the questioner's premise.",
      "principle": "loaded_question_fallacy_identification"
    },
    {
      "id": 203,
      "category": "scientific_method",
      "question": "What is the role of negative controls in experimental validation?",
      "canonical_answer": "Negative controls should produce no effect if the experimental system is working correctly, helping identify false positives and systematic errors.",
      "principle": "negative_controls_experimental_validation"
    },
    {
      "id": 204,
      "category": "skeptical_analysis",
      "question": "How do you evaluate the significance of contradictory expert opinions?",
      "canonical_answer": "Evaluate the experts' qualifications, potential conflicts, quality of evidence they cite, and whether disagreement reflects genuine uncertainty or motivated reasoning.",
      "principle": "contradictory_expert_opinion_evaluation"
    },
    {
      "id": 205,
      "category": "falsifiability",
      "question": "What role do thought experiments play in scientific reasoning?",
      "canonical_answer": "Thought experiments clarify concepts, reveal logical implications, and identify testable predictions, but they cannot replace empirical testing of scientific hypotheses.",
      "principle": "thought_experiments_scientific_reasoning_role"
    },
    {
      "id": 206,
      "category": "peer_review",
      "question": "How can artificial intelligence impact the future of peer review?",
      "canonical_answer": "AI could help with initial screening, plagiarism detection, statistical error identification, and reviewer matching, but human judgment remains essential for evaluating significance and interpretation.",
      "principle": "ai_peer_review_impact_potential"
    },
    {
      "id": 207,
      "category": "independent_verification",
      "question": "What ethical considerations arise in replication research?",
      "canonical_answer": "Ethical considerations include proper attribution, avoiding public criticism that could damage careers, balancing scientific integrity with collegiality, and responsible communication of failures.",
      "principle": "replication_research_ethical_considerations"
    },
    {
      "id": 208,
      "category": "extraordinary_claims",
      "question": "How do you assess scientific claims that lack conventional experimental support?",
      "canonical_answer": "Look for alternative forms of evidence like natural experiments, convergent evidence from multiple sources, theoretical consistency, and whether claims make testable predictions.",
      "principle": "non_experimental_scientific_claims_assessment"
    },
    {
      "id": 209,
      "category": "cognitive_biases",
      "question": "How does the clustering illusion affect interpretation of research data?",
      "canonical_answer": "Clustering illusion leads people to see patterns in random data, potentially resulting in false discoveries or overinterpretation of spurious correlations.",
      "principle": "clustering_illusion_data_interpretation"
    },
    {
      "id": 210,
      "category": "baloney_detection",
      "question": "What are the warning signs of manipulated or fabricated research images?",
      "canonical_answer": "Warning signs include inconsistent lighting, repeated patterns, unnatural boundaries, statistical irregularities, and results that are too clean or perfect.",
      "principle": "fabricated_research_images_detection"
    },
    {
      "id": 211,
      "category": "evidence_evaluation",
      "question": "Why do real-world evidence studies complement randomized controlled trials?",
      "canonical_answer": "Real-world evidence provides information about effectiveness in diverse populations, long-term outcomes, rare adverse events, and practical implementation challenges.",
      "principle": "real_world_evidence_rct_complementarity"
    },
    {
      "id": 212,
      "category": "logical_fallacies",
      "question": "What characterizes the Texas sharpshooter fallacy in data analysis?",
      "canonical_answer": "Texas sharpshooter fallacy involves finding patterns after looking at data, then claiming the patterns were predicted. It's like shooting randomly then drawing targets around clusters.",
      "principle": "texas_sharpshooter_fallacy_data_analysis"
    },
    {
      "id": 213,
      "category": "scientific_method",
      "question": "How do you design experiments to detect small but important effects?",
      "canonical_answer": "Use adequate sample sizes based on power analysis, minimize measurement error, reduce confounding variables, and use sensitive and specific outcome measures.",
      "principle": "small_effect_detection_experimental_design"
    },
    {
      "id": 214,
      "category": "skeptical_analysis",
      "question": "What factors should influence your confidence in emerging scientific consensus?",
      "canonical_answer": "Consider the quality and quantity of evidence, independence of supporting studies, expertise of contributing scientists, and whether consensus formed through proper scientific discourse.",
      "principle": "emerging_consensus_confidence_factors"
    },
    {
      "id": 215,
      "category": "falsifiability",
      "question": "How do you evaluate scientific theories that make probabilistic rather than deterministic predictions?",
      "canonical_answer": "Probabilistic theories are falsifiable if they make specific statistical predictions that can be tested against observed frequencies across multiple trials or conditions.",
      "principle": "probabilistic_theory_falsifiability_evaluation"
    },
    {
      "id": 216,
      "category": "peer_review",
      "question": "What are the advantages and disadvantages of collaborative peer review models?",
      "canonical_answer": "Collaborative review can provide more comprehensive evaluation and reduce individual biases, but may also lead to groupthink or compromise quality for consensus.",
      "principle": "collaborative_peer_review_tradeoffs"
    },
    {
      "id": 217,
      "category": "independent_verification",
      "question": "Why might exact replications sometimes be less informative than conceptual replications?",
      "canonical_answer": "Exact replications may repeat the same methodological limitations, while conceptual replications test the robustness of findings across different conditions and approaches.",
      "principle": "exact_vs_conceptual_replication_informativeness"
    },
    {
      "id": 218,
      "category": "extraordinary_claims",
      "question": "How do you evaluate scientific claims that challenge established measurement standards?",
      "canonical_answer": "Require independent verification using alternative methods, multiple laboratories, careful calibration procedures, and demonstration that the challenge is methodologically sound.",
      "principle": "measurement_standard_challenge_evaluation"
    },
    {
      "id": 219,
      "category": "cognitive_biases",
      "question": "How does the affect heuristic influence scientific decision-making?",
      "canonical_answer": "Affect heuristic causes decisions to be influenced by immediate emotional reactions rather than careful analysis, potentially biasing research priorities and interpretation.",
      "principle": "affect_heuristic_scientific_decision_making"
    },
    {
      "id": 220,
      "category": "baloney_detection",
      "question": "What are the characteristics of credible scientific whistleblowing?",
      "canonical_answer": "Credible whistleblowing involves specific documented concerns, attempts to resolve issues internally first, protection of public interest, and willingness to provide evidence.",
      "principle": "credible_scientific_whistleblowing_characteristics"
    },
    {
      "id": 221,
      "category": "evidence_evaluation",
      "question": "Why do umbrella reviews provide valuable perspectives on research evidence?",
      "canonical_answer": "Umbrella reviews synthesize evidence from multiple systematic reviews, providing broad overviews of research areas and identifying consistent patterns across different meta-analyses.",
      "principle": "umbrella_reviews_evidence_perspective"
    },
    {
      "id": 222,
      "category": "logical_fallacies",
      "question": "How do you identify when someone is using the division fallacy?",
      "canonical_answer": "Division fallacy assumes what is true for a group must be true for individuals within the group, ignoring individual variation and the possibility of different properties at different levels.",
      "principle": "division_fallacy_identification"
    },
    {
      "id": 223,
      "category": "scientific_method",
      "question": "What is the importance of intention-to-treat principles beyond clinical trials?",
      "canonical_answer": "Intention-to-treat principles prevent bias in any study where participants may not complete interventions, maintaining randomization benefits and providing realistic effect estimates.",
      "principle": "intention_to_treat_beyond_clinical_trials"
    },
    {
      "id": 224,
      "category": "skeptical_analysis",
      "question": "How do you evaluate the credibility of rapid scientific communications during emergencies?",
      "canonical_answer": "Balance urgency with quality: check methodology, look for peer review or expert validation, consider preliminary nature, and update beliefs as more evidence emerges.",
      "principle": "emergency_scientific_communication_credibility"
    },
    {
      "id": 225,
      "category": "falsifiability",
      "question": "What makes computer simulations and models scientifically valuable?",
      "canonical_answer": "Valuable models make testable predictions, are validated against real data, have transparent assumptions, undergo sensitivity analysis, and acknowledge limitations.",
      "principle": "computer_simulation_scientific_value"
    },
    {
      "id": 226,
      "category": "peer_review",
      "question": "How do time pressures affect the quality of peer review?",
      "canonical_answer": "Time pressures can reduce review depth, increase reliance on superficial indicators, decrease attention to methodology details, and compromise thoroughness of evaluation.",
      "principle": "time_pressure_peer_review_quality_impact"
    },
    {
      "id": 227,
      "category": "independent_verification",
      "question": "What role should systematic replication efforts play in science policy?",
      "canonical_answer": "Systematic replication should inform funding priorities, regulatory decisions, and clinical guidelines, ensuring policies are based on robust rather than preliminary evidence.",
      "principle": "systematic_replication_science_policy_role"
    },
    {
      "id": 228,
      "category": "extraordinary_claims",
      "question": "How do you assess scientific claims that rely heavily on statistical modeling?",
      "canonical_answer": "Evaluate model assumptions, validation procedures, uncertainty quantification, sensitivity analyses, and whether conclusions are robust across different modeling approaches.",
      "principle": "statistical_modeling_claims_assessment"
    },
    {
      "id": 229,
      "category": "cognitive_biases",
      "question": "How does the availability heuristic affect priority setting in research funding?",
      "canonical_answer": "Availability heuristic leads to overemphasis on recently publicized or memorable research areas, potentially underfunding important but less visible problems.",
      "principle": "availability_heuristic_research_funding_priorities"
    },
    {
      "id": 230,
      "category": "baloney_detection",
      "question": "What are the warning signs of ghost authorship in scientific publications?",
      "canonical_answer": "Warning signs include authors without apparent relevant expertise, inconsistent writing styles, reluctance to discuss methodology details, and industry-favorable conclusions without corresponding expertise.",
      "principle": "ghost_authorship_detection_warning_signs"
    },
    {
      "id": 231,
      "category": "evidence_evaluation",
      "question": "Why do individual participant data meta-analyses provide stronger evidence than aggregate data meta-analyses?",
      "canonical_answer": "Individual participant data allows standardized analyses, exploration of subgroups, time-to-event analyses, and detection of reporting biases that aggregate data cannot reveal.",
      "principle": "individual_vs_aggregate_data_meta_analysis"
    },
    {
      "id": 232,
      "category": "logical_fallacies",
      "question": "What is the prosecutor's fallacy in statistical interpretation?",
      "canonical_answer": "Prosecutor's fallacy confuses the probability of observing evidence given innocence with the probability of innocence given the evidence, leading to incorrect conclusions about causation.",
      "principle": "prosecutors_fallacy_statistical_interpretation"
    },
    {
      "id": 233,
      "category": "scientific_method",
      "question": "How do you design studies to minimize the impact of confounding by indication?",
      "canonical_answer": "Use randomization when possible, instrumental variables, propensity score matching, or natural experiments that approximate random assignment of treatments.",
      "principle": "confounding_by_indication_study_design"
    },
    {
      "id": 234,
      "category": "skeptical_analysis",
      "question": "What factors make scientific controversies more likely to be resolved through evidence?",
      "canonical_answer": "Controversies are more resolvable when they involve testable claims, clear measurement criteria, limited conflicts of interest, and willingness to accept empirical resolution.",
      "principle": "scientific_controversy_resolution_factors"
    },
    {
      "id": 235,
      "category": "falsifiability",
      "question": "How do you assess the scientific value of descriptive versus mechanistic theories?",
      "canonical_answer": "Descriptive theories organize observations and make predictions. Mechanistic theories additionally explain why phenomena occur. Both can be valuable if they generate testable hypotheses.",
      "principle": "descriptive_vs_mechanistic_theory_value"
    },
    {
      "id": 236,
      "category": "peer_review",
      "question": "What are the potential benefits of incentivizing high-quality peer review?",
      "canonical_answer": "Incentives could improve review thoroughness, attract qualified reviewers, reward constructive feedback, and enhance the overall quality of scientific evaluation.",
      "principle": "peer_review_incentives_potential_benefits"
    },
    {
      "id": 237,
      "category": "independent_verification",
      "question": "Why might some scientific findings be particularly difficult to replicate?",
      "canonical_answer": "Difficult-to-replicate findings may involve complex protocols, specialized expertise, expensive equipment, rare populations, or depend on contextual factors not fully described.",
      "principle": "difficult_replication_scientific_findings"
    },
    {
      "id": 238,
      "category": "extraordinary_claims",
      "question": "How do you evaluate extraordinary claims that lack direct experimental evidence?",
      "canonical_answer": "Look for indirect evidence, theoretical consistency, natural experiments, convergent evidence from multiple sources, and whether the claims generate testable predictions.",
      "principle": "indirect_evidence_extraordinary_claims"
    },
    {
      "id": 239,
      "category": "cognitive_biases",
      "question": "How does the illusion of control affect research planning and interpretation?",
      "canonical_answer": "Illusion of control leads researchers to overestimate their ability to control outcomes, potentially resulting in inadequate sample sizes or overconfident interpretations.",
      "principle": "illusion_of_control_research_planning"
    },
    {
      "id": 240,
      "category": "baloney_detection",
      "question": "What are the key features of responsible science communication to the public?",
      "canonical_answer": "Responsible communication presents uncertainties, avoids overgeneralization, provides context, acknowledges limitations, and distinguishes between preliminary and established findings.",
      "principle": "responsible_science_communication_features"
    },
    {
      "id": 241,
      "category": "evidence_evaluation",
      "question": "Why do living systematic reviews provide advantages over traditional reviews?",
      "canonical_answer": "Living systematic reviews are continuously updated as new evidence emerges, providing more current evidence synthesis and timely incorporation of important new findings.",
      "principle": "living_systematic_reviews_advantages"
    },
    {
      "id": 242,
      "category": "logical_fallacies",
      "question": "How do you identify when someone is committing the base rate fallacy?",
      "canonical_answer": "Base rate fallacy ignores the overall probability of events when interpreting specific information, leading to incorrect probability judgments and diagnostic errors.",
      "principle": "base_rate_fallacy_identification"
    },
    {
      "id": 243,
      "category": "scientific_method",
      "question": "What is the role of sensitivity analyses in strengthening research conclusions?",
      "canonical_answer": "Sensitivity analyses test whether conclusions hold under different assumptions, analytical choices, or data exclusions, demonstrating robustness of findings.",
      "principle": "sensitivity_analyses_conclusion_strengthening"
    },
    {
      "id": 244,
      "category": "skeptical_analysis",
      "question": "How do you evaluate the reliability of scientific information from social media?",
      "canonical_answer": "Verify sources, check original publications, look for peer review, assess credentials, consider potential biases, and cross-reference with authoritative sources.",
      "principle": "social_media_scientific_information_reliability"
    },
    {
      "id": 245,
      "category": "falsifiability",
      "question": "What role do crucial tests play in comparing competing scientific theories?",
      "canonical_answer": "Crucial tests attempt to distinguish between competing theories by identifying situations where they make different, mutually exclusive predictions about observable outcomes.",
      "principle": "crucial_tests_theory_comparison_role"
    },
    {
      "id": 246,
      "category": "peer_review",
      "question": "How do editorial policies influence the quality and direction of scientific research?",
      "canonical_answer": "Editorial policies affect what research gets published, the standards applied, the types of studies conducted, and ultimately the direction of scientific progress.",
      "principle": "editorial_policies_research_quality_direction"
    },
    {
      "id": 247,
      "category": "independent_verification",
      "question": "What institutional changes could better support replication research?",
      "canonical_answer": "Changes could include funding mechanisms for replication, career incentives, specialized journals, data sharing requirements, and recognition of replication's scientific value.",
      "principle": "institutional_replication_research_support"
    },
    {
      "id": 248,
      "category": "extraordinary_claims",
      "question": "How do you evaluate scientific claims that promise paradigm shifts?",
      "canonical_answer": "Evaluate evidence quality, independent verification, peer review, explanation of existing phenomena, novel predictions, and whether the shift is necessary rather than merely novel.",
      "principle": "paradigm_shift_claims_evaluation"
    },
    {
      "id": 249,
      "category": "cognitive_biases",
      "question": "How does the narrative fallacy affect interpretation of scientific findings?",
      "canonical_answer": "Narrative fallacy leads to preferring explanations that fit coherent stories rather than those supported by evidence, potentially distorting understanding of complex phenomena.",
      "principle": "narrative_fallacy_scientific_interpretation"
    },
    {
      "id": 250,
      "category": "baloney_detection",
      "question": "What are the essential elements of scientific skepticism?",
      "canonical_answer": "Essential elements include demanding evidence, questioning assumptions, considering alternative explanations, recognizing limitations, and maintaining openness to changing conclusions based on evidence.",
      "principle": "scientific_skepticism_essential_elements"
    },
    {
      "id": 251,
      "category": "evidence_evaluation",
      "question": "Why do factorial designs provide efficient testing of multiple research questions?",
      "canonical_answer": "Factorial designs allow simultaneous testing of multiple factors and their interactions, providing more information per participant than separate studies of individual factors.",
      "principle": "factorial_designs_research_efficiency"
    },
    {
      "id": 252,
      "category": "logical_fallacies",
      "question": "What characterizes the hasty generalization fallacy in research interpretation?",
      "canonical_answer": "Hasty generalization draws broad conclusions from insufficient evidence, typically based on small samples or limited observations that may not be representative.",
      "principle": "hasty_generalization_research_interpretation"
    },
    {
      "id": 253,
      "category": "scientific_method",
      "question": "How do you distinguish between exploratory data analysis and confirmatory hypothesis testing?",
      "canonical_answer": "Exploratory analysis generates hypotheses from patterns in data. Confirmatory testing evaluates pre-specified hypotheses on independent data to avoid capitalizing on chance.",
      "principle": "exploratory_vs_confirmatory_data_analysis"
    },
    {
      "id": 254,
      "category": "skeptical_analysis",
      "question": "What principles should guide scientific evaluation during public health emergencies?",
      "canonical_answer": "Balance speed with rigor, prioritize transparent communication, acknowledge uncertainties, use best available evidence, and update recommendations as evidence evolves.",
      "principle": "emergency_scientific_evaluation_principles"
    },
    {
      "id": 255,
      "category": "falsifiability",
      "question": "How do you assess whether a scientific research program is progressive or degenerative?",
      "canonical_answer": "Progressive programs make novel predictions, discover unexpected phenomena, and expand explanatory scope. Degenerative programs only accommodate known facts through ad hoc modifications.",
      "principle": "progressive_vs_degenerative_research_programs"
    },
    {
      "id": 256,
      "category": "cognitive_biases",
      "question": "How do multiple cognitive biases interact to compromise scientific reasoning?",
      "canonical_answer": "Multiple biases can reinforce each other, creating compound effects that severely distort evidence evaluation, hypothesis generation, and conclusion drawing in scientific research.",
      "principle": "multiple_biases_scientific_reasoning_interaction"
    }
  ]
}