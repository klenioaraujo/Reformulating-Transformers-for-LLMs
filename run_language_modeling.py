#!/usr/bin/env python3
"""
Language Modeling Pipeline for PureSpectralTransformer

This script trains and evaluates PureSpectralTransformer on WikiText-103
using the unified spectral framework with .Î¨cws format.

Usage:
  python3 run_language_modeling.py --model_type pure_spectral --output_dir ./results
"""

import argparse
import os
import sys
import math
import json
from pathlib import Path
from typing import Dict, List, Optional, Any
import torch
import torch.nn as nn
from datasets import Dataset, load_dataset
from transformers import (
    AutoTokenizer, Trainer, TrainingArguments,
    DataCollatorForLanguageModeling
)

# Add src directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.data.cws_manager import CWSDataManager
from train_spectral import PureSpectralTransformer


class SpectralLanguageModelingDataset:
    """
    Dataset for spectral language modeling using .Î¨cws format.

    This class handles the conversion of raw text to spectral representations
    and creates a dataset compatible with Hugging Face Trainer.
    """

    def __init__(self, cache_dir: str = "data/wikitext-103-cws",
                 max_seq_length: int = 128, tokenizer_name: str = "bert-base-uncased"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.max_seq_length = max_seq_length

        # Initialize components
        self.cws_manager = CWSDataManager()
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    def prepare_spectral_dataset(self, raw_dataset: Dataset, split: str) -> Dataset:
        """
        Prepare spectral dataset from raw text data.

        Args:
            raw_dataset: Raw dataset from Hugging Face
            split: Dataset split ('train', 'validation', 'test')

        Returns:
            Hugging Face Dataset with spectral representations
        """
        print(f"ğŸ“Š Preparing spectral dataset for {split} split...")

        # Filter out empty texts
        texts = [text for text in raw_dataset[split]['text'] if text.strip()]
        print(f"   Found {len(texts)} non-empty texts")

        spectral_data = []
        labels_data = []

        for i, text in enumerate(texts):
            if i % 100 == 0:
                print(f"   Processing text {i}/{len(texts)}")

            try:
                # Tokenize text
                tokens = self.tokenizer(
                    text,
                    truncation=True,
                    max_length=self.max_seq_length,
                    return_tensors='pt',
                    padding='max_length'
                )

                input_ids = tokens['input_ids'].squeeze(0)

                # Convert to spectral representation
                cws_path = self.cws_manager.convert(
                    'text',
                    text,
                    output_path=self.cache_dir / f"{split}_{i:06d}.Î¨cws"
                )

                # Load spectral data
                spectral_tensor = self.cws_manager.load(cws_path)

                # Ensure proper shape
                if spectral_tensor.dim() == 2:
                    seq_len, spectral_dim = spectral_tensor.shape
                    if seq_len > self.max_seq_length:
                        spectral_tensor = spectral_tensor[:self.max_seq_length]
                    elif seq_len < self.max_seq_length:
                        # Pad with zeros
                        pad_len = self.max_seq_length - seq_len
                        spectral_tensor = torch.cat([
                            spectral_tensor,
                            torch.zeros(pad_len, spectral_dim)
                        ], dim=0)

                spectral_data.append(spectral_tensor.numpy())
                labels_data.append(input_ids.numpy())

            except Exception as e:
                print(f"âš ï¸ Error processing text {i}: {e}")
                continue

        print(f"âœ… Created {len(spectral_data)} spectral samples for {split}")

        # Create Hugging Face dataset
        dataset_dict = {
            'input_spectrum': spectral_data,
            'labels': labels_data
        }

        return Dataset.from_dict(dataset_dict)

    def load_or_prepare_dataset(self, dataset_name: str = "wikitext",
                              dataset_config: str = "wikitext-103-raw-v1") -> Dict[str, Dataset]:
        """
        Load or prepare spectral dataset.

        Args:
            dataset_name: Name of the dataset
            dataset_config: Dataset configuration

        Returns:
            Dictionary with train, validation, and test datasets
        """
        # Check if cached dataset exists
        cache_file = self.cache_dir / "dataset_info.json"
        if cache_file.exists():
            print("ğŸ“ Loading cached spectral dataset...")
            with open(cache_file, 'r') as f:
                cache_info = json.load(f)

            datasets = {}
            for split in ['train', 'validation', 'test']:
                split_file = self.cache_dir / f"{split}_dataset.arrow"
                if split_file.exists():
                    datasets[split] = Dataset.from_file(str(split_file))
                    print(f"   Loaded {split}: {len(datasets[split])} samples")

            if len(datasets) == 3:
                return datasets

        # Load raw dataset
        print("ğŸ“¥ Loading raw dataset...")
        raw_dataset = load_dataset(dataset_name, dataset_config)

        # Prepare spectral datasets
        datasets = {}
        for split in ['train', 'validation', 'test']:
            if split in raw_dataset:
                spectral_dataset = self.prepare_spectral_dataset(raw_dataset, split)

                # Cache the dataset
                cache_path = self.cache_dir / f"{split}_dataset.arrow"
                spectral_dataset.to_file(str(cache_path))

                datasets[split] = spectral_dataset

        # Save cache info
        cache_info = {
            'dataset_name': dataset_name,
            'dataset_config': dataset_config,
            'max_seq_length': self.max_seq_length,
            'num_samples': {split: len(dataset) for split, dataset in datasets.items()}
        }
        with open(cache_file, 'w') as f:
            json.dump(cache_info, f, indent=2)

        return datasets


class SpectralLanguageModel(nn.Module):
    """
    Wrapper for PureSpectralTransformer compatible with Hugging Face Trainer.
    """

    def __init__(self, vocab_size: int, spectral_dim: int = 256,
                 n_layers: int = 6, n_heads: int = 8, max_seq_length: int = 128):
        super().__init__()
        self.vocab_size = vocab_size

        # Pure spectral transformer
        self.transformer = PureSpectralTransformer(
            vocab_size=vocab_size,
            spectral_dim=spectral_dim,
            n_layers=n_layers,
            n_heads=n_heads,
            max_seq_length=max_seq_length
        )

    def forward(self, input_spectrum: torch.Tensor, labels: Optional[torch.Tensor] = None):
        """
        Forward pass for spectral language modeling.

        Args:
            input_spectrum: Spectral input [batch_size, seq_len, spectral_dim]
            labels: Target token IDs [batch_size, seq_len]

        Returns:
            Dictionary with loss and logits
        """
        # For spectral language modeling, we need to convert spectral data
        # to token-like format. We'll use a simple approach where we treat
        # the spectral sequence as input and predict the next "token" in sequence

        batch_size, seq_len, spectral_dim = input_spectrum.shape

        # Create input_ids from sequence indices
        input_ids = torch.arange(seq_len, device=input_spectrum.device).unsqueeze(0).expand(batch_size, -1)

        # Forward pass through transformer
        logits = self.transformer(input_ids)

        loss = None
        if labels is not None:
            # Shift for causal language modeling
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()

            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, self.vocab_size),
                           shift_labels.view(-1))

        return {'loss': loss, 'logits': logits}


def compute_metrics(eval_pred):
    """
    Compute perplexity for language modeling evaluation.

    Args:
        eval_pred: Tuple of (predictions, labels)

    Returns:
        Dictionary with perplexity metric
    """
    predictions, labels = eval_pred

    # Calculate cross-entropy loss
    loss_fct = nn.CrossEntropyLoss()

    # Shift for causal language modeling
    shift_logits = predictions[..., :-1, :]
    shift_labels = labels[..., 1:]

    # Calculate loss
    loss = loss_fct(
        shift_logits.reshape(-1, shift_logits.shape[-1]),
        shift_labels.reshape(-1)
    )

    # Calculate perplexity
    perplexity = math.exp(loss.item())

    return {"perplexity": perplexity, "loss": loss.item()}


def main():
    parser = argparse.ArgumentParser(description='Train and evaluate PureSpectralTransformer on WikiText-103')

    # Dataset parameters
    parser.add_argument('--dataset_name', type=str, default='wikitext',
                       help='Name of the dataset')
    parser.add_argument('--dataset_config', type=str, default='wikitext-103-raw-v1',
                       help='Dataset configuration')
    parser.add_argument('--cache_dir', type=str, default='data/wikitext-103-cws',
                       help='Cache directory for spectral data')
    parser.add_argument('--max_seq_length', type=int, default=128,
                       help='Maximum sequence length')

    # Model architecture
    parser.add_argument('--spectral_dim', type=int, default=256,
                       help='Spectral dimension')
    parser.add_argument('--n_layers', type=int, default=6,
                       help='Number of transformer layers')
    parser.add_argument('--n_heads', type=int, default=8,
                       help='Number of attention heads')
    parser.add_argument('--vocab_size', type=int, default=10000,
                       help='Vocabulary size')

    # Training parameters
    parser.add_argument('--batch_size', type=int, default=8,
                       help='Batch size')
    parser.add_argument('--num_epochs', type=int, default=3,
                       help='Number of training epochs')
    parser.add_argument('--learning_rate', type=float, default=5e-5,
                       help='Learning rate')
    parser.add_argument('--warmup_steps', type=int, default=500,
                       help='Warmup steps')
    parser.add_argument('--weight_decay', type=float, default=0.01,
                       help='Weight decay')

    # Output
    parser.add_argument('--output_dir', type=str, default='./results',
                       help='Output directory for results')
    parser.add_argument('--logging_dir', type=str, default='./logs',
                       help='Logging directory')

    args = parser.parse_args()

    print("ğŸ”® Starting Language Modeling Pipeline for PureSpectralTransformer")
    print("=" * 70)

    # Create output directories
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Initialize spectral dataset
    print("ğŸ“Š Initializing spectral dataset...")
    dataset_preparer = SpectralLanguageModelingDataset(
        cache_dir=args.cache_dir,
        max_seq_length=args.max_seq_length
    )

    # Load or prepare datasets
    datasets = dataset_preparer.load_or_prepare_dataset(
        args.dataset_name, args.dataset_config
    )

    print(f"ğŸ“ˆ Dataset statistics:")
    for split, dataset in datasets.items():
        print(f"   {split}: {len(dataset)} samples")

    # Create model
    print("ğŸ¤– Creating PureSpectralTransformer model...")
    model = SpectralLanguageModel(
        vocab_size=args.vocab_size,
        spectral_dim=args.spectral_dim,
        n_layers=args.n_layers,
        n_heads=args.n_heads,
        max_seq_length=args.max_seq_length
    )

    # Calculate parameter efficiency
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š Model parameters: {total_params:,}")

    # Training arguments
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        overwrite_output_dir=True,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        warmup_steps=args.warmup_steps,
        weight_decay=args.weight_decay,
        logging_dir=args.logging_dir,
        logging_steps=100,
        eval_steps=500,
        save_steps=1000,
        evaluation_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="perplexity",
        greater_is_better=False,
        report_to=None,  # Disable external logging for simplicity
    )

    # Data collator for language modeling
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=dataset_preparer.tokenizer,
        mlm=False,  # Causal language modeling
        return_tensors="pt"
    )

    # Create trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=datasets['train'],
        eval_dataset=datasets['validation'],
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    # Start training
    print("ğŸ¯ Starting training...")
    trainer.train()

    # Evaluate on test set
    print("ğŸ“Š Evaluating on test set...")
    test_results = trainer.evaluate(eval_dataset=datasets['test'])

    # Save final results
    results_file = output_dir / "final_results.json"
    with open(results_file, 'w') as f:
        json.dump({
            'model_config': {
                'spectral_dim': args.spectral_dim,
                'n_layers': args.n_layers,
                'n_heads': args.n_heads,
                'vocab_size': args.vocab_size,
                'total_parameters': total_params
            },
            'training_config': {
                'batch_size': args.batch_size,
                'learning_rate': args.learning_rate,
                'num_epochs': args.num_epochs
            },
            'final_metrics': test_results
        }, f, indent=2)

    print(f"\nğŸ‰ Training and evaluation completed!")
    print(f"ğŸ“ Results saved to: {results_file}")
    print(f"ğŸ“Š Final test perplexity: {test_results.get('eval_perplexity', 'N/A'):.2f}")
    print(f"ğŸ“ˆ Parameter efficiency: {total_params:,} parameters")

    # Compare with standard transformer baseline
    standard_params_estimate = (
        args.vocab_size * args.spectral_dim +  # embedding
        args.n_layers * (args.spectral_dim * args.spectral_dim * 4) +  # attention
        args.n_layers * (args.spectral_dim * (args.spectral_dim * 4) * 2) +  # feed-forward
        args.spectral_dim * args.vocab_size  # output projection
    )

    efficiency_ratio = total_params / standard_params_estimate
    print(f"ğŸ“Š Parameter ratio vs standard: {efficiency_ratio:.4f}x")

    if efficiency_ratio < 1.0:
        print("âœ… EXCELLENT EFFICIENCY - PureSpectralTransformer is more parameter-efficient!")
    else:
        print("âš ï¸  MODERATE EFFICIENCY - Further optimization needed")


if __name__ == '__main__':
    main()